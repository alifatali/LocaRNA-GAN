{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "396972db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Acc=0.4089, Val F1=0.4588\n",
      "Epoch 2: Val Acc=0.4743, Val F1=0.5458\n",
      "Epoch 3: Val Acc=0.6052, Val F1=0.6538\n",
      "Epoch 4: Val Acc=0.5911, Val F1=0.6428\n",
      "Epoch 5: Val Acc=0.6002, Val F1=0.6323\n",
      "Epoch 6: Val Acc=0.6777, Val F1=0.6997\n",
      "Epoch 7: Val Acc=0.6828, Val F1=0.7289\n",
      "Epoch 8: Val Acc=0.6918, Val F1=0.7320\n",
      "Epoch 9: Val Acc=0.6898, Val F1=0.7365\n",
      "Epoch 10: Val Acc=0.7039, Val F1=0.7389\n",
      "Epoch 11: Val Acc=0.6959, Val F1=0.7433\n",
      "Epoch 12: Val Acc=0.7130, Val F1=0.7402\n",
      "Epoch 13: Val Acc=0.7221, Val F1=0.7740\n",
      "Epoch 14: Val Acc=0.7382, Val F1=0.7791\n",
      "Epoch 15: Val Acc=0.7261, Val F1=0.7704\n",
      "Epoch 16: Val Acc=0.7331, Val F1=0.7892\n",
      "Epoch 17: Val Acc=0.7492, Val F1=0.7969\n",
      "Epoch 18: Val Acc=0.7462, Val F1=0.7889\n",
      "Epoch 19: Val Acc=0.7644, Val F1=0.8175\n",
      "Epoch 20: Val Acc=0.7573, Val F1=0.8046\n",
      "Epoch 21: Val Acc=0.7633, Val F1=0.8000\n",
      "Epoch 22: Val Acc=0.7583, Val F1=0.8108\n",
      "Epoch 23: Val Acc=0.7664, Val F1=0.8175\n",
      "Epoch 24: Val Acc=0.7674, Val F1=0.8203\n",
      "Epoch 25: Val Acc=0.7744, Val F1=0.8164\n",
      "Epoch 26: Val Acc=0.7795, Val F1=0.8320\n",
      "Epoch 27: Val Acc=0.7784, Val F1=0.8255\n",
      "Epoch 28: Val Acc=0.7835, Val F1=0.8328\n",
      "Epoch 29: Val Acc=0.7724, Val F1=0.8281\n",
      "Epoch 30: Val Acc=0.7895, Val F1=0.8405\n",
      "Epoch 31: Val Acc=0.7835, Val F1=0.8394\n",
      "Epoch 32: Val Acc=0.7795, Val F1=0.8351\n",
      "Epoch 33: Val Acc=0.7865, Val F1=0.8384\n",
      "Epoch 34: Val Acc=0.7875, Val F1=0.8454\n",
      "Epoch 35: Val Acc=0.8016, Val F1=0.8522\n",
      "Epoch 36: Val Acc=0.7784, Val F1=0.8328\n",
      "Epoch 37: Val Acc=0.7865, Val F1=0.8231\n",
      "Epoch 38: Val Acc=0.7885, Val F1=0.8410\n",
      "Epoch 39: Val Acc=0.7784, Val F1=0.8267\n",
      "Epoch 40: Val Acc=0.7936, Val F1=0.8413\n",
      "Epoch 41: Val Acc=0.7815, Val F1=0.8263\n",
      "Epoch 42: Val Acc=0.7936, Val F1=0.8395\n",
      "Epoch 43: Val Acc=0.7835, Val F1=0.8158\n",
      "Epoch 44: Val Acc=0.7754, Val F1=0.8327\n",
      "Epoch 45: Val Acc=0.7825, Val F1=0.8326\n",
      "Early stopping triggered!\n",
      "ğŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8201    0.7514    0.7843      1062\n",
      "           1     0.8214    0.8054    0.8133       971\n",
      "           2     0.9722    1.0000    0.9859        70\n",
      "           3     0.6814    0.9114    0.7798       237\n",
      "           4     0.8214    0.9718    0.8903       142\n",
      "\n",
      "    accuracy                         0.8074      2482\n",
      "   macro avg     0.8233    0.8880    0.8507      2482\n",
      "weighted avg     0.8118    0.8074    0.8070      2482\n",
      "\n",
      "Best validation F1: 0.8522 (epoch 35)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, auc, classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 5\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "output_dir = \"F:\\\\payan-nameh\\\\faz2 . 1404.04.02\\\\Date\\\\RNALocate\\\\\"\n",
    "\n",
    "# ğŸ”¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "X_train_attention = np.load(f\"{output_dir}X_train_attention.npy\")\n",
    "X_val_attention = np.load(f\"{output_dir}X_val_attention.npy\")\n",
    "X_test_attention = np.load(f\"{output_dir}X_test_attention.npy\")\n",
    "y_train = np.load(f\"{output_dir}y_train.npy\")\n",
    "y_val = np.load(f\"{output_dir}y_val.npy\")\n",
    "y_test = np.load(f\"{output_dir}y_test.npy\")\n",
    "X_gan = np.load(f\"{output_dir}X_train_augmented.npy\")\n",
    "y_gan = np.load(f\"{output_dir}y_train_augmented.npy\")\n",
    "\n",
    "# Ù‡Ù…Ø§Ù‡Ù†Ú¯â€ŒØ³Ø§Ø²ÛŒ Ø§Ø¨Ø¹Ø§Ø¯ GAN Ø¨Ø§ Ø´Ø¨Ú©Ù‡\n",
    "class GanReducer(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=256):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "gan_reducer = GanReducer(input_dim=X_gan.shape[1], output_dim=X_train_attention.shape[1]).to(device)\n",
    "X_gan = X_gan.astype(np.float32)\n",
    "X_gan = torch.FloatTensor(X_gan).to(device)\n",
    "with torch.no_grad():\n",
    "    X_gan = gan_reducer(X_gan).cpu().numpy()\n",
    "\n",
    "# ØªØ±Ú©ÛŒØ¨ Ùˆ Ø´Ø§ÙÙ„\n",
    "X_train_combined = np.concatenate([X_train_attention, X_gan], axis=0)\n",
    "y_train_combined = np.concatenate([y_train, y_gan], axis=0)\n",
    "X_train_combined, y_train_combined = shuffle(X_train_combined, y_train_combined, random_state=42)\n",
    "input_dim = X_train_combined.shape[1]\n",
    "\n",
    "# Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§Ø³Øª Ùˆ Ù„ÙˆØ¯Ø±\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train_combined), torch.LongTensor(y_train_combined))\n",
    "val_dataset = TensorDataset(torch.FloatTensor(X_val_attention), torch.LongTensor(y_val))\n",
    "test_dataset = TensorDataset(torch.FloatTensor(X_test_attention), torch.LongTensor(y_test))\n",
    "\n",
    "# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² WeightedRandomSampler (Ø±Ø§Ù‡ Ø­Ù„ 2)\n",
    "class_counts = np.bincount(y_train_combined)\n",
    "weights = 1.0 / class_counts[y_train_combined]\n",
    "sampler = WeightedRandomSampler(weights, len(y_train_combined), replacement=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ğŸ”¶ Ú¯Ø§Ù… 5.3: Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "class FinalClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),  # Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø¹Ù…ÛŒÙ‚â€ŒØªØ± (Ø±Ø§Ù‡ Ø­Ù„ 3)\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = FinalClassifier(input_dim=input_dim).to(device)\n",
    "\n",
    "# ğŸ”¶ Ú¯Ø§Ù… 5.4: ØªØ¹Ø±ÛŒÙ loss + optimizer + scheduler\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_combined), y=y_train_combined)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-2)  # Ú©Ø§Ù‡Ø´ lr (Ø±Ø§Ù‡ Ø­Ù„ 4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)  # Ø§ÙØ²Ø§ÛŒØ´ patience (Ø±Ø§Ù‡ Ø­Ù„ 1)\n",
    "\n",
    "# ğŸ”¶ Ú¯Ø§Ù… 5.5: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Early Stopping\n",
    "best_val_f1 = 0.0\n",
    "patience_counter = 0\n",
    "val_f1_list, val_acc_list = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = loss_fn(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            val_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
    "            val_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1_list.append(val_f1)\n",
    "    val_acc_list.append(val_acc)\n",
    "    print(f\"Epoch {epoch+1}: Val Acc={val_acc:.4f}, Val F1={val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), f\"{output_dir}best_model.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 10:  # Ø§ÙØ²Ø§ÛŒØ´ patience\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    scheduler.step(1.0 - val_f1)\n",
    "\n",
    "# ğŸŸ© Ù…Ø±Ø­Ù„Ù‡ 6: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬\n",
    "model.load_state_dict(torch.load(f\"{output_dir}best_model.pth\"))\n",
    "model.eval()\n",
    "all_preds, all_probs, all_labels = [], [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        all_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
    "        all_probs.extend(out.cpu().numpy())\n",
    "        all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "np.save(f\"{output_dir}all_preds.npy\", np.array(all_preds))\n",
    "np.save(f\"{output_dir}all_probs.npy\", np.array(all_probs))\n",
    "np.save(f\"{output_dir}all_labels.npy\", np.array(all_labels))\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "print(\"ğŸ“Š Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n",
    "print(f\"Best validation F1: {best_val_f1:.4f} (epoch {np.argmax(val_f1_list)+1})\")\n",
    "\n",
    "# Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ (Ú©ÙˆØªØ§Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¯Ú¯ÛŒ)\n",
    "plt.figure()\n",
    "plt.plot(val_f1_list, label=\"F1 (macro)\")\n",
    "plt.plot(val_acc_list, label=\"Accuracy\")\n",
    "plt.title(\"Validation Scores per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(f\"{output_dir}f1_curve.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f80f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Acc=0.6647, Val F1=0.6866\n",
      "Epoch 2: Val Acc=0.6324, Val F1=0.6630\n",
      "Epoch 3: Val Acc=0.6949, Val F1=0.7192\n",
      "Epoch 4: Val Acc=0.6999, Val F1=0.7324\n",
      "Epoch 5: Val Acc=0.7210, Val F1=0.7596\n",
      "Epoch 6: Val Acc=0.7351, Val F1=0.7581\n",
      "Epoch 7: Val Acc=0.7482, Val F1=0.7859\n",
      "Epoch 8: Val Acc=0.7472, Val F1=0.7833\n",
      "Epoch 9: Val Acc=0.7472, Val F1=0.7891\n",
      "Epoch 10: Val Acc=0.7633, Val F1=0.8054\n",
      "Epoch 11: Val Acc=0.7795, Val F1=0.8283\n",
      "Epoch 12: Val Acc=0.7623, Val F1=0.8078\n",
      "Epoch 13: Val Acc=0.7644, Val F1=0.8114\n",
      "Epoch 14: Val Acc=0.7694, Val F1=0.8209\n",
      "Epoch 15: Val Acc=0.7865, Val F1=0.8315\n",
      "Epoch 16: Val Acc=0.7986, Val F1=0.8464\n",
      "Epoch 17: Val Acc=0.8077, Val F1=0.8607\n",
      "Epoch 18: Val Acc=0.8026, Val F1=0.8467\n",
      "Epoch 19: Val Acc=0.7895, Val F1=0.8396\n",
      "Epoch 20: Val Acc=0.7855, Val F1=0.8360\n",
      "Epoch 21: Val Acc=0.8117, Val F1=0.8592\n",
      "Epoch 22: Val Acc=0.8006, Val F1=0.8492\n",
      "Early stopping triggered!\n",
      "ğŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8133    0.7957    0.8044      1062\n",
      "           1     0.8353    0.7992    0.8168       971\n",
      "           2     1.0000    0.9857    0.9928        70\n",
      "           3     0.7491    0.8819    0.8101       237\n",
      "           4     0.8373    0.9789    0.9026       142\n",
      "\n",
      "    accuracy                         0.8211      2482\n",
      "   macro avg     0.8470    0.8883    0.8653      2482\n",
      "weighted avg     0.8224    0.8211    0.8207      2482\n",
      "\n",
      "Best validation F1: 0.8607 (epoch 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, auc, classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 5  # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ RNALocate (Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…)\n",
    "batch_size = 64\n",
    "num_epochs = 60\n",
    "output_dir = \"F:\\\\payan-nameh\\\\faz2 . 1404.04.02\\\\Date\\\\RNALocate\\\\\"\n",
    "\n",
    "# ğŸ”¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "# -------------------------------\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ attention (Ú¯Ø§Ù… Û³)\n",
    "X_train_attention = np.load(f\"{output_dir}X_train_attention.npy\")\n",
    "X_val_attention = np.load(f\"{output_dir}X_val_attention.npy\")\n",
    "X_test_attention = np.load(f\"{output_dir}X_test_attention.npy\")\n",
    "\n",
    "y_train = np.load(f\"{output_dir}y_train.npy\")\n",
    "y_val = np.load(f\"{output_dir}y_val.npy\")\n",
    "y_test = np.load(f\"{output_dir}y_test.npy\")\n",
    "\n",
    "# -------------------------------\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ GAN (Ú¯Ø§Ù… Û´)\n",
    "X_gan = np.load(f\"{output_dir}X_train_augmented.npy\")\n",
    "y_gan = np.load(f\"{output_dir}y_train_augmented.npy\")\n",
    "\n",
    "# -------------------------------\n",
    "# Ù‡Ù…Ø§Ù‡Ù†Ú¯â€ŒØ³Ø§Ø²ÛŒ Ø§Ø¨Ø¹Ø§Ø¯ GAN Ø¨Ø§ Ø´Ø¨Ú©Ù‡ (Ø±Ø§Ù‡ Ø­Ù„ Ø´Ù…Ø§Ø±Ù‡ 2)\n",
    "class GanReducer(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=256):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "gan_reducer = GanReducer(input_dim=X_gan.shape[1], output_dim=X_train_attention.shape[1]).to(device)\n",
    "X_gan = X_gan.astype(np.float32)\n",
    "X_gan = torch.FloatTensor(X_gan).to(device)\n",
    "with torch.no_grad():\n",
    "    X_gan = gan_reducer(X_gan).cpu().numpy()\n",
    "\n",
    "# -------------------------------\n",
    "# ØªØ±Ú©ÛŒØ¨ attention + GAN\n",
    "X_train_combined = np.concatenate([X_train_attention, X_gan], axis=0)\n",
    "y_train_combined = np.concatenate([y_train, y_gan], axis=0)\n",
    "\n",
    "# -------------------------------\n",
    "# Ø´Ø§ÙÙ„ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "X_train_combined, y_train_combined = shuffle(X_train_combined, y_train_combined, random_state=42)\n",
    "\n",
    "# -------------------------------\n",
    "# Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)\n",
    "input_dim = X_train_combined.shape[1]\n",
    "\n",
    "# -------------------------------\n",
    "# Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§Ø³Øª Ùˆ Ù„ÙˆØ¯Ø±\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train_combined), torch.LongTensor(y_train_combined))\n",
    "val_dataset = TensorDataset(torch.FloatTensor(X_val_attention), torch.LongTensor(y_val))\n",
    "test_dataset = TensorDataset(torch.FloatTensor(X_test_attention), torch.LongTensor(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ğŸ”¶ Ú¯Ø§Ù… 5.3: Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ (Ø±Ø§Ù‡ Ø­Ù„ Ø´Ù…Ø§Ø±Ù‡ 4: Ø³Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„)\n",
    "class FinalClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "            # Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ù…Ø¯Ù„ Ø¹Ù…ÛŒÙ‚â€ŒØªØ±ØŒ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø§ÛŒÙ†Ùˆ ÙØ¹Ø§Ù„ Ú©Ù†ÛŒ:\n",
    "            # nn.Linear(input_dim, 1024),\n",
    "            # nn.LayerNorm(1024),  # Ú©Ø§Ù…Ù†Øªâ€ŒØ´Ø¯Ù‡ (Ø±Ø§Ù‡ Ø­Ù„ Ø´Ù…Ø§Ø±Ù‡ 3)\n",
    "            # nn.GELU(),\n",
    "            # nn.Dropout(0.3),\n",
    "            # nn.Linear(1024, 512),\n",
    "            # nn.LayerNorm(512),  # Ú©Ø§Ù…Ù†Øªâ€ŒØ´Ø¯Ù‡ (Ø±Ø§Ù‡ Ø­Ù„ Ø´Ù…Ø§Ø±Ù‡ 3)\n",
    "            # nn.GELU(),\n",
    "            # nn.Dropout(0.3),\n",
    "            # nn.Linear(512, 256),\n",
    "            # nn.LayerNorm(256),  # Ú©Ø§Ù…Ù†Øªâ€ŒØ´Ø¯Ù‡ (Ø±Ø§Ù‡ Ø­Ù„ Ø´Ù…Ø§Ø±Ù‡ 3)\n",
    "            # nn.GELU(),\n",
    "            # nn.Dropout(0.3),\n",
    "            # nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = FinalClassifier(input_dim=input_dim).to(device)\n",
    "\n",
    "# ğŸ”¶ Ú¯Ø§Ù… 5.4: ØªØ¹Ø±ÛŒÙ loss + optimizer + scheduler (Ø±Ø§Ù‡ Ø­Ù„ Ø´Ù…Ø§Ø±Ù‡ 1: Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ CrossEntropyLoss)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_combined), y=y_train_combined)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)  # Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ CrossEntropyLoss\n",
    "# Ø¨Ø±Ø§ÛŒ ØªØ³Øª Focal LossØŒ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø§ÛŒÙ†Ùˆ ÙØ¹Ø§Ù„ Ú©Ù†ÛŒ:\n",
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, gamma=2.0, weight=None):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.gamma = gamma\n",
    "#         self.weight = weight\n",
    "#     def forward(self, input, target):\n",
    "#         ce_loss = nn.CrossEntropyLoss(weight=self.weight, reduction='none')(input, target)\n",
    "#         pt = torch.exp(-ce_loss)\n",
    "#         focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "#         return focal_loss.mean()\n",
    "# loss_fn = FocalLoss(gamma=2.0, weight=class_weights_tensor)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4)\n",
    "\n",
    "# ğŸ”¶ Ú¯Ø§Ù… 5.5: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Early Stopping\n",
    "best_val_f1 = 0.0\n",
    "patience_counter = 0\n",
    "val_f1_list, val_acc_list = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = loss_fn(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            val_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
    "            val_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1_list.append(val_f1)\n",
    "    val_acc_list.append(val_acc)\n",
    "    print(f\"Epoch {epoch+1}: Val Acc={val_acc:.4f}, Val F1={val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), f\"{output_dir}best_model.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= 5:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    scheduler.step(1.0 - val_f1)\n",
    "\n",
    "# ğŸŸ© Ù…Ø±Ø­Ù„Ù‡ 6: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ (Week 6)\n",
    "# ğŸ”¶ Ú¯Ø§Ù… 6.1: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Test\n",
    "model.load_state_dict(torch.load(f\"{output_dir}best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_probs, all_labels = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        all_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
    "        all_probs.extend(out.cpu().numpy())\n",
    "        all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "# ğŸ”¸ 2. Ø°Ø®ÛŒØ±Ù‡ all_preds, all_probs, all_labels\n",
    "np.save(f\"{output_dir}all_preds.npy\", np.array(all_preds))\n",
    "np.save(f\"{output_dir}all_probs.npy\", np.array(all_probs))\n",
    "np.save(f\"{output_dir}all_labels.npy\", np.array(all_labels))\n",
    "\n",
    "# ğŸ”¶ Ú¯Ø§Ù… 6.2: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "macro_precision = precision_score(all_labels, all_preds, average='macro')\n",
    "weighted_precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "macro_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "weighted_recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "weighted_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(\"ğŸ“Š Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n",
    "\n",
    "with open(f\"{output_dir}classification_report.txt\", \"w\") as f:\n",
    "    f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "    f.write(f\"Macro-Precision: {macro_precision:.4f}\\n\")\n",
    "    f.write(f\"Weighted-Precision: {weighted_precision:.4f}\\n\")\n",
    "    f.write(f\"Macro-Recall: {macro_recall:.4f}\\n\")\n",
    "    f.write(f\"Weighted-Recall: {weighted_recall:.4f}\\n\")\n",
    "    f.write(f\"Macro-F1: {macro_f1:.4f}\\n\")\n",
    "    f.write(f\"Weighted-F1: {weighted_f1:.4f}\\n\")\n",
    "\n",
    "# ğŸ”¸ 3. Ù†Ù…Ø§ÛŒØ´ Ø±ÙˆÙ†Ø¯ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† epoch\n",
    "print(f\"Best validation F1: {best_val_f1:.4f} (epoch {np.argmax(val_f1_list)+1})\")\n",
    "\n",
    "# âœ… Ù…Ø±Ø­Ù„Ù‡ 2: Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§\n",
    "# Ú¯Ø²Ø§Ø±Ø´ Ø¯Ù‚ÛŒÙ‚\n",
    "report_dict = classification_report(all_labels, all_preds, output_dict=True)\n",
    "\n",
    "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù‚Ø§Ø¯ÛŒØ±\n",
    "classes = [str(i) for i in range(num_classes)]\n",
    "precision_vals = [report_dict[c]['precision'] for c in classes]\n",
    "recall_vals = [report_dict[c]['recall'] for c in classes]\n",
    "f1_vals = [report_dict[c]['f1-score'] for c in classes]\n",
    "support_vals = [report_dict[c]['support'] for c in classes]\n",
    "\n",
    "# Ø³Ø§Ø®Øª DataFrame Ø¨Ø±Ø§ÛŒ Ù¾Ù„Ø§Øªâ€ŒÙ‡Ø§\n",
    "df_metrics = pd.DataFrame({\n",
    "    'Class': classes,\n",
    "    'Precision': precision_vals,\n",
    "    'Recall': recall_vals,\n",
    "    'F1-Score': f1_vals,\n",
    "    'Support': support_vals\n",
    "})\n",
    "\n",
    "# Bar plot for precision, recall, f1\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_metrics.set_index('Class')[['Precision', 'Recall', 'F1-Score']].plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Per-Class Evaluation Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}per_class_metrics_bar.png\")\n",
    "plt.close()\n",
    "\n",
    "# Plot class support\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x='Class', y='Support', data=df_metrics, palette='pastel')\n",
    "plt.title(\"Number of Samples per Class (Support)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}class_support.png\")\n",
    "plt.close()\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=range(num_classes))\n",
    "disp.plot(cmap='Blues', values_format=\".2f\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.savefig(f\"{output_dir}normalized_confusion_matrix.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ğŸ”¶ Ú¯Ø§Ù… 6.3: Ø±Ø³Ù… Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ (Ù‚Ø¨Ù„ÛŒ)\n",
    "# Ù†Ù…ÙˆØ¯Ø§Ø± F1 Ùˆ Accuracy Ø¯Ø± Ø·ÙˆÙ„ Ø²Ù…Ø§Ù†\n",
    "plt.figure()\n",
    "plt.plot(val_f1_list, label=\"F1 (macro)\")\n",
    "plt.plot(val_acc_list, label=\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Validation Scores per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(f\"{output_dir}f1_curve.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Ù†Ù…ÙˆØ¯Ø§Ø± ROC-AUC\n",
    "y_bin = label_binarize(all_labels, classes=range(num_classes))\n",
    "probs = np.array(all_probs)\n",
    "\n",
    "plt.figure()\n",
    "for i in range(num_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_bin[:, i], probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"Class {i} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (One-vs-Rest)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(f\"{output_dir}roc_curve.png\", bbox_inches='tight')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio-ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
