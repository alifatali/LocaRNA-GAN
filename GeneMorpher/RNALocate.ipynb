{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0165eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef, accuracy_score, roc_auc_score, average_precision_score, f1_score, precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# تنظیم دستگاه (GPU یا CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"دستگاه: {device}\")\n",
    "\n",
    "# تابع برای Data Augmentation (تغییر تصادفی نوکلئوتیدها)\n",
    "def augment_sequence(seq, mutation_rate=0.03):\n",
    "    nucleotides = ['A', 'C', 'G', 'T']\n",
    "    seq = list(seq)\n",
    "    for i in range(len(seq)):\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            seq[i] = np.random.choice([n for n in nucleotides if n != seq[i]])\n",
    "    return ''.join(seq)\n",
    "\n",
    "# تابع برای Mixup\n",
    "def mixup_data(tfidf, tfidf_perm, distance, distance_perm, ssf, ssf_perm, pcp, pcp_perm, labels, alpha=0.4):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    tfidf_mixed = lam * tfidf + (1 - lam) * tfidf_perm\n",
    "    distance_mixed = lam * distance + (1 - lam) * distance_perm\n",
    "    ssf_mixed = lam * ssf + (1 - lam) * ssf_perm\n",
    "    pcp_mixed = lam * pcp + (1 - lam) * pcp_perm\n",
    "    labels_mixed = labels\n",
    "    return tfidf_mixed, distance_mixed, ssf_mixed, pcp_mixed, labels_mixed\n",
    "\n",
    "# تابع برای استخراج ویژگی‌های TF (k-mer)\n",
    "def extract_tf_features(fasta_files, k=5, augment=True):\n",
    "    kmer_dict = {''.join(n): i for i, n in enumerate(product('ACGT', repeat=k))}\n",
    "    features = []\n",
    "    labels = []\n",
    "    sequences = []\n",
    "    location_to_label = {\n",
    "        'Cytoplasm': 0,\n",
    "        'Endoplasmic_reticulum': 1,\n",
    "        'Extracellular_region': 2,\n",
    "        'Mitochondria': 3,\n",
    "        'Nucleus': 4\n",
    "    }\n",
    "\n",
    "    for fasta_file in tqdm(fasta_files, desc=\"استخراج ویژگی‌های TF\"):\n",
    "        file_name = os.path.basename(fasta_file)\n",
    "        parts = file_name.split('_')\n",
    "        if 'reticulum' in parts or 'region' in parts:\n",
    "            location = parts[0] + '_' + parts[1]\n",
    "        else:\n",
    "            location = parts[0]\n",
    "        label = location_to_label[location]\n",
    "\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            seq = str(record.seq)\n",
    "            kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "            kmer_count = Counter(kmers)\n",
    "            feature_vector = np.zeros(len(kmer_dict))\n",
    "            for kmer, count in kmer_count.items():\n",
    "                if kmer in kmer_dict:\n",
    "                    feature_vector[kmer_dict[kmer]] = count / len(kmers)\n",
    "            features.append(feature_vector)\n",
    "            labels.append(label)\n",
    "            sequences.append(seq)\n",
    "\n",
    "            if augment:\n",
    "                augmented_seq = augment_sequence(seq)\n",
    "                kmers = [augmented_seq[i:i+k] for i in range(len(augmented_seq)-k+1)]\n",
    "                kmer_count = Counter(kmers)\n",
    "                feature_vector = np.zeros(len(kmer_dict))\n",
    "                for kmer, count in kmer_count.items():\n",
    "                    if kmer in kmer_dict:\n",
    "                        feature_vector[kmer_dict[kmer]] = count / len(kmers)\n",
    "                features.append(feature_vector)\n",
    "                labels.append(label)\n",
    "                sequences.append(augmented_seq)\n",
    "    \n",
    "    return np.array(features), np.array(labels), sequences\n",
    "\n",
    "# تابع برای استخراج Distance-based Subsequence Profiles\n",
    "def extract_distance_based_features(sequences, max_k=12):\n",
    "    patterns = [''.join(p) for p in product('ACGT', repeat=2)]\n",
    "    features = []\n",
    "    \n",
    "    for seq in tqdm(sequences, desc=\"استخراج ویژگی‌های Distance-based\"):\n",
    "        feature_vector = np.zeros((max_k + 1) * len(patterns))\n",
    "        for k in range(max_k + 1):\n",
    "            for i, pattern in enumerate(patterns):\n",
    "                count = 0\n",
    "                for j in range(len(seq) - k - 2):\n",
    "                    if seq[j] == pattern[0] and seq[j + k + 1] == pattern[1]:\n",
    "                        count += 1\n",
    "                feature_vector[k * len(patterns) + i] = count / (len(seq) - k - 1) if len(seq) - k - 1 > 0 else 0\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# تابع برای استخراج ویژگی‌های ساختار ثانویه\n",
    "def extract_ssf_features(sequences):\n",
    "    features = []\n",
    "    \n",
    "    for seq in tqdm(sequences, desc=\"استخراج ویژگی‌های SSF\"):\n",
    "        seq_len = len(seq)\n",
    "        gc_content = (seq.count('G') + seq.count('C')) / seq_len if seq_len > 0 else 0\n",
    "        au_pairs = sum(1 for i in range(seq_len-1) if (seq[i] == 'A' and seq[i+1] == 'U') or (seq[i] == 'U' and seq[i+1] == 'A'))\n",
    "        gc_pairs = sum(1 for i in range(seq_len-1) if (seq[i] == 'G' and seq[i+1] == 'C') or (seq[i] == 'C' and seq[i+1] == 'G'))\n",
    "        loop_count = sum(1 for i in range(seq_len-2) if seq[i:i+3] in ['AAA', 'UUU', 'AAU', 'UUA'])\n",
    "        hairpin_count = sum(1 for i in range(seq_len-4) if seq[i:i+5] in ['AUGCA', 'UACGU'])\n",
    "        free_energy = -1.0 * gc_pairs - 0.5 * au_pairs\n",
    "        \n",
    "        feature_vector = np.array([\n",
    "            gc_content,\n",
    "            au_pairs / seq_len if seq_len > 0 else 0,\n",
    "            gc_pairs / seq_len if seq_len > 0 else 0,\n",
    "            loop_count / seq_len if seq_len > 0 else 0,\n",
    "            hairpin_count / seq_len if seq_len > 0 else 0,\n",
    "            free_energy / seq_len if seq_len > 0 else 0\n",
    "        ])\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# تابع برای استخراج ویژگی‌های فیزیکوشیمیایی\n",
    "def extract_pcp_features(sequences, max_length=1000):\n",
    "    properties = {\n",
    "        'A': [0.62, 0.3, -0.5],\n",
    "        'C': [0.29, 0.4, -0.2],\n",
    "        'G': [0.48, 0.5, -0.3],\n",
    "        'T': [0.58, 0.2, -0.1],\n",
    "        'U': [0.58, 0.2, -0.1],\n",
    "        'N': [0.0, 0.0, 0.0]\n",
    "    }\n",
    "    \n",
    "    features = []\n",
    "    for seq in tqdm(sequences, desc=\"استخراج ویژگی‌های PCP\"):\n",
    "        prop_matrix = np.zeros((max_length, 3))\n",
    "        for i in range(min(len(seq), max_length)):\n",
    "            prop_matrix[i] = properties.get(seq[i], [0.0, 0.0, 0.0])\n",
    "        mean_props = prop_matrix.mean(axis=0)\n",
    "        std_props = prop_matrix.std(axis=0)\n",
    "        feature_vector = np.concatenate([mean_props, std_props])\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# تعریف شاخه ویژگی با Residual Connections\n",
    "class FeatureBranch(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(FeatureBranch, self).__init__()\n",
    "        self.conv1d_3 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv1d_5 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=128, num_heads=8)\n",
    "        self.norm = nn.LayerNorm(128)\n",
    "        self.linear = nn.Linear(128, output_dim)\n",
    "        self.residual_fc = nn.Linear(input_dim, 128)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = self.residual_fc(x).unsqueeze(2)\n",
    "        x = x.unsqueeze(2)\n",
    "        x1 = self.conv1d_3(x)\n",
    "        x2 = self.conv1d_5(x)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = x + residual\n",
    "        x = x.permute(2, 0, 1)\n",
    "        x, _ = self.mha(x, x, x)\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# تعریف مدل کامل\n",
    "class RNALocateModel(nn.Module):\n",
    "    def __init__(self, tfidf_dim=512, distance_dim=208, ssf_dim=6, pcp_dim=6, linear_dim=256, num_classes=5):\n",
    "        super(RNALocateModel, self).__init__()\n",
    "        self.tfidf_branch = FeatureBranch(tfidf_dim, linear_dim)\n",
    "        self.distance_branch = FeatureBranch(distance_dim, linear_dim)\n",
    "        self.ssf_branch = FeatureBranch(ssf_dim, linear_dim)\n",
    "        self.pcp_branch = FeatureBranch(pcp_dim, linear_dim)\n",
    "        self.fc = nn.Linear(linear_dim * 4, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, tfidf, distance, ssf, pcp):\n",
    "        tfidf_out = self.tfidf_branch(tfidf)\n",
    "        distance_out = self.distance_branch(distance)\n",
    "        ssf_out = self.ssf_branch(ssf)\n",
    "        pcp_out = self.pcp_branch(pcp)\n",
    "        combined = torch.cat((tfidf_out, distance_out, ssf_out, pcp_out), dim=1)\n",
    "        fc_out = F.relu(self.fc(combined))\n",
    "        fc_out = self.dropout(fc_out)\n",
    "        final_out = self.output(fc_out)\n",
    "        return final_out, fc_out\n",
    "\n",
    "# تعریف Generator برای WGAN-GP\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=128, label_dim=5, output_dim=256+208+6+6):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim + label_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, labels):\n",
    "        input = torch.cat((noise, labels), dim=1)\n",
    "        return self.model(input)\n",
    "\n",
    "# تعریف Discriminator برای WGAN-GP\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=256+208+6+6, label_dim=5):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + label_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features, labels):\n",
    "        input = torch.cat((features, labels), dim=1)\n",
    "        return self.model(input)\n",
    "\n",
    "# محاسبه گرادیان پنالتی برای WGAN-GP\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, labels, device):\n",
    "    alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "    alpha = alpha.expand_as(real_samples)\n",
    "    interpolates = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "    interpolates = interpolates.requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates, labels)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(d_interpolates).to(device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# محاسبه وزن کلاس‌ها با Label Smoothing\n",
    "def compute_class_weights(labels, smoothing=0.05):\n",
    "    class_counts = np.bincount(labels)\n",
    "    n_classes = len(class_counts)\n",
    "    n_samples = len(labels)\n",
    "    weights = n_samples / (n_classes * class_counts)\n",
    "    weights = (1 - smoothing) * weights + smoothing / n_classes\n",
    "    return torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# آموزش WGAN-GP\n",
    "def train_wgan_gp(generator, discriminator, train_loader, num_epochs_gan, device, noise_dim=100, n_critic=10, lambda_gp=10):\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs_gan), desc=\"آموزش WGAN-GP\"):\n",
    "        for i, (tfidf, distance, ssf, pcp, labels) in enumerate(train_loader):\n",
    "            tfidf, distance, ssf, pcp, labels = tfidf.to(device), distance.to(device), ssf.to(device), pcp.to(device), labels.to(device)\n",
    "            features = torch.cat([tfidf, distance, ssf, pcp], dim=1)\n",
    "            batch_size = tfidf.size(0)\n",
    "            \n",
    "            # آموزش Discriminator\n",
    "            for _ in range(n_critic):\n",
    "                d_optimizer.zero_grad()\n",
    "                noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "                fake_features = generator(noise, F.one_hot(labels, num_classes=5).float())\n",
    "                real_output = discriminator(features, F.one_hot(labels, num_classes=5).float())\n",
    "                fake_output = discriminator(fake_features.detach(), F.one_hot(labels, num_classes=5).float())\n",
    "                gradient_penalty = compute_gradient_penalty(\n",
    "                    discriminator, features, fake_features.detach(), F.one_hot(labels, num_classes=5).float(), device\n",
    "                )\n",
    "                d_loss = -torch.mean(real_output) + torch.mean(fake_output) + lambda_gp * gradient_penalty\n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "            \n",
    "            # آموزش Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "            fake_features = generator(noise, F.one_hot(labels, num_classes=5).float())\n",
    "            fake_output = discriminator(fake_features, F.one_hot(labels, num_classes=5).float())\n",
    "            g_loss = -torch.mean(fake_output)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs_gan}, D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "    \n",
    "    return generator, discriminator\n",
    "\n",
    "# تولید داده‌های مصنوعی با WGAN-GP\n",
    "def generate_synthetic_data(generator, num_samples, labels, noise_dim=100):\n",
    "    noise = torch.randn(num_samples, noise_dim).to(device)\n",
    "    labels_one_hot = F.one_hot(torch.tensor(labels), num_classes=5).float().to(device)\n",
    "    synthetic_features = generator(noise, labels_one_hot)\n",
    "    synthetic_features = synthetic_features.detach().cpu().numpy()\n",
    "    synthetic_tfidf = synthetic_features[:, :256]\n",
    "    synthetic_distance = synthetic_features[:, 256:256+208]\n",
    "    synthetic_ssf = synthetic_features[:, 256+208:256+208+6]\n",
    "    synthetic_pcp = synthetic_features[:, 256+208+6:]\n",
    "\n",
    "    return synthetic_tfidf, synthetic_distance, synthetic_ssf, synthetic_pcp, labels\n",
    "\n",
    "# تابع آموزش مدل PyTorch\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device, checkpoint_path, class_weights, patience=7):\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.05)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-4)\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"آموزش مدل PyTorch\"):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for tfidf, distance, ssf, pcp, labels in train_loader:\n",
    "            tfidf, distance, ssf, pcp, labels = tfidf.to(device), distance.to(device), ssf.to(device), pcp.to(device), labels.to(device)\n",
    "            if np.random.rand() < 0.7:\n",
    "                idx = torch.randperm(tfidf.size(0))\n",
    "                tfidf_mixed, distance_mixed, ssf_mixed, pcp_mixed, labels_mixed = mixup_data(\n",
    "                    tfidf, tfidf[idx], distance, distance[idx], ssf, ssf[idx], pcp, pcp[idx], labels, alpha=0.6\n",
    "                )\n",
    "                tfidf, distance, ssf, pcp, labels = tfidf_mixed, distance_mixed, ssf_mixed, pcp_mixed, labels_mixed\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(tfidf, distance, ssf, pcp)\n",
    "            labels = labels.long()\n",
    "            if labels.ndim > 1:\n",
    "                labels = labels.argmax(dim=1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss_epoch = train_loss / len(train_loader)\n",
    "        train_acc_epoch = train_correct / train_total\n",
    "        train_losses.append(train_loss_epoch)\n",
    "        train_accuracies.append(train_acc_epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for tfidf, distance, ssf, pcp, labels in val_loader:\n",
    "                tfidf, distance, ssf, pcp, labels = tfidf.to(device), distance.to(device), ssf.to(device), pcp.to(device), labels.to(device)\n",
    "                outputs, _ = model(tfidf, distance, ssf, pcp)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                if labels.ndim > 1:\n",
    "                    labels = labels.argmax(dim=1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        val_accuracies.append(val_acc)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss_epoch:.4f}, Train Accuracy: {train_acc_epoch:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"بهترین مدل ذخیره شد: {checkpoint_path}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"صبر: {patience_counter}/{patience}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early Stopping: توقف آموزش به دلیل عدم بهبود در دقت اعتبارسنجی\")\n",
    "            break\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'epoch': range(1, len(train_losses) + 1),\n",
    "        'train_loss': train_losses,\n",
    "        'train_accuracy': train_accuracies,\n",
    "        'val_accuracy': val_accuracies\n",
    "    })\n",
    "    metrics_df.to_csv(os.path.join(os.path.dirname(checkpoint_path), 'training_metrics.csv'), index=False)\n",
    "    print(f\"متریک‌های آموزش ذخیره شد: {os.path.join(os.path.dirname(checkpoint_path), 'training_metrics.csv')}\")\n",
    "    \n",
    "    return train_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# تابع برای استخراج ویژگی‌ها از مدل PyTorch\n",
    "def extract_features(model, data_loader, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for tfidf, distance, ssf, pcp, labels in tqdm(data_loader, desc=\"استخراج ویژگی‌ها\"):\n",
    "        tfidf, distance, ssf, pcp, labels = tfidf.to(device), distance.to(device), ssf.to(device), pcp.to(device), labels.to(device)\n",
    "        _, fc_out = model(tfidf, distance, ssf, pcp)\n",
    "        features.append(fc_out.detach().cpu().numpy())\n",
    "        labels_list.append(labels.cpu().numpy())\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels_list, axis=0)\n",
    "    return features, labels\n",
    "\n",
    "# مسیر ذخیره‌سازی فایل‌ها\n",
    "output_dir = \"F:\\\\payan-nameh\\\\Optimized_version2\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# فایل‌های FASTA\n",
    "fasta_files = [\n",
    "    \"F:\\\\New Version\\\\Data\\\\rnalocate\\\\Cytoplasm_train.fasta\",\n",
    "    \"F:\\\\New Version\\\\Data\\\\rnalocate\\\\Endoplasmic_reticulum_train.fasta\",\n",
    "    \"F:\\\\New Version\\\\Data\\\\rnalocate\\\\Extracellular_region_train.fasta\",\n",
    "    \"F:\\\\New Version\\\\Data\\\\rnalocate\\\\Mitochondria_train.fasta\",\n",
    "    \"F:\\\\New Version\\\\Data\\\\rnalocate\\\\Nucleus_train.fasta\"\n",
    "]\n",
    "\n",
    "# استخراج ویژگی‌ها\n",
    "tf_features, y, sequences = extract_tf_features(fasta_files, k=5, augment=True)\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_features = tfidf.fit_transform(tf_features).toarray()\n",
    "distance_features = extract_distance_based_features(sequences, max_k=12)\n",
    "ssf_features = extract_ssf_features(sequences)\n",
    "pcp_features = extract_pcp_features(sequences, max_length=1000)\n",
    "\n",
    "print(\"تعداد نمونه‌ها در TF-IDF:\", tfidf_features.shape[0])\n",
    "print(\"تعداد نمونه‌ها در Distance-based:\", distance_features.shape[0])\n",
    "print(\"تعداد نمونه‌ها در SSF:\", ssf_features.shape[0])\n",
    "print(\"تعداد نمونه‌ها در PCP:\", pcp_features.shape[0])\n",
    "\n",
    "# نرمال‌سازی و کاهش ابعاد TF-IDF\n",
    "scaler_tfidf = StandardScaler()\n",
    "scaler_distance = StandardScaler()\n",
    "scaler_ssf = StandardScaler()\n",
    "scaler_pcp = StandardScaler()\n",
    "tfidf_features_scaled = scaler_tfidf.fit_transform(tfidf_features)\n",
    "distance_features_scaled = scaler_distance.fit_transform(distance_features)\n",
    "ssf_features_scaled = scaler_ssf.fit_transform(ssf_features)\n",
    "pcp_features_scaled = scaler_pcp.fit_transform(pcp_features)\n",
    "\n",
    "pca = PCA(n_components=256, random_state=42)\n",
    "tfidf_features_scaled = pca.fit_transform(tfidf_features_scaled)\n",
    "\n",
    "joblib.dump(scaler_tfidf, os.path.join(output_dir, 'scaler_tfidf.pkl'))\n",
    "joblib.dump(scaler_distance, os.path.join(output_dir, 'scaler_distance.pkl'))\n",
    "joblib.dump(scaler_ssf, os.path.join(output_dir, 'scaler_ssf.pkl'))\n",
    "joblib.dump(scaler_pcp, os.path.join(output_dir, 'scaler_pcp.pkl'))\n",
    "joblib.dump(pca, os.path.join(output_dir, 'pca_tfidf.pkl'))\n",
    "print(\"اسکیلرها و PCA ذخیره شدند.\")\n",
    "\n",
    "# تقسیم داده‌ها\n",
    "X_tfidf_train, X_tfidf_test, X_distance_train, X_distance_test, X_ssf_train, X_ssf_test, X_pcp_train, X_pcp_test, y_train, y_test = train_test_split(\n",
    "    tfidf_features_scaled, distance_features_scaled, ssf_features_scaled, pcp_features_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"تعداد نمونه‌ها در train:\", X_tfidf_train.shape[0])\n",
    "print(\"تعداد نمونه‌ها در test:\", X_tfidf_test.shape[0])\n",
    "print(\"تعداد نمونه‌ها در هر کلاس (train):\", np.bincount(y_train))\n",
    "print(\"تعداد نمونه‌ها در هر کلاس (test):\", np.bincount(y_test))\n",
    "\n",
    "# آموزش WGAN-GP\n",
    "generator = Generator(noise_dim=100, label_dim=5, output_dim=256+208+6+6).to(device)\n",
    "discriminator = Discriminator(input_dim=256+208+6+6, label_dim=5).to(device)\n",
    "train_dataset_temp = TensorDataset(\n",
    "    torch.tensor(X_tfidf_train, dtype=torch.float32),\n",
    "    torch.tensor(X_distance_train, dtype=torch.float32),\n",
    "    torch.tensor(X_ssf_train, dtype=torch.float32),\n",
    "    torch.tensor(X_pcp_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.long)\n",
    ")\n",
    "train_loader_temp = DataLoader(train_dataset_temp, batch_size=32, shuffle=True)\n",
    "generator, discriminator = train_wgan_gp(generator, discriminator, train_loader_temp, num_epochs_gan=200, device=device)\n",
    "\n",
    "# تولید داده‌های مصنوعی\n",
    "synthetic_tfidf, synthetic_distance, synthetic_ssf, synthetic_pcp, synthetic_labels = generate_synthetic_data(\n",
    "    generator, num_samples=4000, labels=[1] * 2000 + [2] * 2000\n",
    ")\n",
    "\n",
    "# t-SNE برای چک کیفیت داده‌های مصنوعی\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "combined_features = np.concatenate([X_tfidf_train[:1000], synthetic_tfidf[:1000]], axis=0)\n",
    "tsne_results = tsne.fit_transform(combined_features)\n",
    "plt.scatter(tsne_results[:1000, 0], tsne_results[:1000, 1], c='blue', label='Real')\n",
    "plt.scatter(tsne_results[1000:, 0], tsne_results[1000:, 1], c='red', label='Synthetic')\n",
    "plt.legend()\n",
    "plt.title('t-SNE of Real vs Synthetic Data')\n",
    "plt.savefig(os.path.join(output_dir, 'tsne_synthetic_data.png'))\n",
    "plt.close()\n",
    "print(f\"t-SNE ذخیره شد: {os.path.join(output_dir, 'tsne_synthetic_data.png')}\")\n",
    "\n",
    "# ترکیب داده‌های مصنوعی با داده‌های واقعی\n",
    "X_tfidf_train = np.concatenate([X_tfidf_train, synthetic_tfidf], axis=0)\n",
    "X_distance_train = np.concatenate([X_distance_train, synthetic_distance], axis=0)\n",
    "X_ssf_train = np.concatenate([X_ssf_train, synthetic_ssf], axis=0)\n",
    "X_pcp_train = np.concatenate([X_pcp_train, synthetic_pcp], axis=0)\n",
    "y_train = np.concatenate([y_train, synthetic_labels], axis=0)\n",
    "\n",
    "# اعمال RandomOverSampler\n",
    "sampling_strategy = {0: 8496, 1: 5000, 2: 4000, 3: 2500, 4: 7768}\n",
    "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_train_combined = np.concatenate([X_tfidf_train, X_distance_train, X_ssf_train, X_pcp_train], axis=1)\n",
    "X_train_combined_balanced, y_train_balanced = ros.fit_resample(X_train_combined, y_train)\n",
    "X_tfidf_train_balanced = X_train_combined_balanced[:, :X_tfidf_train.shape[1]]\n",
    "X_distance_train_balanced = X_train_combined_balanced[:, X_tfidf_train.shape[1]:X_tfidf_train.shape[1]+X_distance_train.shape[1]]\n",
    "X_ssf_train_balanced = X_train_combined_balanced[:, X_tfidf_train.shape[1]+X_distance_train.shape[1]:X_tfidf_train.shape[1]+X_distance_train.shape[1]+X_ssf_train.shape[1]]\n",
    "X_pcp_train_balanced = X_train_combined_balanced[:, X_tfidf_train.shape[1]+X_distance_train.shape[1]+X_ssf_train.shape[1]:]\n",
    "\n",
    "print(\"شکل y_train_balanced:\", y_train_balanced.shape)\n",
    "if y_train_balanced.ndim > 1:\n",
    "    y_train_balanced = np.argmax(y_train_balanced, axis=1)\n",
    "    print(\"y_train_balanced به بردار یک‌بعدی تبدیل شد:\", y_train_balanced.shape)\n",
    "\n",
    "print(\"تعداد نمونه‌ها در هر کلاس بعد از RandomOverSampler (train):\", np.bincount(y_train_balanced))\n",
    "print(\"شکل داده‌های متعادل‌شده TF-IDF (train):\", X_tfidf_train_balanced.shape)\n",
    "print(\"شکل داده‌های متعادل‌شده Distance-based (train):\", X_distance_train_balanced.shape)\n",
    "print(\"شکل داده‌های متعادل‌شده SSF (train):\", X_ssf_train_balanced.shape)\n",
    "print(\"شکل داده‌های متعادل‌شده PCP (train):\", X_pcp_train_balanced.shape)\n",
    "\n",
    "# تبدیل داده‌ها به Tensor\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_tfidf_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(X_distance_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(X_ssf_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(X_pcp_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(y_train_balanced, dtype=torch.long)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(X_tfidf_test, dtype=torch.float32),\n",
    "    torch.tensor(X_distance_test, dtype=torch.float32),\n",
    "    torch.tensor(X_ssf_test, dtype=torch.float32),\n",
    "    torch.tensor(X_pcp_test, dtype=torch.float32),\n",
    "    torch.tensor(y_test, dtype=torch.long)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# محاسبه وزن کلاس‌ها\n",
    "class_weights = torch.tensor([1.0, 2.8, 3.5, 1.0, 1.0]).to(device)\n",
    "\n",
    "# آموزش مدل PyTorch\n",
    "model = RNALocateModel(tfidf_dim=256).to(device)\n",
    "train_losses, train_accuracies, val_accuracies = train_model(\n",
    "    model, train_loader, test_loader, num_epochs=100, device=device,\n",
    "    checkpoint_path=os.path.join(output_dir, 'best_pytorch_model.pth'), class_weights=class_weights\n",
    ")\n",
    "\n",
    "# بارگذاری بهترین مدل\n",
    "model.load_state_dict(torch.load(os.path.join(output_dir, 'best_pytorch_model.pth')))\n",
    "print(\"بهترین مدل PyTorch بارگذاری شد.\")\n",
    "\n",
    "# استخراج ویژگی‌ها\n",
    "X_train_features, y_train_features = extract_features(model, train_loader, device)\n",
    "X_test_features, y_test_features = extract_features(model, test_loader, device)\n",
    "\n",
    "np.save(os.path.join(output_dir, 'X_test_features.npy'), X_test_features)\n",
    "np.save(os.path.join(output_dir, 'y_test_features.npy'), y_test_features)\n",
    "print(\"ویژگی‌های تست ذخیره شدند.\")\n",
    "\n",
    "print(\"شکل ویژگی‌های استخراج‌شده (train):\", X_train_features.shape)\n",
    "print(\"شکل ویژگی‌های استخراج‌شده (test):\", X_test_features.shape)\n",
    "\n",
    "# بهینه‌سازی SVM\n",
    "param_grid = {\n",
    "    'C': [10, 50, 100, 200],\n",
    "    'kernel': ['rbf'],\n",
    "    'gamma': [0.01, 0.05, 0.1]\n",
    "}\n",
    "svm_model = SVC(probability=True, random_state=42)\n",
    "grid = GridSearchCV(svm_model, param_grid, cv=15, scoring='f1_macro', n_jobs=4)\n",
    "grid.fit(X_train_features, y_train_features)\n",
    "\n",
    "print(\"بهترین پارامترها:\", grid.best_params_)\n",
    "print(\"بهترین F1-score در GridSearch:\", grid.best_score_)\n",
    "\n",
    "best_svm = grid.best_estimator_\n",
    "\n",
    "# ارزیابی مدل روی داده آموزش\n",
    "y_train_pred = cross_val_predict(best_svm, X_train_features, y_train_features, cv=10)\n",
    "f1_macro_train = cross_val_score(best_svm, X_train_features, y_train_features, cv=10, scoring='f1_macro').mean()\n",
    "accuracy_train = accuracy_score(y_train_features, y_train_pred)\n",
    "precision_train = precision_score(y_train_features, y_train_pred, average='macro')\n",
    "recall_train = recall_score(y_train_features, y_train_pred, average='macro')\n",
    "mcc_train = matthews_corrcoef(y_train_features, y_train_pred)\n",
    "\n",
    "print(\"\\nمعیارهای کلی روی داده آموزش (cross-validation):\")\n",
    "print(f\"F1-score (ماکرو): {f1_macro_train:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"Precision: {precision_train:.4f}\")\n",
    "print(f\"Recall: {recall_train:.4f}\")\n",
    "print(f\"MCC: {mcc_train:.4f}\")\n",
    "\n",
    "class_names = ['Cytoplasm', 'Endoplasmic_reticulum', 'Extracellular_region', 'Mitochondria', 'Nucleus']\n",
    "print(\"\\nگزارش معیارها برای هر کلاس (train):\")\n",
    "print(classification_report(y_train_features, y_train_pred, target_names=class_names))\n",
    "\n",
    "# ارزیابی مدل روی داده تست\n",
    "y_test_pred = best_svm.predict(X_test_features)\n",
    "y_test_pred_proba = best_svm.predict_proba(X_test_features)\n",
    "f1_macro_test = f1_score(y_test_features, y_test_pred, average='macro')\n",
    "f1_weighted_test = f1_score(y_test_features, y_test_pred, average='weighted')\n",
    "accuracy_test = accuracy_score(y_test_features, y_test_pred)\n",
    "precision_test = precision_score(y_test_features, y_test_pred, average='macro')\n",
    "recall_test = recall_score(y_test_features, y_test_pred, average='macro')\n",
    "mcc_test = matthews_corrcoef(y_test_features, y_test_pred)\n",
    "auc_roc_test = roc_auc_score(label_binarize(y_test_features, classes=[0,1,2,3,4]), y_test_pred_proba, average='macro')\n",
    "auc_pr_test = average_precision_score(label_binarize(y_test_features, classes=[0,1,2,3,4]), y_test_pred_proba, average='weighted')\n",
    "\n",
    "test_metrics = {\n",
    "    'f1_macro': f1_macro_test,\n",
    "    'f1_weighted': f1_weighted_test,\n",
    "    'accuracy': accuracy_test,\n",
    "    'precision': precision_test,\n",
    "    'recall': recall_test,\n",
    "    'mcc': mcc_test,\n",
    "    'auc_roc': auc_roc_test,\n",
    "    'auc_pr': auc_pr_test\n",
    "}\n",
    "test_metrics_df = pd.DataFrame([test_metrics])\n",
    "test_metrics_df.to_csv(os.path.join(output_dir, 'test_metrics.csv'), index=False)\n",
    "print(f\"متریک‌های تست ذخیره شد: {os.path.join(output_dir, 'test_metrics.csv')}\")\n",
    "\n",
    "print(\"\\nمعیارهای کلی روی داده تست:\")\n",
    "print(f\"F1-score (ماکرو): {f1_macro_test:.4f}\")\n",
    "print(f\"F1-score (وزنی): {f1_weighted_test:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall: {recall_test:.4f}\")\n",
    "print(f\"MCC: {mcc_test:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc_test:.4f}\")\n",
    "print(f\"AUC-PR: {auc_pr_test:.4f}\")\n",
    "\n",
    "print(\"\\nگزارش معیارها برای هر کلاس (test):\")\n",
    "print(classification_report(y_test_features, y_test_pred, target_names=class_names))\n",
    "\n",
    "# محاسبه و ذخیره Confusion Matrix\n",
    "cm = confusion_matrix(y_test_features, y_test_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "plt.close()\n",
    "print(f\"Confusion Matrix ذخیره شد: {os.path.join(output_dir, 'confusion_matrix.png')}\")\n",
    "\n",
    "# محاسبه و ذخیره ROC و PR curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "label_binarized = label_binarize(y_test_features, classes=[0, 1, 2, 3, 4])\n",
    "for i, class_name in enumerate(class_names):\n",
    "    fpr, tpr, _ = roc_curve(label_binarized[:, i], y_test_pred_proba[:, i])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, label=f'{class_name} (AUC={roc_auc_score(label_binarized[:, i], y_test_pred_proba[:, i]):.2f})')\n",
    "    precision, recall, _ = precision_recall_curve(label_binarized[:, i], y_test_pred_proba[:, i])\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall, precision, label=f'{class_name} (AUC-PR={average_precision_score(label_binarized[:, i], y_test_pred_proba[:, i]):.2f})')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True')\n",
    "plt.title('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'roc_pr_curves.png'))\n",
    "plt.close()\n",
    "print(f\"ROC و PR Curves ذخیره شد: {os.path.join(output_dir, 'roc_pr_curves.png')}\")\n",
    "\n",
    "# ذخیره مدل\n",
    "joblib.dump(best_svm, os.path.join(output_dir, 'best_svm_model_hybrid.pkl'))\n",
    "print(f\"مدل ذخیره شد: {os.path.join(output_dir, 'best_svm_model_hybrid.pkl')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
