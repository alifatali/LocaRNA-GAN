{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e564e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============Cefra-seq========================\n",
    "import os\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# تابع برای استخراج ویژگی‌های TF (k-mer)\n",
    "def extract_tf_features(sequences, k=5):\n",
    "    kmer_dict = {''.join(n): i for i, n in enumerate(product('ACGT', repeat=k))}\n",
    "    features = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "        kmer_count = Counter(kmers)\n",
    "        feature_vector = np.zeros(len(kmer_dict))\n",
    "        for kmer, count in kmer_count.items():\n",
    "            if kmer in kmer_dict:\n",
    "                feature_vector[kmer_dict[kmer]] = count / len(kmers)\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# تابع برای استخراج Distance-based Subsequence Profiles\n",
    "def extract_distance_based_features(sequences, max_k=8):\n",
    "    patterns = [''.join(p) for p in product('ACGT', repeat=2)]  # 16 ترکیب\n",
    "    features = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        feature_vector = np.zeros((max_k + 1) * len(patterns))  # 9 * 16 = 144\n",
    "        for k in range(max_k + 1):\n",
    "            for i, pattern in enumerate(patterns):\n",
    "                count = 0\n",
    "                for j in range(len(seq) - k - 2):\n",
    "                    if seq[j] == pattern[0] and seq[j + k + 1] == pattern[1]:\n",
    "                        count += 1\n",
    "                feature_vector[k * len(patterns) + i] = count / (len(seq) - k - 1) if len(seq) - k - 1 > 0 else 0\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# فایل FASTA\n",
    "fasta_file = \"F:\\\\New Version\\\\Data\\\\cefra-seq\\\\cefra_seq_cDNA_screened.fa\"\n",
    "\n",
    "# خواندن توالی‌ها و اطلاعات مکان‌یابی\n",
    "sequences = []\n",
    "locations = []\n",
    "\n",
    "for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "    sequences.append(str(record.seq))\n",
    "    description = record.description\n",
    "    dist_str = description.split(\"dist:\")[1]\n",
    "    dist_values = [float(x) for x in dist_str.split(\"_\")]\n",
    "    locations.append(dist_values)\n",
    "\n",
    "locations = np.array(locations)\n",
    "y = np.argmax(locations, axis=1)\n",
    "print(\"تعداد نمونه‌ها در هر کلاس (کل داده‌ها):\", np.bincount(y))\n",
    "\n",
    "# استخراج ویژگی‌های TF\n",
    "tf_features = extract_tf_features(sequences, k=5)\n",
    "\n",
    "# تبدیل TF به TF-IDF\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_features = tfidf.fit_transform(tf_features).toarray()\n",
    "\n",
    "# استخراج Distance-based Subsequence Profiles\n",
    "distance_features = extract_distance_based_features(sequences, max_k=8)\n",
    "\n",
    "# بررسی تعداد نمونه‌ها\n",
    "print(\"تعداد نمونه‌ها در TF-IDF:\", tfidf_features.shape[0])\n",
    "print(\"تعداد نمونه‌ها در Distance-based:\", distance_features.shape[0])\n",
    "\n",
    "# ترکیب ویژگی‌ها (بدون PPI)\n",
    "X = np.hstack((tfidf_features, distance_features))\n",
    "print(\"شکل ماتریس ویژگی‌ها:\", X.shape)\n",
    "\n",
    "# نرمال‌سازی ویژگی‌ها\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# اعمال SMOTE برای تعادل داده‌ها\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_scaled, y)\n",
    "print(\"تعداد نمونه‌ها در هر کلاس قبل از SMOTE:\", np.bincount(y))\n",
    "print(\"تعداد نمونه‌ها در هر کلاس بعد از SMOTE:\", np.bincount(y_balanced))\n",
    "\n",
    "# بهینه‌سازی SVM با Grid Search\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.1, 1]}\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "grid = GridSearchCV(svm, param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "grid.fit(X_balanced, y_balanced)\n",
    "print(\"بهترین پارامترها:\", grid.best_params_)\n",
    "print(\"بهترین F1-score در Grid Search:\", grid.best_score_)\n",
    "\n",
    "# استفاده از بهترین مدل برای ارزیابی\n",
    "best_svm = grid.best_estimator_\n",
    "\n",
    "# پیش‌بینی‌ها با cross-validation\n",
    "y_pred = cross_val_predict(best_svm, X_balanced, y_balanced, cv=10)\n",
    "\n",
    "# محاسبه معیارهای کلی\n",
    "f1_macro = cross_val_score(best_svm, X_balanced, y_balanced, cv=10, scoring='f1_macro').mean()\n",
    "accuracy = accuracy_score(y_balanced, y_pred)\n",
    "mcc = matthews_corrcoef(y_balanced, y_pred)\n",
    "\n",
    "print(\"\\nمعیارهای کلی:\")\n",
    "print(f\"میانگین F1-score (ماکرو) با 10-fold cross-validation: {f1_macro:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"MCC: {mcc:.4f}\")\n",
    "\n",
    "# گزارش معیارها برای هر کلاس\n",
    "class_names = ['Cytosol', 'Nucleus', 'Membrane', 'Insoluble']\n",
    "print(\"\\nگزارش معیارها برای هر کلاس:\")\n",
    "print(classification_report(y_balanced, y_pred, target_names=class_names))\n",
    "\n",
    "# محاسبه MCC برای هر کلاس\n",
    "print(\"\\nMCC برای هر کلاس:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    y_true_binary = (y_balanced == i).astype(int)\n",
    "    y_pred_binary = (y_pred == i).astype(int)\n",
    "    mcc_class = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "    print(f\"{class_name}: {mcc_class:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709a4dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing CeFra-Seq Data phase\n",
      "Number of gene ids: 6845\n",
      "Unique gene ids: 6845\n",
      "Number of all mRNA's: 6845\n",
      "Length of y: 6845\n",
      "چند شناسه ژن اول از CeFra-Seq: ['ENSG00000000003', 'ENSG00000000419', 'ENSG00000001084', 'ENSG00000001167', 'ENSG00000001460']\n",
      "Cytosol: 2086 Nucleus: 2168 Membrane: 211 Insoluble: 2380\n",
      "Shape of X DATA: (6845,)\n",
      "Shape of y DATA: (6845,)\n",
      "تعداد نمونه‌ها در هر کلاس (کل داده‌ها): [2086 2168  211 2380]\n",
      "تعداد نمونه‌ها در TF-IDF: 6845\n",
      "تعداد نمونه‌ها در Distance-based: 6845\n",
      "شکل ماتریس PPI: (11373, 11373)\n",
      "شکل ماتریس PPI بعد از فیلتر کردن: (6845, 6845)\n",
      "شکل ویژگی‌های PPI بعد از PCA: (6845, 500)\n",
      "شکل ماتریس ویژگی‌ها با PPI: (6845, 1668)\n",
      "تعداد نمونه‌ها در هر کلاس بعد از SMOTE (با PPI): [2380 2380 2380 2380]\n",
      "بهترین پارامترها: {'C': 100, 'gamma': 'scale'}\n",
      "بهترین F1-score در Grid Search: 0.745510280451389\n",
      "\n",
      "معیارهای کلی:\n",
      "میانگین F1-score (ماکرو) با 10-fold cross-validation: 0.7512\n",
      "Accuracy: 0.7550\n",
      "MCC: 0.6742\n",
      "\n",
      "گزارش معیارها برای هر کلاس:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Cytosol       0.71      0.78      0.74      2380\n",
      "     Nucleus       0.67      0.69      0.68      2380\n",
      "    Membrane       0.99      1.00      1.00      2380\n",
      "   Insoluble       0.63      0.55      0.59      2380\n",
      "\n",
      "    accuracy                           0.76      9520\n",
      "   macro avg       0.75      0.76      0.75      9520\n",
      "weighted avg       0.75      0.76      0.75      9520\n",
      "\n",
      "\n",
      "MCC برای هر کلاس:\n",
      "Cytosol: 0.6522\n",
      "Nucleus: 0.5731\n",
      "Membrane: 0.9964\n",
      "Insoluble: 0.4673\n"
     ]
    }
   ],
   "source": [
    "#=================Cefra-seq with PPI==================\n",
    "import os\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# تابع پیش‌پردازش داده‌ها\n",
    "def process_cefra_data():\n",
    "    print(\"Start processing CeFra-Seq Data phase\")\n",
    "\n",
    "    # خواندن توالی‌ها و اطلاعات مکان‌یابی\n",
    "    fasta_file = \"F:\\\\New Version\\\\Data\\\\cefra-seq\\\\cefra_seq_cDNA_screened.fa\"\n",
    "    X = []  # توالی‌ها\n",
    "    y = []  # برچسب‌ها\n",
    "    gene_ids = []  # شناسه‌های ژن\n",
    "    all_gene_ids = []  # لیست اولیه ژن‌ها\n",
    "\n",
    "    # ذخیره لیست اولیه ژن‌ها\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        all_gene_ids.append(record.id)\n",
    "\n",
    "    # خواندن دوباره برای فیلتر کردن\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        description = record.description\n",
    "        dist_str = description.split(\"dist:\")[1]\n",
    "        dist_values = [float(x) for x in dist_str.split(\"_\")]\n",
    "        dist_values = np.array(dist_values)\n",
    "        dist_values = dist_values / dist_values.sum()\n",
    "        # بررسی تفاوت بین بیشترین و دومین مقدار\n",
    "        sorted_values = np.sort(dist_values)[::-1]\n",
    "        diff = sorted_values[0] - sorted_values[1]\n",
    "        if diff > 0.1:  # آستانه برای حذف نمونه‌های مبهم\n",
    "            label = int(np.argmax(dist_values))\n",
    "            y.append(label)\n",
    "            gene_ids.append(record.id)\n",
    "            X.append(str(record.seq))\n",
    "\n",
    "    print(\"Number of gene ids:\", len(gene_ids))\n",
    "    new = list(set(gene_ids))\n",
    "    print(\"Unique gene ids:\", len(new))\n",
    "    print(\"Number of all mRNA's:\", len(X))\n",
    "    print(\"Length of y:\", len(y))\n",
    "    print(\"چند شناسه ژن اول از CeFra-Seq:\", gene_ids[:5])\n",
    "\n",
    "    # شمارش تعداد نمونه‌ها در هر کلاس\n",
    "    cytosol = 0\n",
    "    nucleus = 0\n",
    "    membrane = 0\n",
    "    insoluble = 0\n",
    "    for item in y:\n",
    "        if item == 0:\n",
    "            cytosol += 1\n",
    "        elif item == 1:\n",
    "            nucleus += 1\n",
    "        elif item == 2:\n",
    "            membrane += 1\n",
    "        elif item == 3:\n",
    "            insoluble += 1\n",
    "    print(\"Cytosol:\", cytosol, \"Nucleus:\", nucleus, \"Membrane:\", membrane, \"Insoluble:\", insoluble)\n",
    "\n",
    "    # بدون فیلتر کردن برای Homo sapiens\n",
    "    gene_info = [[gen, None, None] for gen in gene_ids]\n",
    "\n",
    "    # تبدیل به آرایه NumPy\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y, dtype=np.int32)\n",
    "    print(\"Shape of X DATA:\", np.shape(X))\n",
    "    print(\"Shape of y DATA:\", np.shape(y))\n",
    "\n",
    "    # ذخیره داده‌ها\n",
    "    with open(\"C:\\\\Users\\\\Ali\\\\Desktop\\\\Grok\\\\cefra_sequences.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(X, fp)\n",
    "    with open(\"C:\\\\Users\\\\Ali\\\\Desktop\\\\Grok\\\\cefra_targets.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(y, fp)\n",
    "    with open(\"C:\\\\Users\\\\Ali\\\\Desktop\\\\Grok\\\\cefra_gene_info.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(gene_info, fp)\n",
    "\n",
    "    return X, y, gene_info, gene_ids, all_gene_ids\n",
    "\n",
    "# تابع برای استخراج ویژگی‌های TF (k-mer)\n",
    "def extract_tf_features(sequences, k=5):\n",
    "    kmer_dict = {''.join(n): i for i, n in enumerate(product('ACGT', repeat=k))}\n",
    "    features = []\n",
    "    for seq in sequences:\n",
    "        kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "        kmer_count = Counter(kmers)\n",
    "        feature_vector = np.zeros(len(kmer_dict))\n",
    "        for kmer, count in kmer_count.items():\n",
    "            if kmer in kmer_dict:\n",
    "                feature_vector[kmer_dict[kmer]] = count / len(kmers)\n",
    "        features.append(feature_vector)\n",
    "    return np.array(features)\n",
    "\n",
    "# تابع برای استخراج Distance-based Subsequence Profiles\n",
    "def extract_distance_based_features(sequences, max_k=8):\n",
    "    patterns = [''.join(p) for p in product('ACGT', repeat=2)]\n",
    "    features = []\n",
    "    for seq in sequences:\n",
    "        feature_vector = np.zeros((max_k + 1) * len(patterns))\n",
    "        for k in range(max_k + 1):\n",
    "            for i, pattern in enumerate(patterns):\n",
    "                count = 0\n",
    "                for j in range(len(seq) - k - 2):\n",
    "                    if seq[j] == pattern[0] and seq[j + k + 1] == pattern[1]:\n",
    "                        count += 1\n",
    "                feature_vector[k * len(patterns) + i] = count / (len(seq) - k - 1) if len(seq) - k - 1 > 0 else 0\n",
    "        features.append(feature_vector)\n",
    "    return np.array(features)\n",
    "\n",
    "# پیش‌پردازش داده‌ها\n",
    "sequences, y, gene_info, gene_ids, all_gene_ids = process_cefra_data()\n",
    "\n",
    "# چاپ تعداد نمونه‌ها در هر کلاس\n",
    "print(\"تعداد نمونه‌ها در هر کلاس (کل داده‌ها):\", np.bincount(y))\n",
    "\n",
    "# بررسی اینکه آیا داده‌ای وجود دارد\n",
    "if len(sequences) == 0:\n",
    "    raise ValueError(\"هیچ داده‌ای وجود ندارد. لطفاً داده‌های ورودی را بررسی کنید.\")\n",
    "\n",
    "# استخراج ویژگی‌های TF\n",
    "tf_features = extract_tf_features(sequences, k=5)\n",
    "\n",
    "# تبدیل TF به TF-IDF\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_features = tfidf.fit_transform(tf_features).toarray()\n",
    "\n",
    "# استخراج Distance-based Subsequence Profiles\n",
    "distance_features = extract_distance_based_features(sequences, max_k=8)\n",
    "\n",
    "# بررسی تعداد نمونه‌ها\n",
    "print(\"تعداد نمونه‌ها در TF-IDF:\", tfidf_features.shape[0])\n",
    "print(\"تعداد نمونه‌ها در Distance-based:\", distance_features.shape[0])\n",
    "\n",
    "# بارگذاری ماتریس PPI\n",
    "ppi_matrix = np.load(\"F:\\\\New Version\\\\PPIdata\\\\ppiMatrixScoress.npy\")\n",
    "print(\"شکل ماتریس PPI:\", ppi_matrix.shape)\n",
    "\n",
    "# فیلتر کردن ماتریس PPI بر اساس ژن‌های باقی‌مانده\n",
    "indices = [all_gene_ids.index(gene_id) for gene_id in gene_ids]\n",
    "ppi_matrix_filtered = ppi_matrix[indices, :][:, indices]\n",
    "print(\"شکل ماتریس PPI بعد از فیلتر کردن:\", ppi_matrix_filtered.shape)\n",
    "\n",
    "# کاهش ابعاد با PCA\n",
    "pca = PCA(n_components=500)\n",
    "ppi_features = pca.fit_transform(ppi_matrix_filtered)\n",
    "print(\"شکل ویژگی‌های PPI بعد از PCA:\", ppi_features.shape)\n",
    "\n",
    "# ترکیب ویژگی‌ها (TF-IDF + Distance-based + PPI)\n",
    "X_with_ppi = np.hstack((tfidf_features, distance_features, ppi_features))\n",
    "print(\"شکل ماتریس ویژگی‌ها با PPI:\", X_with_ppi.shape)\n",
    "\n",
    "# نرمال‌سازی ویژگی‌ها\n",
    "scaler = StandardScaler()\n",
    "X_scaled_with_ppi = scaler.fit_transform(X_with_ppi)\n",
    "\n",
    "# اعمال SMOTE برای تعادل داده‌ها\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced_with_ppi, y_balanced = smote.fit_resample(X_scaled_with_ppi, y)\n",
    "print(\"تعداد نمونه‌ها در هر کلاس بعد از SMOTE (با PPI):\", np.bincount(y_balanced))\n",
    "\n",
    "# بهینه‌سازی SVM با Grid Search\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto', 0.01, 0.1, 1, 10]}\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "grid = GridSearchCV(svm, param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "grid.fit(X_balanced_with_ppi, y_balanced)\n",
    "print(\"بهترین پارامترها:\", grid.best_params_)\n",
    "print(\"بهترین F1-score در Grid Search:\", grid.best_score_)\n",
    "\n",
    "# استفاده از بهترین مدل برای ارزیابی\n",
    "best_svm = grid.best_estimator_\n",
    "\n",
    "# پیش‌بینی‌ها با cross-validation\n",
    "y_pred = cross_val_predict(best_svm, X_balanced_with_ppi, y_balanced, cv=10)\n",
    "\n",
    "# محاسبه معیارهای کلی\n",
    "f1_macro = cross_val_score(best_svm, X_balanced_with_ppi, y_balanced, cv=10, scoring='f1_macro').mean()\n",
    "accuracy = accuracy_score(y_balanced, y_pred)\n",
    "mcc = matthews_corrcoef(y_balanced, y_pred)\n",
    "\n",
    "print(\"\\nمعیارهای کلی:\")\n",
    "print(f\"میانگین F1-score (ماکرو) با 10-fold cross-validation: {f1_macro:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"MCC: {mcc:.4f}\")\n",
    "\n",
    "# گزارش معیارها برای هر کلاس\n",
    "class_names = ['Cytosol', 'Nucleus', 'Membrane', 'Insoluble']\n",
    "print(\"\\nگزارش معیارها برای هر کلاس:\")\n",
    "print(classification_report(y_balanced, y_pred, target_names=class_names))\n",
    "\n",
    "# محاسبه MCC برای هر کلاس\n",
    "print(\"\\nMCC برای هر کلاس:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    y_true_binary = (y_balanced == i).astype(int)\n",
    "    y_pred_binary = (y_pred == i).astype(int)\n",
    "    mcc_class = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "    print(f\"{class_name}: {mcc_class:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f020afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "دستگاه: cuda\n",
      "تعداد نمونه‌ها در هر کلاس (کل داده‌ها): [13408 13836  4320 13928]\n",
      "تعداد نمونه‌ها در TF-IDF: 45492\n",
      "تعداد نمونه‌ها در Distance-based: 45492\n",
      "تعداد نمونه‌ها در SSF: 45492\n",
      "تعداد نمونه‌ها در PCP: 45492\n",
      "تعداد نمونه‌ها در Dist: 45492\n",
      "تعداد نمونه‌ها در train: 36393\n",
      "تعداد نمونه‌ها در test: 9099\n",
      "تعداد نمونه‌ها در هر کلاس (train): [10726 11069  3456 11142]\n",
      "تعداد نمونه‌ها در هر کلاس (test): [2682 2767  864 2786]\n",
      "تعداد نمونه‌ها در هر کلاس بعد از SMOTE (train): [11000 11500  5000 11500]\n",
      "Epoch 1/40, Train Loss: 1.1466, Val Accuracy: 0.4726\n",
      "بهترین مدل ذخیره شد: best_cefra_model.pth\n",
      "Epoch 2/40, Train Loss: 0.9987, Val Accuracy: 0.4335\n",
      "صبر: 1/7\n",
      "Epoch 3/40, Train Loss: 0.9229, Val Accuracy: 0.5472\n",
      "بهترین مدل ذخیره شد: best_cefra_model.pth\n",
      "Epoch 4/40, Train Loss: 0.9015, Val Accuracy: 0.4236\n",
      "صبر: 1/7\n",
      "Epoch 5/40, Train Loss: 0.8673, Val Accuracy: 0.4958\n",
      "صبر: 2/7\n",
      "Epoch 6/40, Train Loss: 0.8723, Val Accuracy: 0.3736\n",
      "صبر: 3/7\n",
      "Epoch 7/40, Train Loss: 0.8474, Val Accuracy: 0.4099\n",
      "صبر: 4/7\n",
      "Epoch 8/40, Train Loss: 0.8585, Val Accuracy: 0.5265\n",
      "صبر: 5/7\n",
      "Epoch 9/40, Train Loss: 0.8471, Val Accuracy: 0.3722\n",
      "صبر: 6/7\n",
      "Epoch 10/40, Train Loss: 0.8056, Val Accuracy: 0.4597\n",
      "صبر: 7/7\n",
      "Early Stopping: توقف آموزش به دلیل عدم بهبود در دقت اعتبارسنجی\n",
      "بهترین مدل PyTorch بارگذاری شد.\n",
      "بهترین پارامترها: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "بهترین F1-score در Grid Search: 0.8900968499260568\n",
      "\n",
      "معیارهای کلی روی داده آموزش (cross-validation):\n",
      "میانگین F1-score (ماکرو) با 10-fold cross-validation: 0.8917\n",
      "Accuracy: 0.9085\n",
      "MCC: 0.8744\n",
      "\n",
      "گزارش معیارها برای هر کلاس (train):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Insoluble       0.86      0.88      0.87     11000\n",
      "    Membrane       0.95      0.95      0.95     11500\n",
      "     Nucleus       0.86      0.75      0.80      5000\n",
      "     Cytosol       0.94      0.96      0.95     11500\n",
      "\n",
      "    accuracy                           0.91     39000\n",
      "   macro avg       0.90      0.89      0.89     39000\n",
      "weighted avg       0.91      0.91      0.91     39000\n",
      "\n",
      "\n",
      "معیارهای کلی روی داده تست:\n",
      "F1-score (ماکرو): 0.8483\n",
      "Accuracy: 0.9042\n",
      "MCC: 0.8662\n",
      "AUC-ROC: 0.9829\n",
      "AUC-PR: 0.9372\n",
      "\n",
      "گزارش معیارها برای هر کلاس (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Insoluble       0.86      0.88      0.87      2682\n",
      "    Membrane       0.94      0.94      0.94      2767\n",
      "     Nucleus       0.76      0.68      0.72       864\n",
      "     Cytosol       0.94      0.96      0.95      2786\n",
      "\n",
      "    accuracy                           0.90      9099\n",
      "   macro avg       0.88      0.87      0.87      9099\n",
      "weighted avg       0.90      0.90      0.90      9099\n",
      "\n",
      "\n",
      "MCC برای هر کلاس (test):\n",
      "Insoluble: 0.8165\n",
      "Membrane: 0.9185\n",
      "Nucleus: 0.6944\n",
      "Cytosol: 0.9295\n",
      "\n",
      "مدل ذخیره شد: best_cefra_svm_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef, accuracy_score, roc_auc_score, average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# تنظیم دستگاه (GPU یا CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"دستگاه: {device}\")\n",
    "\n",
    "# تابع برای Data Augmentation (تغییر تصادفی نوکلئوتیدها)\n",
    "def augment_sequence(seq, mutation_rate=0.03):\n",
    "    nucleotides = ['A', 'C', 'G', 'T']\n",
    "    seq = list(seq)\n",
    "    for i in range(len(seq)):\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            seq[i] = np.random.choice([n for n in nucleotides if n != seq[i]])\n",
    "    return ''.join(seq)\n",
    "\n",
    "# تابع برای استخراج کلاس و مقادیر dist از هدر\n",
    "def get_class_and_dist_from_header(header):\n",
    "    class_names = ['Insoluble', 'Membrane', 'Nucleus', 'Cytosol']\n",
    "    try:\n",
    "        dist_part = header.split(\"dist:\")[1].split()[0]\n",
    "        dist_values = list(map(float, dist_part.split(\"_\")))\n",
    "        max_index = np.argmax(dist_values)\n",
    "        return class_names[max_index], dist_values\n",
    "    except (IndexError, ValueError):\n",
    "        return None, None\n",
    "\n",
    "# تابع برای استخراج ویژگی‌های TF (k-mer) و مقادیر dist\n",
    "def extract_tf_features(fasta_files, k=5, augment=False):\n",
    "    kmer_dict = {''.join(n): i for i, n in enumerate(product('ACGT', repeat=k))}\n",
    "    features = []\n",
    "    labels = []\n",
    "    sequences = []\n",
    "    dist_features = []\n",
    "    class_to_label = {'Insoluble': 0, 'Membrane': 1, 'Nucleus': 2, 'Cytosol': 3}\n",
    "\n",
    "    for fasta_file in fasta_files:\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            seq = str(record.seq)\n",
    "            header = record.description\n",
    "            class_name, dist_values = get_class_and_dist_from_header(header)\n",
    "            if class_name is None or dist_values is None:\n",
    "                continue\n",
    "\n",
    "            label = class_to_label[class_name]\n",
    "            kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "            kmer_count = Counter(kmers)\n",
    "            feature_vector = np.zeros(len(kmer_dict))\n",
    "            for kmer, count in kmer_count.items():\n",
    "                if kmer in kmer_dict:\n",
    "                    feature_vector[kmer_dict[kmer]] = count / len(kmers)\n",
    "            features.append(feature_vector)\n",
    "            labels.append(label)\n",
    "            sequences.append(seq)\n",
    "            dist_features.append(dist_values)\n",
    "\n",
    "            # اعمال Data Augmentation\n",
    "            if augment:\n",
    "                augmented_seq = augment_sequence(seq)\n",
    "                kmers = [augmented_seq[i:i+k] for i in range(len(augmented_seq)-k+1)]\n",
    "                kmer_count = Counter(kmers)\n",
    "                feature_vector = np.zeros(len(kmer_dict))\n",
    "                for kmer, count in kmer_count.items():\n",
    "                    if kmer in kmer_dict:\n",
    "                        feature_vector[kmer_dict[kmer]] = count / len(kmers)\n",
    "                features.append(feature_vector)\n",
    "                labels.append(label)\n",
    "                sequences.append(augmented_seq)\n",
    "                dist_features.append(dist_values)\n",
    "    \n",
    "    return np.array(features), np.array(labels), sequences, np.array(dist_features)\n",
    "\n",
    "# تابع برای استخراج Distance-based Subsequence Profiles\n",
    "def extract_distance_based_features(sequences, max_k=8):\n",
    "    patterns = [''.join(p) for p in product('ACGT', repeat=2)]\n",
    "    features = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        feature_vector = np.zeros((max_k + 1) * len(patterns))\n",
    "        for k in range(max_k + 1):\n",
    "            for i, pattern in enumerate(patterns):\n",
    "                count = 0\n",
    "                for j in range(len(seq) - k - 2):\n",
    "                    if seq[j] == pattern[0] and seq[j + k + 1] == pattern[1]:\n",
    "                        count += 1\n",
    "                feature_vector[k * len(patterns) + i] = count / (len(seq) - k - 1) if len(seq) - k - 1 > 0 else 0\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# تابع برای استخراج ویژگی‌های ساختار ثانویه (ساده‌شده)\n",
    "def extract_ssf_features(sequences):\n",
    "    features = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        seq_len = len(seq)\n",
    "        gc_content = (seq.count('G') + seq.count('C')) / seq_len if seq_len > 0 else 0\n",
    "        au_pairs = sum(1 for i in range(seq_len-1) if (seq[i] == 'A' and seq[i+1] == 'U') or (seq[i] == 'U' and seq[i+1] == 'A'))\n",
    "        gc_pairs = sum(1 for i in range(seq_len-1) if (seq[i] == 'G' and seq[i+1] == 'C') or (seq[i] == 'C' and seq[i+1] == 'G'))\n",
    "        loop_count = sum(1 for i in range(seq_len-2) if seq[i:i+3] in ['AAA', 'UUU', 'AAU', 'UUA'])\n",
    "        free_energy = -1.0 * gc_pairs - 0.5 * au_pairs\n",
    "        \n",
    "        feature_vector = np.array([\n",
    "            gc_content,\n",
    "            au_pairs / seq_len if seq_len > 0 else 0,\n",
    "            gc_pairs / seq_len if seq_len > 0 else 0,\n",
    "            loop_count / seq_len if seq_len > 0 else 0,\n",
    "            free_energy / seq_len if seq_len > 0 else 0\n",
    "        ])\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# تابع برای استخراج ویژگی‌های فیزیکوشیمیایی با کوتاه کردن توالی‌ها\n",
    "def extract_pcp_features(sequences, max_length=1000):\n",
    "    properties = {\n",
    "        'A': [0.62, 0.3, -0.5],\n",
    "        'C': [0.29, 0.4, -0.2],\n",
    "        'G': [0.48, 0.5, -0.3],\n",
    "        'T': [0.58, 0.2, -0.1],\n",
    "        'U': [0.58, 0.2, -0.1],\n",
    "        'N': [0.0, 0.0, 0.0]\n",
    "    }\n",
    "    \n",
    "    features = []\n",
    "    for seq in sequences:\n",
    "        # کوتاه کردن توالی به max_length\n",
    "        seq = seq[:max_length]\n",
    "        prop_matrix = np.zeros((max_length, 3))\n",
    "        for i in range(len(seq)):\n",
    "            prop_matrix[i] = properties.get(seq[i], [0.0, 0.0, 0.0])\n",
    "        mean_props = prop_matrix.mean(axis=0)\n",
    "        std_props = prop_matrix.std(axis=0)\n",
    "        feature_vector = np.concatenate([mean_props, std_props])\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# تعریف شاخه ویژگی (با Conv1D، BatchNorm و MultiHeadAttention)\n",
    "class FeatureBranch(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(FeatureBranch, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=input_dim, out_channels=48, kernel_size=1, padding=0)\n",
    "        self.bn = nn.BatchNorm1d(48)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=48, num_heads=4)\n",
    "        self.norm = nn.LayerNorm(48)\n",
    "        self.linear = nn.Linear(48, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        x, _ = self.mha(x, x, x)\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# تعریف مدل کامل با شاخه جدید برای dist\n",
    "class CeFraSeqModel(nn.Module):\n",
    "    def __init__(self, tfidf_dim=1024, distance_dim=144, ssf_dim=5, pcp_dim=6, dist_dim=4, linear_dim=256, num_classes=4):\n",
    "        super(CeFraSeqModel, self).__init__()\n",
    "        self.tfidf_branch = FeatureBranch(tfidf_dim, linear_dim)\n",
    "        self.distance_branch = FeatureBranch(distance_dim, linear_dim)\n",
    "        self.ssf_branch = FeatureBranch(ssf_dim, linear_dim)\n",
    "        self.pcp_branch = FeatureBranch(pcp_dim, linear_dim)\n",
    "        self.dist_branch = FeatureBranch(dist_dim, linear_dim)\n",
    "        self.fc = nn.Linear(linear_dim * 5, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, tfidf, distance, ssf, pcp, dist):\n",
    "        tfidf_out = self.tfidf_branch(tfidf)\n",
    "        distance_out = self.distance_branch(distance)\n",
    "        ssf_out = self.ssf_branch(ssf)\n",
    "        pcp_out = self.pcp_branch(pcp)\n",
    "        dist_out = self.dist_branch(dist)\n",
    "        combined = torch.cat((tfidf_out, distance_out, ssf_out, pcp_out, dist_out), dim=1)\n",
    "        fc_out = F.relu(self.fc(combined))\n",
    "        fc_out = self.dropout(fc_out)\n",
    "        final_out = self.output(fc_out)\n",
    "        return final_out, fc_out\n",
    "\n",
    "# محاسبه وزن کلاس‌ها\n",
    "def compute_class_weights(labels):\n",
    "    class_counts = np.bincount(labels)\n",
    "    n_classes = len(class_counts)\n",
    "    n_samples = len(labels)\n",
    "    weights = n_samples / (n_classes * class_counts)\n",
    "    return torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "# تابع آموزش مدل PyTorch\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device, checkpoint_path, class_weights, patience=7):\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for tfidf, distance, ssf, pcp, dist, labels in train_loader:\n",
    "            tfidf, distance, ssf, pcp, dist, labels = tfidf.to(device), distance.to(device), ssf.to(device), pcp.to(device), dist.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(tfidf, distance, ssf, pcp, dist)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # اعتبارسنجی\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for tfidf, distance, ssf, pcp, dist, labels in val_loader:\n",
    "                tfidf, distance, ssf, pcp, dist, labels = tfidf.to(device), distance.to(device), ssf.to(device), pcp.to(device), dist.to(device), labels.to(device)\n",
    "                outputs, _ = model(tfidf, distance, ssf, pcp, dist)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        # ذخیره بهترین مدل\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"بهترین مدل ذخیره شد: {checkpoint_path}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"صبر: {patience_counter}/{patience}\")\n",
    "        \n",
    "        # Early Stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early Stopping: توقف آموزش به دلیل عدم بهبود در دقت اعتبارسنجی\")\n",
    "            break\n",
    "\n",
    "# تابع برای استخراج ویژگی‌ها از مدل PyTorch\n",
    "def extract_features(model, data_loader, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tfidf, distance, ssf, pcp, dist, labels in data_loader:\n",
    "            tfidf, distance, ssf, pcp, dist, labels = tfidf.to(device), distance.to(device), ssf.to(device), pcp.to(device), dist.to(device), labels.to(device)\n",
    "            _, fc_out = model(tfidf, distance, ssf, pcp, dist)\n",
    "            features.append(fc_out.cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels_list, axis=0)\n",
    "    return features, labels\n",
    "\n",
    "# فایل‌های FASTA\n",
    "fasta_files = [\n",
    "    \"F:\\\\New Version\\\\Data\\\\cefra-seq\\\\cefra_seq_cDNA_ann_screened.fa\",\n",
    "    \"F:\\\\New Version\\\\Data\\\\cefra-seq\\\\cefra_seq_cDNA_screened.fa\"\n",
    "]\n",
    "\n",
    "# استخراج ویژگی‌های TF (با Data Augmentation) و مقادیر dist\n",
    "tf_features, y, sequences, dist_features = extract_tf_features(fasta_files, k=5, augment=True)\n",
    "\n",
    "# چاپ تعداد نمونه‌ها در هر کلاس\n",
    "print(\"تعداد نمونه‌ها در هر کلاس (کل داده‌ها):\", np.bincount(y))\n",
    "\n",
    "# تبدیل TF به TF-IDF\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_features = tfidf.fit_transform(tf_features).toarray()\n",
    "\n",
    "# استخراج Distance-based Subsequence Profiles\n",
    "distance_features = extract_distance_based_features(sequences, max_k=8)\n",
    "\n",
    "# استخراج Secondary Structure Features\n",
    "ssf_features = extract_ssf_features(sequences)\n",
    "\n",
    "# استخراج Physicochemical Properties\n",
    "pcp_features = extract_pcp_features(sequences, max_length=1000)\n",
    "\n",
    "# بررسی تعداد نمونه‌ها\n",
    "print(\"تعداد نمونه‌ها در TF-IDF:\", tfidf_features.shape[0])\n",
    "print(\"تعداد نمونه‌ها در Distance-based:\", distance_features.shape[0])\n",
    "print(\"تعداد نمونه‌ها در SSF:\", ssf_features.shape[0])\n",
    "print(\"تعداد نمونه‌ها در PCP:\", pcp_features.shape[0])\n",
    "print(\"تعداد نمونه‌ها در Dist:\", dist_features.shape[0])\n",
    "\n",
    "# نرمال‌سازی ویژگی‌ها\n",
    "scaler_tfidf = StandardScaler()\n",
    "scaler_distance = StandardScaler()\n",
    "scaler_ssf = StandardScaler()\n",
    "scaler_pcp = StandardScaler()\n",
    "scaler_dist = StandardScaler()\n",
    "tfidf_features_scaled = scaler_tfidf.fit_transform(tfidf_features)\n",
    "distance_features_scaled = scaler_distance.fit_transform(distance_features)\n",
    "ssf_features_scaled = scaler_ssf.fit_transform(ssf_features)\n",
    "pcp_features_scaled = scaler_pcp.fit_transform(pcp_features)\n",
    "dist_features_scaled = scaler_dist.fit_transform(dist_features)\n",
    "\n",
    "# تقسیم داده‌ها به train و test (80% train، 20% test)\n",
    "X_tfidf_train, X_tfidf_test, X_distance_train, X_distance_test, X_ssf_train, X_ssf_test, X_pcp_train, X_pcp_test, X_dist_train, X_dist_test, y_train, y_test = train_test_split(\n",
    "    tfidf_features_scaled, distance_features_scaled, ssf_features_scaled, pcp_features_scaled, dist_features_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# چاپ تعداد نمونه‌ها در train و test\n",
    "print(\"تعداد نمونه‌ها در train:\", X_tfidf_train.shape[0])\n",
    "print(\"تعداد نمونه‌ها در test:\", X_tfidf_test.shape[0])\n",
    "print(\"تعداد نمونه‌ها در هر کلاس (train):\", np.bincount(y_train))\n",
    "print(\"تعداد نمونه‌ها در هر کلاس (test):\", np.bincount(y_test))\n",
    "\n",
    "# اعمال SMOTE با استراتژی اصلاح‌شده\n",
    "sampling_strategy = {0: 11000, 1: 11500, 2: 5000, 3: 11500}  # تنظیم برای تعادل کلاس‌ها\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_tfidf_train_balanced, y_train_balanced = smote.fit_resample(X_tfidf_train, y_train)\n",
    "X_distance_train_balanced, _ = smote.fit_resample(X_distance_train, y_train)\n",
    "X_ssf_train_balanced, _ = smote.fit_resample(X_ssf_train, y_train)\n",
    "X_pcp_train_balanced, _ = smote.fit_resample(X_pcp_train, y_train)\n",
    "X_dist_train_balanced, _ = smote.fit_resample(X_dist_train, y_train)\n",
    "\n",
    "# چاپ تعداد نمونه‌ها بعد از SMOTE\n",
    "print(\"تعداد نمونه‌ها در هر کلاس بعد از SMOTE (train):\", np.bincount(y_train_balanced))\n",
    "\n",
    "# تبدیل داده‌ها به Tensor برای PyTorch\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_tfidf_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(X_distance_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(X_ssf_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(X_pcp_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(X_dist_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(y_train_balanced, dtype=torch.long)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(X_tfidf_test, dtype=torch.float32),\n",
    "    torch.tensor(X_distance_test, dtype=torch.float32),\n",
    "    torch.tensor(X_ssf_test, dtype=torch.float32),\n",
    "    torch.tensor(X_pcp_test, dtype=torch.float32),\n",
    "    torch.tensor(X_dist_test, dtype=torch.float32),\n",
    "    torch.tensor(y_test, dtype=torch.long)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "# محاسبه وزن کلاس‌ها برای PyTorch\n",
    "class_weights = compute_class_weights(y_train_balanced)\n",
    "\n",
    "# آموزش مدل PyTorch\n",
    "model = CeFraSeqModel(tfidf_dim=1024, distance_dim=144, ssf_dim=5, pcp_dim=6, dist_dim=4, linear_dim=256, num_classes=4).to(device)\n",
    "train_model(model, train_loader, test_loader, num_epochs=40, device=device, checkpoint_path=\"best_cefra_model.pth\", class_weights=class_weights, patience=7)\n",
    "\n",
    "# بارگذاری بهترین مدل PyTorch\n",
    "model.load_state_dict(torch.load(\"best_cefra_model.pth\"))\n",
    "print(\"بهترین مدل PyTorch بارگذاری شد.\")\n",
    "\n",
    "# استخراج ویژگی‌ها از مدل PyTorch\n",
    "X_train_features, y_train_features = extract_features(model, train_loader, device)\n",
    "X_test_features, y_test_features = extract_features(model, test_loader, device)\n",
    "\n",
    "# بهینه‌سازی SVM با Grid Search روی ویژگی‌های استخراج‌شده\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "svm_model = SVC(probability=True, random_state=42)\n",
    "grid = GridSearchCV(svm_model, param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "grid.fit(X_train_features, y_train_features)\n",
    "\n",
    "# چاپ بهترین پارامترها\n",
    "print(\"بهترین پارامترها:\", grid.best_params_)\n",
    "print(\"بهترین F1-score در Grid Search:\", grid.best_score_)\n",
    "\n",
    "# استفاده از بهترین مدل SVM\n",
    "best_svm = grid.best_estimator_\n",
    "\n",
    "# ارزیابی مدل روی داده آموزش با cross-validation\n",
    "y_train_pred = cross_val_predict(best_svm, X_train_features, y_train_features, cv=10)\n",
    "f1_macro_train = cross_val_score(best_svm, X_train_features, y_train_features, cv=10, scoring='f1_macro').mean()\n",
    "accuracy_train = accuracy_score(y_train_features, y_train_pred)\n",
    "mcc_train = matthews_corrcoef(y_train_features, y_train_pred)\n",
    "\n",
    "print(\"\\nمعیارهای کلی روی داده آموزش (cross-validation):\")\n",
    "print(f\"میانگین F1-score (ماکرو) با 10-fold cross-validation: {f1_macro_train:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"MCC: {mcc_train:.4f}\")\n",
    "\n",
    "# گزارش معیارها برای هر کلاس روی داده آموزش\n",
    "class_names = ['Insoluble', 'Membrane', 'Nucleus', 'Cytosol']\n",
    "print(\"\\nگزارش معیارها برای هر کلاس (train):\")\n",
    "print(classification_report(y_train_features, y_train_pred, target_names=class_names))\n",
    "\n",
    "# ارزیابی مدل روی داده تست\n",
    "y_test_pred = best_svm.predict(X_test_features)\n",
    "y_test_pred_proba = best_svm.predict_proba(X_test_features)\n",
    "f1_macro_test = cross_val_score(best_svm, X_test_features, y_test_features, cv=10, scoring='f1_macro').mean()\n",
    "accuracy_test = accuracy_score(y_test_features, y_test_pred)\n",
    "mcc_test = matthews_corrcoef(y_test_features, y_test_pred)\n",
    "\n",
    "# محاسبه AUC-ROC و AUC-PR\n",
    "y_test_bin = label_binarize(y_test_features, classes=[0, 1, 2, 3])\n",
    "auc_roc_test = roc_auc_score(y_test_bin, y_test_pred_proba, average='macro')\n",
    "auc_pr_test = average_precision_score(y_test_bin, y_test_pred_proba, average='macro')\n",
    "\n",
    "print(\"\\nمعیارهای کلی روی داده تست:\")\n",
    "print(f\"F1-score (ماکرو): {f1_macro_test:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"MCC: {mcc_test:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc_test:.4f}\")\n",
    "print(f\"AUC-PR: {auc_pr_test:.4f}\")\n",
    "\n",
    "# گزارش معیارها برای هر کلاس روی داده تست\n",
    "print(\"\\nگزارش معیارها برای هر کلاس (test):\")\n",
    "print(classification_report(y_test_features, y_test_pred, target_names=class_names))\n",
    "\n",
    "# محاسبه MCC برای هر کلاس روی داده تست\n",
    "print(\"\\nMCC برای هر کلاس (test):\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    y_true_binary = (y_test_features == i).astype(int)\n",
    "    y_pred_binary = (y_test_pred == i).astype(int)\n",
    "    mcc_class = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "    print(f\"{class_name}: {mcc_class:.4f}\")\n",
    "\n",
    "# ذخیره مدل برای استفاده بعدی\n",
    "joblib.dump(best_svm, 'best_cefra_svm_model.pkl')\n",
    "print(\"\\nمدل ذخیره شد: best_cefra_svm_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13946d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========cefra_seq_model_fainaly=====================\n",
    "import os\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef, accuracy_score, roc_auc_score, average_precision_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from torch.nn import functional as F\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# تنظیم دستگاه (GPU یا CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"دستگاه: {device}\")\n",
    "\n",
    "# تابع برای Data Augmentation (تغییر تصادفی نوکلئوتیدها)\n",
    "def augment_sequence(seq, mutation_rate=0.03):\n",
    "    nucleotides = ['A', 'C', 'G', 'T']\n",
    "    seq = list(seq)\n",
    "    for i in range(len(seq)):\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            seq[i] = np.random.choice([n for n in nucleotides if n != seq[i]])\n",
    "    return ''.join(seq)\n",
    "\n",
    "# تابع برای استخراج کلاس و مقادیر dist از هدر\n",
    "def get_class_and_dist_from_header(header):\n",
    "    class_names = ['Insoluble', 'Membrane', 'Nucleus', 'Cytosol']\n",
    "    try:\n",
    "        dist_part = header.split(\"dist:\")[1].split()[0]\n",
    "        dist_values = list(map(float, dist_part.split(\"_\")))\n",
    "        max_index = np.argmax(dist_values)\n",
    "        return class_names[max_index], dist_values\n",
    "    except (IndexError, ValueError):\n",
    "        return None, None\n",
    "\n",
    "# تابع برای استخراج ویژگی‌های TF (k-mer) و مقادیر dist\n",
    "def extract_tf_features(fasta_files, k=5, augment=False):\n",
    "    kmer_dict = {''.join(n): i for i, n in enumerate(product('ACGT', repeat=k))}\n",
    "    features = []\n",
    "    labels = []\n",
    "    sequences = []\n",
    "    dist_features = []\n",
    "    class_to_label = {'Insoluble': 0, 'Membrane': 1, 'Nucleus': 2, 'Cytosol': 3}\n",
    "\n",
    "    for fasta_file in fasta_files:\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            seq = str(record.seq)\n",
    "            header = record.description\n",
    "            class_name, dist_values = get_class_and_dist_from_header(header)\n",
    "            if class_name is None or dist_values is None:\n",
    "                continue\n",
    "\n",
    "            label = class_to_label[class_name]\n",
    "            kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "            kmer_count = Counter(kmers)\n",
    "            feature_vector = np.zeros(len(kmer_dict))\n",
    "            for kmer, count in kmer_count.items():\n",
    "                if kmer in kmer_dict:\n",
    "                    feature_vector[kmer_dict[kmer]] = count / len(kmers)\n",
    "            features.append(feature_vector)\n",
    "            labels.append(label)\n",
    "            sequences.append(seq)\n",
    "            dist_features.append(dist_values)\n",
    "\n",
    "            # اعمال Data Augmentation\n",
    "            if augment:\n",
    "                augmented_seq = augment_sequence(seq)\n",
    "                kmers = [augmented_seq[i:i+k] for i in range(len(augmented_seq)-k+1)]\n",
    "                kmer_count = Counter(kmers)\n",
    "                feature_vector = np.zeros(len(kmer_dict))\n",
    "                for kmer, count in kmer_count.items():\n",
    "                    if kmer in kmer_dict:\n",
    "                        feature_vector[kmer_dict[kmer]] = count / len(kmers)\n",
    "                features.append(feature_vector)\n",
    "                labels.append(label)\n",
    "                sequences.append(augmented_seq)\n",
    "                dist_features.append(dist_values)\n",
    "    \n",
    "    return np.array(features), np.array(labels), sequences, np.array(dist_features)\n",
    "\n",
    "# تابع برای استخراج Distance-based Subsequence Profiles\n",
    "def extract_distance_based_features(sequences, max_k=8):\n",
    "    patterns = [''.join(p) for p in product('ACGT', repeat=2)]\n",
    "    features = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        feature_vector = np.zeros((max_k + 1) * len(patterns))\n",
    "        for k in range(max_k + 1):\n",
    "            for i, pattern in enumerate(patterns):\n",
    "                count = 0\n",
    "                for j in range(len(seq) - k - 2):\n",
    "                    if seq[j] == pattern[0] and seq[j + k + 1] == pattern[1]:\n",
    "                        count += 1\n",
    "                feature_vector[k * len(patterns) + i] = count / (len(seq) - k - 1) if len(seq) - k - 1 > 0 else 0\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# تابع برای استخراج ویژگی‌های ساختار ثانویه (ساده‌شده)\n",
    "def extract_ssf_features(sequences):\n",
    "    features = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        seq_len = len(seq)\n",
    "        gc_content = (seq.count('G') + seq.count('C')) / seq_len if seq_len > 0 else 0\n",
    "        au_pairs = sum(1 for i in range(seq_len-1) if (seq[i] == 'A' and seq[i+1] == 'U') or (seq[i] == 'U' and seq[i+1] == 'A'))\n",
    "        gc_pairs = sum(1 for i in range(seq_len-1) if (seq[i] == 'G' and seq[i+1] == 'C') or (seq[i] == 'C' and seq[i+1] == 'G'))\n",
    "        loop_count = sum(1 for i in range(seq_len-2) if seq[i:i+3] in ['AAA', 'UUU', 'AAU', 'UUA'])\n",
    "        free_energy = -1.0 * gc_pairs - 0.5 * au_pairs\n",
    "        \n",
    "        feature_vector = np.array([\n",
    "            gc_content,\n",
    "            au_pairs / seq_len if seq_len > 0 else 0,\n",
    "            gc_pairs / seq_len if seq_len > 0 else 0,\n",
    "            loop_count / seq_len if seq_len > 0 else 0,\n",
    "            free_energy / seq_len if seq_len > 0 else 0\n",
    "        ])\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# تابع برای استخراج ویژگی‌های فیزیکوشیمیایی با کوتاه کردن توالی‌ها\n",
    "def extract_pcp_features(sequences, max_length=1000):\n",
    "    properties = {\n",
    "        'A': [0.62, 0.3, -0.5],\n",
    "        'C': [0.29, 0.4, -0.2],\n",
    "        'G': [0.48, 0.5, -0.3],\n",
    "        'T': [0.58, 0.2, -0.1],\n",
    "        'U': [0.58, 0.2, -0.1],\n",
    "        'N': [0.0, 0.0, 0.0]\n",
    "    }\n",
    "    \n",
    "    features = []\n",
    "    for seq in sequences:\n",
    "        # کوتاه کردن توالی به max_length\n",
    "        seq = seq[:max_length]\n",
    "        prop_matrix = np.zeros((max_length, 3))\n",
    "        for i in range(len(seq)):\n",
    "            prop_matrix[i] = properties.get(seq[i], [0.0, 0.0, 0.0])\n",
    "        mean_props = prop_matrix.mean(axis=0)\n",
    "        std_props = prop_matrix.std(axis=0)\n",
    "        feature_vector = np.concatenate([mean_props, std_props])\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# تعریف شاخه ویژگی (با Conv1D، BatchNorm و MultiHeadAttention)\n",
    "class FeatureBranch(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(FeatureBranch, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=input_dim, out_channels=48, kernel_size=1, padding=0)\n",
    "        self.bn = nn.BatchNorm1d(48)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=48, num_heads=4)\n",
    "        self.norm = nn.LayerNorm(48)\n",
    "        self.linear = nn.Linear(48, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        x, _ = self.mha(x, x, x)\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# تعریف مدل کامل با شاخه جدید برای dist\n",
    "class CeFraSeqModel(nn.Module):\n",
    "    def __init__(self, tfidf_dim=1024, distance_dim=144, ssf_dim=5, pcp_dim=6, dist_dim=4, linear_dim=256, num_classes=4):\n",
    "        super(CeFraSeqModel, self).__init__()\n",
    "        self.tfidf_branch = FeatureBranch(tfidf_dim, linear_dim)\n",
    "        self.distance_branch = FeatureBranch(distance_dim, linear_dim)\n",
    "        self.ssf_branch = FeatureBranch(ssf_dim, linear_dim)\n",
    "        self.pcp_branch = FeatureBranch(pcp_dim, linear_dim)\n",
    "        self.dist_branch = FeatureBranch(dist_dim, linear_dim)\n",
    "        self.fc = nn.Linear(linear_dim * 5, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, tfidf, distance, ssf, pcp, dist):\n",
    "        tfidf_out = self.tfidf_branch(tfidf)\n",
    "        distance_out = self.distance_branch(distance)\n",
    "        ssf_out = self.ssf_branch(ssf)\n",
    "        pcp_out = self.pcp_branch(pcp)\n",
    "        dist_out = self.dist_branch(dist)\n",
    "        combined = torch.cat((tfidf_out, distance_out, ssf_out, pcp_out, dist_out), dim=1)\n",
    "        fc_out = F.relu(self.fc(combined))\n",
    "        fc_out = self.dropout(fc_out)\n",
    "        final_out = self.output(fc_out)\n",
    "        return final_out, fc_out\n",
    "\n",
    "# محاسبه وزن کلاس‌ها\n",
    "def compute_class_weights(labels):\n",
    "    class_counts = np.bincount(labels)\n",
    "    n_classes = len(class_counts)\n",
    "    n_samples = len(labels)\n",
    "    weights = n_samples / (n_classes * class_counts)\n",
    "    return torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "# تابع آموزش مدل PyTorch\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device, checkpoint_path, class_weights, patience=7):\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for tfidf, distance, ssf, pcp, dist, labels in train_loader:\n",
    "            tfidf, distance, ssf, pcp, dist, labels = tfidf.to(device), distance.to(device), ssf.to(device), pcp.to(device), dist.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(tfidf, distance, ssf, pcp, dist)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # اعتبارسنجی\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for tfidf, distance, ssf, pcp, dist, labels in val_loader:\n",
    "                tfidf, distance, ssf, pcp, dist, labels = tfidf.to(device), distance.to(device), ssf.to(device), pcp.to(device), dist.to(device), labels.to(device)\n",
    "                outputs, _ = model(tfidf, distance, ssf, pcp, dist)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        # ذخیره بهترین مدل\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"بهترین مدل ذخیره شد: {checkpoint_path}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"صبر: {patience_counter}/{patience}\")\n",
    "        \n",
    "        # Early Stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early Stopping: توقف آموزش به دلیل عدم بهبود در دقت اعتبارسنجی\")\n",
    "            break\n",
    "\n",
    "# تابع برای استخراج ویژگی‌ها از مدل PyTorch\n",
    "def extract_features(model, data_loader, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tfidf, distance, ssf, pcp, dist, labels in data_loader:\n",
    "            tfidf, distance, ssf, pcp, dist, labels = tfidf.to(device), distance.to(device), ssf.to(device), pcp.to(device), dist.to(device), labels.to(device)\n",
    "            _, fc_out = model(tfidf, distance, ssf, pcp, dist)\n",
    "            features.append(fc_out.cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels_list, axis=0)\n",
    "    return features, labels\n",
    "\n",
    "# مسیر ذخیره‌سازی\n",
    "output_dir = \"F:\\\\payan-nameh\\\\Cefra-seq\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# فایل‌های FASTA\n",
    "fasta_files = [\n",
    "    \"F:\\\\New Version\\\\Data\\\\cefra-seq\\\\cefra_seq_cDNA_ann_screened.fa\",\n",
    "    \"F:\\\\New Version\\\\Data\\\\cefra-seq\\\\cefra_seq_cDNA_screened.fa\"\n",
    "]\n",
    "\n",
    "# استخراج ویژگی‌های TF (با Data Augmentation) و مقادیر dist\n",
    "tf_features, y, sequences, dist_features = extract_tf_features(fasta_files, k=5, augment=True)\n",
    "\n",
    "# چاپ تعداد نمونه‌ها در هر کلاس\n",
    "print(\"تعداد نمونه‌ها در هر کلاس (کل داده‌ها):\", np.bincount(y))\n",
    "\n",
    "# تبدیل TF به TF-IDF\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_features = tfidf.fit_transform(tf_features).toarray()\n",
    "\n",
    "# استخراج Distance-based Subsequence Profiles\n",
    "distance_features = extract_distance_based_features(sequences, max_k=8)\n",
    "\n",
    "# استخراج Secondary Structure Features\n",
    "ssf_features = extract_ssf_features(sequences)\n",
    "\n",
    "# استخراج Physicochemical Properties\n",
    "pcp_features = extract_pcp_features(sequences, max_length=1000)\n",
    "\n",
    "# بررسی تعداد نمونه‌ها\n",
    "print(\"تعداد نمونه‌ها در TF-IDF:\", tfidf_features.shape[0])\n",
    "print(\"تعداد نمونه‌ها در Distance-based:\", distance_features.shape[0])\n",
    "print(\"تعداد نمونه‌ها در SSF:\", ssf_features.shape[0])\n",
    "print(\"تعداد نمونه‌ها در PCP:\", pcp_features.shape[0])\n",
    "print(\"تعداد نمونه‌ها در Dist:\", dist_features.shape[0])\n",
    "\n",
    "# نرمال‌سازی ویژگی‌ها\n",
    "scaler_tfidf = StandardScaler()\n",
    "scaler_distance = StandardScaler()\n",
    "scaler_ssf = StandardScaler()\n",
    "scaler_pcp = StandardScaler()\n",
    "scaler_dist = StandardScaler()\n",
    "tfidf_features_scaled = scaler_tfidf.fit_transform(tfidf_features)\n",
    "distance_features_scaled = scaler_distance.fit_transform(distance_features)\n",
    "ssf_features_scaled = scaler_ssf.fit_transform(ssf_features)\n",
    "pcp_features_scaled = scaler_pcp.fit_transform(pcp_features)\n",
    "dist_features_scaled = scaler_dist.fit_transform(dist_features)\n",
    "\n",
    "# ذخیره اسکیلرها\n",
    "joblib.dump(scaler_tfidf, os.path.join(output_dir, 'scaler_tfidf.pkl'))\n",
    "joblib.dump(scaler_distance, os.path.join(output_dir, 'scaler_distance.pkl'))\n",
    "joblib.dump(scaler_ssf, os.path.join(output_dir, 'scaler_ssf.pkl'))\n",
    "joblib.dump(scaler_pcp, os.path.join(output_dir, 'scaler_pcp.pkl'))\n",
    "joblib.dump(scaler_dist, os.path.join(output_dir, 'scaler_dist.pkl'))\n",
    "print(\"اسکیلرها ذخیره شدند.\")\n",
    "\n",
    "# تقسیم داده‌ها به train و test (80% train، 20% test)\n",
    "X_tfidf_train, X_tfidf_test, X_distance_train, X_distance_test, X_ssf_train, X_ssf_test, X_pcp_train, X_pcp_test, X_dist_train, X_dist_test, y_train, y_test = train_test_split(\n",
    "    tfidf_features_scaled, distance_features_scaled, ssf_features_scaled, pcp_features_scaled, dist_features_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# چاپ تعداد نمونه‌ها در train و test\n",
    "print(\"تعداد نمونه‌ها در train:\", X_tfidf_train.shape[0])\n",
    "print(\"تعداد نمونه‌ها در test:\", X_tfidf_test.shape[0])\n",
    "print(\"تعداد نمونه‌ها در هر کلاس (train):\", np.bincount(y_train))\n",
    "print(\"تعداد نمونه‌ها در هر کلاس (test):\", np.bincount(y_test))\n",
    "\n",
    "# اعمال SMOTE با استراتژی اصلاح‌شده\n",
    "sampling_strategy = {0: 11000, 1: 11500, 2: 5000, 3: 11500}  # تنظیم برای تعادل کلاس‌ها\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_tfidf_train_balanced, y_train_balanced = smote.fit_resample(X_tfidf_train, y_train)\n",
    "X_distance_train_balanced, _ = smote.fit_resample(X_distance_train, y_train)\n",
    "X_ssf_train_balanced, _ = smote.fit_resample(X_ssf_train, y_train)\n",
    "X_pcp_train_balanced, _ = smote.fit_resample(X_pcp_train, y_train)\n",
    "X_dist_train_balanced, _ = smote.fit_resample(X_dist_train, y_train)\n",
    "\n",
    "# چاپ تعداد نمونه‌ها بعد از SMOTE\n",
    "print(\"تعداد نمونه‌ها در هر کلاس بعد از SMOTE (train):\", np.bincount(y_train_balanced))\n",
    "\n",
    "# تبدیل داده‌ها به Tensor برای PyTorch\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_tfidf_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(X_distance_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(X_ssf_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(X_pcp_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(X_dist_train_balanced, dtype=torch.float32),\n",
    "    torch.tensor(y_train_balanced, dtype=torch.long)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(X_tfidf_test, dtype=torch.float32),\n",
    "    torch.tensor(X_distance_test, dtype=torch.float32),\n",
    "    torch.tensor(X_ssf_test, dtype=torch.float32),\n",
    "    torch.tensor(X_pcp_test, dtype=torch.float32),\n",
    "    torch.tensor(X_dist_test, dtype=torch.float32),\n",
    "    torch.tensor(y_test, dtype=torch.long)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "# محاسبه وزن کلاس‌ها برای PyTorch\n",
    "class_weights = compute_class_weights(y_train_balanced)\n",
    "\n",
    "# آموزش مدل PyTorch\n",
    "model = CeFraSeqModel(tfidf_dim=1024, distance_dim=144, ssf_dim=5, pcp_dim=6, dist_dim=4, linear_dim=256, num_classes=4).to(device)\n",
    "train_model(model, train_loader, test_loader, num_epochs=40, device=device, checkpoint_path=os.path.join(output_dir, \"best_cefra_model.pth\"), class_weights=class_weights, patience=7)\n",
    "\n",
    "# بارگذاری بهترین مدل PyTorch\n",
    "model.load_state_dict(torch.load(os.path.join(output_dir, \"best_cefra_model.pth\")))\n",
    "print(\"بهترین مدل PyTorch بارگذاری شد.\")\n",
    "\n",
    "# استخراج ویژگی‌ها از مدل PyTorch\n",
    "X_train_features, y_train_features = extract_features(model, train_loader, device)\n",
    "X_test_features, y_test_features = extract_features(model, test_loader, device)\n",
    "\n",
    "# ذخیره ویژگی‌ها\n",
    "np.save(os.path.join(output_dir, 'X_test_features.npy'), X_test_features)\n",
    "np.save(os.path.join(output_dir, 'y_test_features.npy'), y_test_features)\n",
    "print(\"ویژگی‌های تست ذخیره شدند.\")\n",
    "\n",
    "# بهینه‌سازی SVM با Grid Search روی ویژگی‌های استخراج‌شده\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "svm_model = SVC(probability=True, random_state=42)\n",
    "grid = GridSearchCV(svm_model, param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "grid.fit(X_train_features, y_train_features)\n",
    "\n",
    "# چاپ بهترین پارامترها\n",
    "print(\"بهترین پارامترها:\", grid.best_params_)\n",
    "print(\"بهترین F1-score در Grid Search:\", grid.best_score_)\n",
    "\n",
    "# استفاده از بهترین مدل SVM\n",
    "best_svm = grid.best_estimator_\n",
    "\n",
    "# ارزیابی مدل روی داده آموزش با cross-validation\n",
    "y_train_pred = cross_val_predict(best_svm, X_train_features, y_train_features, cv=10)\n",
    "f1_macro_train = cross_val_score(best_svm, X_train_features, y_train_features, cv=10, scoring='f1_macro').mean()\n",
    "accuracy_train = accuracy_score(y_train_features, y_train_pred)\n",
    "mcc_train = matthews_corrcoef(y_train_features, y_train_pred)\n",
    "\n",
    "print(\"\\nمعیارهای کلی روی داده آموزش (cross-validation):\")\n",
    "print(f\"میانگین F1-score (ماکرو) با 10-fold cross-validation: {f1_macro_train:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_train:.4f}\")\n",
    "print(f\"MCC: {mcc_train:.4f}\")\n",
    "\n",
    "# گزارش معیارها برای هر کلاس روی داده آموزش\n",
    "class_names = ['Insoluble', 'Membrane', 'Nucleus', 'Cytosol']\n",
    "print(\"\\nگزارش معیارها برای هر کلاس (train):\")\n",
    "print(classification_report(y_train_features, y_train_pred, target_names=class_names))\n",
    "\n",
    "# ارزیابی مدل روی داده تست\n",
    "y_test_pred = best_svm.predict(X_test_features)\n",
    "y_test_pred_proba = best_svm.predict_proba(X_test_features)\n",
    "f1_macro_test = cross_val_score(best_svm, X_test_features, y_test_features, cv=10, scoring='f1_macro').mean()\n",
    "accuracy_test = accuracy_score(y_test_features, y_test_pred)\n",
    "mcc_test = matthews_corrcoef(y_test_features, y_test_pred)\n",
    "\n",
    "# محاسبه AUC-ROC و AUC-PR\n",
    "y_test_bin = label_binarize(y_test_features, classes=[0, 1, 2, 3])\n",
    "auc_roc_test = roc_auc_score(y_test_bin, y_test_pred_proba, average='macro')\n",
    "auc_pr_test = average_precision_score(y_test_bin, y_test_pred_proba, average='macro')\n",
    "\n",
    "# محاسبه PCC برای هر کلاس و میانگین کلی\n",
    "pcc_per_class = []\n",
    "pcc_macro = 0.0\n",
    "for i, class_name in enumerate(class_names):\n",
    "    y_true_binary = (y_test_features == i).astype(int)\n",
    "    y_pred_binary = (y_test_pred == i).astype(int)\n",
    "    pcc, _ = pearsonr(y_true_binary, y_pred_binary)\n",
    "    pcc_per_class.append(pcc)\n",
    "    print(f\"PCC برای {class_name} (test): {pcc:.4f}\")\n",
    "pcc_macro = np.mean(pcc_per_class) if pcc_per_class else 0.0\n",
    "print(f\"PCC (ماکرو) برای کل داده تست: {pcc_macro:.4f}\")\n",
    "\n",
    "print(\"\\nمعیارهای کلی روی داده تست:\")\n",
    "print(f\"F1-score (ماکرو): {f1_macro_test:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "print(f\"MCC: {mcc_test:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc_test:.4f}\")\n",
    "print(f\"AUC-PR: {auc_pr_test:.4f}\")\n",
    "print(f\"PCC (ماکرو): {pcc_macro:.4f}\")\n",
    "\n",
    "# گزارش معیارها برای هر کلاس روی داده تست\n",
    "print(\"\\nگزارش معیارها برای هر کلاس (test):\")\n",
    "print(classification_report(y_test_features, y_test_pred, target_names=class_names))\n",
    "\n",
    "# محاسبه MCC برای هر کلاس روی داده تست\n",
    "print(\"\\nMCC برای هر کلاس (test):\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    y_true_binary = (y_test_features == i).astype(int)\n",
    "    y_pred_binary = (y_test_pred == i).astype(int)\n",
    "    mcc_class = matthews_corrcoef(y_true_binary, y_pred_binary)\n",
    "    print(f\"{class_name}: {mcc_class:.4f}\")\n",
    "\n",
    "# ذخیره مدل برای استفاده بعدی\n",
    "joblib.dump(best_svm, os.path.join(output_dir, 'best_cefra_svm_model.pkl'))\n",
    "print(f\"\\nمدل ذخیره شد: {os.path.join(output_dir, 'best_cefra_svm_model.pkl')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42bc7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================lot_cefra_evaluation_charts.=============================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import joblib\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# تنظیم مسیر ذخیره‌سازی\n",
    "output_dir = \"F:\\\\payan-nameh\\\\Cefra-seq\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# بارگذاری داده‌های ذخیره‌شده\n",
    "X_test_features = np.load(os.path.join(output_dir, 'X_test_features.npy'))\n",
    "y_test_features = np.load(os.path.join(output_dir, 'y_test_features.npy'))\n",
    "\n",
    "# بارگذاری مدل SVM\n",
    "best_svm = joblib.load(os.path.join(output_dir, 'best_cefra_svm_model.pkl'))\n",
    "print(f\"مدل SVM از مسیر {os.path.join(output_dir, 'best_cefra_svm_model.pkl')} بارگذاری شد.\")\n",
    "\n",
    "# پیش‌بینی روی داده‌های تست\n",
    "y_test_pred = best_svm.predict(X_test_features)\n",
    "y_test_pred_proba = best_svm.predict_proba(X_test_features)\n",
    "\n",
    "# تعریف نام کلاس‌ها\n",
    "class_names = ['Insoluble', 'Membrane', 'Nucleus', 'Cytosol']\n",
    "\n",
    "# 1. رسم نمودار Loss و Accuracy برای داده‌های آموزش\n",
    "# (فرض می‌کنیم train_loss و train_accuracy از مدل PyTorch ذخیره شده)\n",
    "train_losses = []  # باید از خروجی train_model پر شود (در اینجا فرضی است)\n",
    "train_accuracies = []  # باید از خروجی train_model پر شود (در اینجا فرضی است)\n",
    "val_accuracies = []  # باید از خروجی train_model پر شود (در اینجا فرضی است)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy', color='blue')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'train_loss_accuracy.png'))\n",
    "plt.close()\n",
    "print(f\"نمودار Loss و Accuracy ذخیره شد: {os.path.join(output_dir, 'train_loss_accuracy.png')}\")\n",
    "\n",
    "# 2. رسم منحنی ROC برای هر کلاس (داده‌های تست)\n",
    "y_test_bin = label_binarize(y_test_features, classes=[0, 1, 2, 3])\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(class_names)):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_test_pred_proba[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Each Class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(os.path.join(output_dir, 'roc_curve.png'))\n",
    "plt.close()\n",
    "print(f\"منحنی ROC ذخیره شد: {os.path.join(output_dir, 'roc_curve.png')}\")\n",
    "\n",
    "# 3. رسم منحنی Precision-Recall برای هر کلاس (داده‌های تست)\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(class_names)):\n",
    "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_test_pred_proba[:, i])\n",
    "    plt.plot(recall, precision, label=f'{class_names[i]}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve for Each Class')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(os.path.join(output_dir, 'precision_recall_curve.png'))\n",
    "plt.close()\n",
    "print(f\"منحنی Precision-Recall ذخیره شد: {os.path.join(output_dir, 'precision_recall_curve.png')}\")\n",
    "\n",
    "# 4. رسم نمودار میله‌ای برای F1، MCC، PCC (داده‌های تست)\n",
    "f1_per_class = []\n",
    "mcc_per_class = []\n",
    "pcc_per_class = []\n",
    "for i in range(len(class_names)):\n",
    "    y_true_binary = (y_test_features == i).astype(int)\n",
    "    y_pred_binary = (y_test_pred == i).astype(int)\n",
    "    f1_per_class.append(f1_score(y_true_binary, y_pred_binary))\n",
    "    mcc_per_class.append(matthews_corrcoef(y_true_binary, y_pred_binary))\n",
    "    pcc, _ = pearsonr(y_true_binary, y_pred_binary)\n",
    "    pcc_per_class.append(pcc)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'F1': f1_per_class,\n",
    "    'MCC': mcc_per_class,\n",
    "    'PCC': pcc_per_class\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics_df.set_index('Class')[['F1', 'MCC', 'PCC']].plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('F1, MCC, and PCC per Class (Test Data)')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Class')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'f1_mcc_pcc_per_class.png'))\n",
    "plt.close()\n",
    "print(f\"نمودار F1، MCC و PCC ذخیره شد: {os.path.join(output_dir, 'f1_mcc_pcc_per_class.png')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================analyze_cefra_predictions_per_class===================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# تنظیم مسیر ذخیره‌سازی\n",
    "output_dir = \"F:\\\\payan-nameh\\\\Cefra-seq\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# تعریف نام کلاس‌ها\n",
    "class_names = ['Insoluble', 'Membrane', 'Nucleus', 'Cytosol']\n",
    "\n",
    "# تابع برای بارگذاری دنباله‌ها از فایل‌های FASTA\n",
    "def load_sequences(fasta_files):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    class_to_label = {'Insoluble': 0, 'Membrane': 1, 'Nucleus': 2, 'Cytosol': 3}\n",
    "\n",
    "    for fasta_file in fasta_files:\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            header = record.description\n",
    "            class_name, _ = get_class_and_dist_from_header(header)\n",
    "            if class_name is None:\n",
    "                continue\n",
    "            seq = str(record.seq)\n",
    "            sequences.append(seq)\n",
    "            labels.append(class_to_label[class_name])\n",
    "    \n",
    "    return sequences, np.array(labels)\n",
    "\n",
    "# تابع برای استخراج کلاس و مقادیر dist از هدر\n",
    "def get_class_and_dist_from_header(header):\n",
    "    class_names = ['Insoluble', 'Membrane', 'Nucleus', 'Cytosol']\n",
    "    try:\n",
    "        dist_part = header.split(\"dist:\")[1].split()[0]\n",
    "        dist_values = list(map(float, dist_part.split(\"_\")))\n",
    "        max_index = np.argmax(dist_values)\n",
    "        return class_names[max_index], dist_values\n",
    "    except (IndexError, ValueError):\n",
    "        return None, None\n",
    "\n",
    "# فایل‌های FASTA\n",
    "fasta_files = [\n",
    "    \"F:\\\\New Version\\\\Data\\\\cefra-seq\\\\cefra_seq_cDNA_ann_screened.fa\",\n",
    "    \"F:\\\\New Version\\\\Data\\\\cefra-seq\\\\cefra_seq_cDNA_screened.fa\"\n",
    "]\n",
    "\n",
    "# بارگذاری دنباله‌ها و برچسب‌ها\n",
    "all_sequences, all_labels = load_sequences(fasta_files)\n",
    "\n",
    "# بارگذاری ویژگی‌های تست\n",
    "X_test_features = np.load(os.path.join(output_dir, 'X_test_features.npy'))\n",
    "y_test_features = np.load(os.path.join(output_dir, 'y_test_features.npy'))\n",
    "\n",
    "# بارگذاری مدل SVM\n",
    "best_svm = joblib.load(os.path.join(output_dir, 'best_cefra_svm_model.pkl'))\n",
    "print(f\"مدل SVM از مسیر {os.path.join(output_dir, 'best_cefra_svm_model.pkl')} بارگذاری شد.\")\n",
    "\n",
    "# پیش‌بینی روی داده‌های تست\n",
    "y_test_pred = best_svm.predict(X_test_features)\n",
    "\n",
    "# محاسبه تعداد نوکلئوتیدهای درست پیش‌بینی‌شده برای هر کلاس\n",
    "nucleotide_counts = {class_name: {'correct': 0, 'total': 0} for class_name in class_names}\n",
    "sample_indices = []\n",
    "\n",
    "# تقسیم داده‌ها به train و test مشابه کد اصلی\n",
    "_, test_indices = train_test_split(range(len(all_sequences)), test_size=0.2, random_state=42, stratify=all_labels)\n",
    "\n",
    "# بررسی پیش‌بینی‌ها\n",
    "for idx, test_idx in enumerate(test_indices):\n",
    "    true_label = y_test_features[idx]\n",
    "    pred_label = y_test_pred[idx]\n",
    "    seq = all_sequences[test_idx]\n",
    "    seq_length = len(seq)\n",
    "\n",
    "    class_name = class_names[true_label]\n",
    "    nucleotide_counts[class_name]['total'] += seq_length\n",
    "    if true_label == pred_label:\n",
    "        nucleotide_counts[class_name]['correct'] += seq_length\n",
    "\n",
    "    # ذخیره اطلاعات نمونه\n",
    "    sample_indices.append({\n",
    "        'index': idx,\n",
    "        'sequence': seq[:10] + '...' if len(seq) > 10 else seq,\n",
    "        'true_label': class_names[true_label],\n",
    "        'pred_label': class_names[pred_label],\n",
    "        'nucleotide_count': seq_length,\n",
    "        'correct': 'Correct' if true_label == pred_label else 'Incorrect'\n",
    "    })\n",
    "\n",
    "# چاپ تعداد نوکلئوتیدهای درست پیش‌بینی‌شده\n",
    "print(\"\\nتعداد نوکلئوتیدهای درست پیش‌بینی‌شده برای هر کلاس:\")\n",
    "for class_name, counts in nucleotide_counts.items():\n",
    "    correct = counts['correct']\n",
    "    total = counts['total']\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"{class_name}: {correct} از {total} ({accuracy:.2f}%)\")\n",
    "\n",
    "# تبدیل نمونه‌ها به DataFrame\n",
    "samples_df = pd.DataFrame(sample_indices)\n",
    "\n",
    "# انتخاب 4 نمونه درست و 2 نمونه نادرست برای هر کلاس\n",
    "selected_samples = []\n",
    "for class_name in class_names:\n",
    "    class_samples = samples_df[samples_df['true_label'] == class_name]\n",
    "    correct_samples = class_samples[class_samples['correct'] == 'Correct'].head(4)\n",
    "    incorrect_samples = class_samples[class_samples['correct'] == 'Incorrect'].head(2)\n",
    "    selected_samples.extend(correct_samples.to_dict('records'))\n",
    "    selected_samples.extend(incorrect_samples.to_dict('records'))\n",
    "\n",
    "# تبدیل به DataFrame برای نمایش\n",
    "selected_df = pd.DataFrame(selected_samples)\n",
    "\n",
    "# رسم جدول بصری با بهبود گرافیکی\n",
    "fig, ax = plt.subplots(figsize=(14, len(selected_df) * 0.6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# تنظیم رنگ برای ردیف‌ها (سبز برای درست، قرمز برای نادرست)\n",
    "cell_colors = [['lightgreen' if row['correct'] == 'Correct' else 'lightcoral' for _ in range(5)] for row in selected_samples]\n",
    "\n",
    "table = ax.table(cellText=selected_df[['sequence', 'true_label', 'pred_label', 'nucleotide_count', 'correct']].values,\n",
    "                 colLabels=['Sequence (First 10 nt)', 'True Label', 'Predicted Label', 'Nucleotide Count', 'Status'],\n",
    "                 cellLoc='center', colLoc='center', loc='center',\n",
    "                 cellColours=cell_colors)\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1.5, 1.5)\n",
    "\n",
    "# تنظیم حاشیه‌ها و عنوان\n",
    "plt.title(\"Sample Predictions for Each Class\", fontsize=14, pad=20)\n",
    "for (row, col), cell in table.get_celld().items():\n",
    "    cell.set_edgecolor('black')\n",
    "    cell.set_linewidth(0.5)\n",
    "    if row == 0:\n",
    "        cell.set_text_props(weight='bold', color='darkblue')\n",
    "    if col == 0:\n",
    "        cell.set_text_props(ha='left')\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, 'sample_predictions_table.png'), bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"جدول نمونه‌ها با کیفیت بالا ذخیره شد: {os.path.join(output_dir, 'sample_predictions_table.png')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA-V1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
