{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324ce5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare classes identified: [2, 3, 4] with counts: [ 350 1185  710]\n",
      "Epoch [1/1000], Step [1], D Loss: 6.2057, G Loss: -0.0050\n",
      "Epoch [1/1000], Step [5], D Loss: 6.2863, G Loss: -0.0680\n",
      "Epoch [1/1000], Step [10], D Loss: 5.9976, G Loss: -0.1722\n",
      "Epoch [1/1000], Step [15], D Loss: 5.9032, G Loss: -0.2949\n",
      "Epoch [1/1000], Step [20], D Loss: 5.8524, G Loss: -0.4462\n",
      "Epoch [1/1000], Step [25], D Loss: 5.5493, G Loss: -0.6197\n",
      "Epoch [1/1000], Step [30], D Loss: 5.5517, G Loss: -0.7580\n",
      "Epoch [1/1000], Step [35], D Loss: 5.5283, G Loss: -0.9212\n",
      "Epoch [1/1000], Step [40], D Loss: 5.4786, G Loss: -1.0826\n",
      "Epoch [1/1000], Step [45], D Loss: 5.4394, G Loss: -1.2287\n",
      "Epoch [1/1000], Step [50], D Loss: 5.3117, G Loss: -1.4375\n",
      "Epoch [1/1000], Step [55], D Loss: 5.4057, G Loss: -1.6406\n",
      "Epoch [1/1000], Step [60], D Loss: 5.5722, G Loss: -1.8511\n",
      "Epoch [1/1000], Step [65], D Loss: 5.7676, G Loss: -2.1520\n",
      "Epoch [1/1000], Step [70], D Loss: 6.1150, G Loss: -2.4834\n",
      "Epoch [1/1000], Step [75], D Loss: 6.4384, G Loss: -2.9639\n",
      "Epoch [2/1000], Step [1], D Loss: 6.5855, G Loss: -3.1546\n",
      "Epoch [2/1000], Step [5], D Loss: 6.9058, G Loss: -3.3970\n",
      "Epoch [2/1000], Step [10], D Loss: 7.1892, G Loss: -3.4481\n",
      "Epoch [2/1000], Step [15], D Loss: 7.4796, G Loss: -3.7279\n",
      "Epoch [2/1000], Step [20], D Loss: 7.7713, G Loss: -3.6905\n",
      "Epoch [2/1000], Step [25], D Loss: 7.7086, G Loss: -3.6807\n",
      "Epoch [2/1000], Step [30], D Loss: 7.6011, G Loss: -3.8209\n",
      "Epoch [2/1000], Step [35], D Loss: 7.6137, G Loss: -3.7047\n",
      "Epoch [2/1000], Step [40], D Loss: 7.7287, G Loss: -3.9846\n",
      "Epoch [2/1000], Step [45], D Loss: 7.7952, G Loss: -3.7983\n",
      "Epoch [2/1000], Step [50], D Loss: 7.6981, G Loss: -3.9364\n",
      "Epoch [2/1000], Step [55], D Loss: 7.6498, G Loss: -3.7510\n",
      "Epoch [2/1000], Step [60], D Loss: 7.7211, G Loss: -3.7932\n",
      "Epoch [2/1000], Step [65], D Loss: 7.6092, G Loss: -3.7092\n",
      "Epoch [2/1000], Step [70], D Loss: 7.5225, G Loss: -3.7078\n",
      "Epoch [2/1000], Step [75], D Loss: 7.3411, G Loss: -3.7401\n",
      "Epoch [3/1000], Step [1], D Loss: 7.3213, G Loss: -3.7020\n",
      "Epoch [3/1000], Step [5], D Loss: 7.2416, G Loss: -3.7252\n",
      "Epoch [3/1000], Step [10], D Loss: 7.1446, G Loss: -3.5989\n",
      "Epoch [3/1000], Step [15], D Loss: 7.0252, G Loss: -3.7417\n",
      "Epoch [3/1000], Step [20], D Loss: 6.9210, G Loss: -3.7216\n",
      "Epoch [3/1000], Step [25], D Loss: 6.8485, G Loss: -3.7085\n",
      "Epoch [3/1000], Step [30], D Loss: 6.8219, G Loss: -3.7392\n",
      "Epoch [3/1000], Step [35], D Loss: 6.7028, G Loss: -3.7603\n",
      "Epoch [3/1000], Step [40], D Loss: 6.6615, G Loss: -3.7426\n",
      "Epoch [3/1000], Step [45], D Loss: 6.5718, G Loss: -3.6925\n",
      "Epoch [3/1000], Step [50], D Loss: 6.4909, G Loss: -3.6741\n",
      "Epoch [3/1000], Step [55], D Loss: 6.4571, G Loss: -3.6867\n",
      "Epoch [3/1000], Step [60], D Loss: 6.4470, G Loss: -3.6261\n",
      "Epoch [3/1000], Step [65], D Loss: 6.4028, G Loss: -3.6116\n",
      "Epoch [3/1000], Step [70], D Loss: 6.3602, G Loss: -3.5762\n",
      "Epoch [3/1000], Step [75], D Loss: 6.2154, G Loss: -3.5625\n",
      "Epoch [4/1000], Step [1], D Loss: 6.2626, G Loss: -3.5162\n",
      "Epoch [4/1000], Step [5], D Loss: 6.1320, G Loss: -3.5169\n",
      "Epoch [4/1000], Step [10], D Loss: 6.1413, G Loss: -3.5128\n",
      "Epoch [4/1000], Step [15], D Loss: 6.1368, G Loss: -3.4988\n",
      "Epoch [4/1000], Step [20], D Loss: 6.2161, G Loss: -3.5265\n",
      "Epoch [4/1000], Step [25], D Loss: 6.1616, G Loss: -3.5351\n",
      "Epoch [4/1000], Step [30], D Loss: 6.2692, G Loss: -3.5808\n",
      "Epoch [4/1000], Step [35], D Loss: 6.4041, G Loss: -3.6588\n",
      "Epoch [4/1000], Step [40], D Loss: 6.5663, G Loss: -3.7541\n",
      "Epoch [4/1000], Step [45], D Loss: 6.6333, G Loss: -3.7800\n",
      "Epoch [4/1000], Step [50], D Loss: 6.5974, G Loss: -3.7599\n",
      "Epoch [4/1000], Step [55], D Loss: 6.5330, G Loss: -3.6874\n",
      "Epoch [4/1000], Step [60], D Loss: 6.4358, G Loss: -3.6139\n",
      "Epoch [4/1000], Step [65], D Loss: 6.3497, G Loss: -3.5155\n",
      "Epoch [4/1000], Step [70], D Loss: 6.2494, G Loss: -3.4142\n",
      "Epoch [4/1000], Step [75], D Loss: 6.1014, G Loss: -3.3310\n",
      "Epoch [5/1000], Step [1], D Loss: 6.0722, G Loss: -3.2794\n",
      "Epoch [5/1000], Step [5], D Loss: 6.0868, G Loss: -3.2471\n",
      "Epoch [5/1000], Step [10], D Loss: 6.0790, G Loss: -3.2579\n",
      "Epoch [5/1000], Step [15], D Loss: 6.0385, G Loss: -3.2284\n",
      "Epoch [5/1000], Step [20], D Loss: 5.8872, G Loss: -3.1683\n",
      "Epoch [5/1000], Step [25], D Loss: 5.8784, G Loss: -3.1742\n",
      "Epoch [5/1000], Step [30], D Loss: 5.9205, G Loss: -3.2329\n",
      "Epoch [5/1000], Step [35], D Loss: 5.9173, G Loss: -3.2707\n",
      "Epoch [5/1000], Step [40], D Loss: 5.8164, G Loss: -3.2342\n",
      "Epoch [5/1000], Step [45], D Loss: 5.7736, G Loss: -3.2295\n",
      "Epoch [5/1000], Step [50], D Loss: 5.8167, G Loss: -3.2670\n",
      "Epoch [5/1000], Step [55], D Loss: 5.8300, G Loss: -3.2728\n",
      "Epoch [5/1000], Step [60], D Loss: 5.8387, G Loss: -3.3070\n",
      "Epoch [5/1000], Step [65], D Loss: 5.7477, G Loss: -3.2854\n",
      "Epoch [5/1000], Step [70], D Loss: 5.7588, G Loss: -3.2777\n",
      "Epoch [5/1000], Step [75], D Loss: 5.8423, G Loss: -3.2518\n",
      "Epoch [6/1000], Step [1], D Loss: 5.8500, G Loss: -3.2340\n",
      "Epoch [6/1000], Step [5], D Loss: 5.6404, G Loss: -3.2399\n",
      "Epoch [6/1000], Step [10], D Loss: 5.5746, G Loss: -3.2605\n",
      "Epoch [6/1000], Step [15], D Loss: 5.6234, G Loss: -3.2364\n",
      "Epoch [6/1000], Step [20], D Loss: 5.4613, G Loss: -3.1325\n",
      "Epoch [6/1000], Step [25], D Loss: 5.3527, G Loss: -3.0832\n",
      "Epoch [6/1000], Step [30], D Loss: 5.3003, G Loss: -3.0526\n",
      "Epoch [6/1000], Step [35], D Loss: 5.3349, G Loss: -3.0765\n",
      "Epoch [6/1000], Step [40], D Loss: 5.3521, G Loss: -3.0112\n",
      "Epoch [6/1000], Step [45], D Loss: 5.3833, G Loss: -2.9941\n",
      "Epoch [6/1000], Step [50], D Loss: 5.1959, G Loss: -2.9852\n",
      "Epoch [6/1000], Step [55], D Loss: 5.3209, G Loss: -3.0728\n",
      "Epoch [6/1000], Step [60], D Loss: 5.4271, G Loss: -3.1000\n",
      "Epoch [6/1000], Step [65], D Loss: 5.5161, G Loss: -3.1313\n",
      "Epoch [6/1000], Step [70], D Loss: 5.7002, G Loss: -3.1793\n",
      "Epoch [6/1000], Step [75], D Loss: 5.5978, G Loss: -3.1890\n",
      "Epoch [7/1000], Step [1], D Loss: 5.7968, G Loss: -3.2126\n",
      "Epoch [7/1000], Step [5], D Loss: 5.7905, G Loss: -3.2292\n",
      "Epoch [7/1000], Step [10], D Loss: 5.8751, G Loss: -3.2396\n",
      "Epoch [7/1000], Step [15], D Loss: 5.9941, G Loss: -3.2307\n",
      "Epoch [7/1000], Step [20], D Loss: 5.9003, G Loss: -3.1421\n",
      "Epoch [7/1000], Step [25], D Loss: 5.7695, G Loss: -3.0654\n",
      "Epoch [7/1000], Step [30], D Loss: 5.7955, G Loss: -2.9694\n",
      "Epoch [7/1000], Step [35], D Loss: 5.6398, G Loss: -2.9021\n",
      "Epoch [7/1000], Step [40], D Loss: 5.5062, G Loss: -2.7737\n",
      "Epoch [7/1000], Step [45], D Loss: 5.6444, G Loss: -2.6910\n",
      "Epoch [7/1000], Step [50], D Loss: 5.5412, G Loss: -2.6200\n",
      "Epoch [7/1000], Step [55], D Loss: 5.6380, G Loss: -2.6160\n",
      "Epoch [7/1000], Step [60], D Loss: 5.6666, G Loss: -2.7428\n",
      "Epoch [7/1000], Step [65], D Loss: 5.7962, G Loss: -2.8689\n",
      "Epoch [7/1000], Step [70], D Loss: 5.7792, G Loss: -2.9642\n",
      "Epoch [7/1000], Step [75], D Loss: 5.7244, G Loss: -3.0641\n",
      "Epoch [8/1000], Step [1], D Loss: 5.7898, G Loss: -3.1830\n",
      "Epoch [8/1000], Step [5], D Loss: 5.8959, G Loss: -3.2348\n",
      "Epoch [8/1000], Step [10], D Loss: 5.9491, G Loss: -3.2245\n",
      "Epoch [8/1000], Step [15], D Loss: 5.9514, G Loss: -3.1750\n",
      "Epoch [8/1000], Step [20], D Loss: 5.7651, G Loss: -3.1185\n",
      "Epoch [8/1000], Step [25], D Loss: 5.6872, G Loss: -3.1284\n",
      "Epoch [8/1000], Step [30], D Loss: 5.6018, G Loss: -3.1018\n",
      "Epoch [8/1000], Step [35], D Loss: 5.4968, G Loss: -3.0735\n",
      "Epoch [8/1000], Step [40], D Loss: 5.6516, G Loss: -2.9897\n",
      "Epoch [8/1000], Step [45], D Loss: 5.6328, G Loss: -3.0079\n",
      "Epoch [8/1000], Step [50], D Loss: 5.5626, G Loss: -3.0122\n",
      "Epoch [8/1000], Step [55], D Loss: 5.5578, G Loss: -2.9579\n",
      "Epoch [8/1000], Step [60], D Loss: 5.6155, G Loss: -2.9223\n",
      "Epoch [8/1000], Step [65], D Loss: 5.6139, G Loss: -2.9793\n",
      "Epoch [8/1000], Step [70], D Loss: 5.6634, G Loss: -2.9659\n",
      "Epoch [8/1000], Step [75], D Loss: 5.7752, G Loss: -2.9634\n",
      "Epoch [9/1000], Step [1], D Loss: 5.8358, G Loss: -2.9046\n",
      "Epoch [9/1000], Step [5], D Loss: 5.5371, G Loss: -2.9131\n",
      "Epoch [9/1000], Step [10], D Loss: 5.3695, G Loss: -2.8947\n",
      "Epoch [9/1000], Step [15], D Loss: 5.3159, G Loss: -2.8494\n",
      "Epoch [9/1000], Step [20], D Loss: 5.2824, G Loss: -2.8119\n",
      "Epoch [9/1000], Step [25], D Loss: 5.1720, G Loss: -2.7976\n",
      "Epoch [9/1000], Step [30], D Loss: 5.3863, G Loss: -2.8399\n",
      "Epoch [9/1000], Step [35], D Loss: 5.5307, G Loss: -2.9296\n",
      "Epoch [9/1000], Step [40], D Loss: 5.7576, G Loss: -3.0712\n",
      "Epoch [9/1000], Step [45], D Loss: 5.6541, G Loss: -3.0799\n",
      "Epoch [9/1000], Step [50], D Loss: 5.6939, G Loss: -3.0769\n",
      "Epoch [9/1000], Step [55], D Loss: 5.4922, G Loss: -2.9576\n",
      "Epoch [9/1000], Step [60], D Loss: 5.4143, G Loss: -2.8488\n",
      "Epoch [9/1000], Step [65], D Loss: 5.3538, G Loss: -2.8433\n",
      "Epoch [9/1000], Step [70], D Loss: 5.3291, G Loss: -2.8126\n",
      "Epoch [9/1000], Step [75], D Loss: 5.2914, G Loss: -2.7363\n",
      "Epoch [10/1000], Step [1], D Loss: 5.2203, G Loss: -2.6527\n",
      "Epoch [10/1000], Step [5], D Loss: 5.2586, G Loss: -2.6912\n",
      "Epoch [10/1000], Step [10], D Loss: 5.4740, G Loss: -2.7935\n",
      "Epoch [10/1000], Step [15], D Loss: 5.6359, G Loss: -2.9350\n",
      "Epoch [10/1000], Step [20], D Loss: 5.8987, G Loss: -3.0566\n",
      "Epoch [10/1000], Step [25], D Loss: 6.0546, G Loss: -3.1325\n",
      "Epoch [10/1000], Step [30], D Loss: 6.0586, G Loss: -3.1784\n",
      "Epoch [10/1000], Step [35], D Loss: 5.9305, G Loss: -3.0921\n",
      "Epoch [10/1000], Step [40], D Loss: 5.9346, G Loss: -3.0914\n",
      "Epoch [10/1000], Step [45], D Loss: 5.8154, G Loss: -3.0614\n",
      "Epoch [10/1000], Step [50], D Loss: 5.8288, G Loss: -3.0560\n",
      "Epoch [10/1000], Step [55], D Loss: 5.7826, G Loss: -3.0698\n",
      "Epoch [10/1000], Step [60], D Loss: 5.6246, G Loss: -3.1306\n",
      "Epoch [10/1000], Step [65], D Loss: 5.6957, G Loss: -3.2143\n",
      "Epoch [10/1000], Step [70], D Loss: 5.7671, G Loss: -3.3462\n",
      "Epoch [10/1000], Step [75], D Loss: 6.0851, G Loss: -3.4275\n",
      "Epoch [11/1000], Step [1], D Loss: 5.9956, G Loss: -3.4968\n",
      "Epoch [11/1000], Step [5], D Loss: 6.0242, G Loss: -3.4657\n",
      "Epoch [11/1000], Step [10], D Loss: 5.9471, G Loss: -3.5242\n",
      "Epoch [11/1000], Step [15], D Loss: 6.1366, G Loss: -3.5501\n",
      "Epoch [11/1000], Step [20], D Loss: 6.1383, G Loss: -3.5790\n",
      "Epoch [11/1000], Step [25], D Loss: 6.1953, G Loss: -3.5692\n",
      "Epoch [11/1000], Step [30], D Loss: 6.0387, G Loss: -3.3896\n",
      "Epoch [11/1000], Step [35], D Loss: 5.7396, G Loss: -3.2085\n",
      "Epoch [11/1000], Step [40], D Loss: 5.7451, G Loss: -3.0567\n",
      "Epoch [11/1000], Step [45], D Loss: 5.6611, G Loss: -2.9980\n",
      "Epoch [11/1000], Step [50], D Loss: 5.6696, G Loss: -2.9763\n",
      "Epoch [11/1000], Step [55], D Loss: 5.6975, G Loss: -2.9751\n",
      "Epoch [11/1000], Step [60], D Loss: 5.9113, G Loss: -3.0055\n",
      "Epoch [11/1000], Step [65], D Loss: 5.8429, G Loss: -3.0391\n",
      "Epoch [11/1000], Step [70], D Loss: 5.7290, G Loss: -3.0371\n",
      "Epoch [11/1000], Step [75], D Loss: 5.4637, G Loss: -3.0805\n",
      "Epoch [12/1000], Step [1], D Loss: 5.4857, G Loss: -3.0592\n",
      "Epoch [12/1000], Step [5], D Loss: 5.3822, G Loss: -3.0019\n",
      "Epoch [12/1000], Step [10], D Loss: 5.3907, G Loss: -2.9741\n",
      "Epoch [12/1000], Step [15], D Loss: 5.3705, G Loss: -3.0223\n",
      "Epoch [12/1000], Step [20], D Loss: 5.4997, G Loss: -3.1916\n",
      "Epoch [12/1000], Step [25], D Loss: 5.0562, G Loss: -3.2074\n",
      "Epoch [12/1000], Step [30], D Loss: 5.4525, G Loss: -3.1360\n",
      "Epoch [12/1000], Step [35], D Loss: 5.1566, G Loss: -3.0219\n",
      "Epoch [12/1000], Step [40], D Loss: 5.1338, G Loss: -2.9365\n",
      "Epoch [12/1000], Step [45], D Loss: 5.3297, G Loss: -2.9255\n",
      "Epoch [12/1000], Step [50], D Loss: 5.4821, G Loss: -2.9564\n",
      "Epoch [12/1000], Step [55], D Loss: 5.4172, G Loss: -2.9508\n",
      "Epoch [12/1000], Step [60], D Loss: 5.2037, G Loss: -3.0143\n",
      "Epoch [12/1000], Step [65], D Loss: 5.0180, G Loss: -3.0849\n",
      "Epoch [12/1000], Step [70], D Loss: 4.5150, G Loss: -3.1317\n",
      "Epoch [12/1000], Step [75], D Loss: 5.1573, G Loss: -3.0742\n",
      "Epoch [13/1000], Step [1], D Loss: 5.2334, G Loss: -3.1328\n",
      "Epoch [13/1000], Step [5], D Loss: 5.3301, G Loss: -3.1800\n",
      "Epoch [13/1000], Step [10], D Loss: 5.5670, G Loss: -3.2907\n",
      "Epoch [13/1000], Step [15], D Loss: 5.5721, G Loss: -3.3881\n",
      "Epoch [13/1000], Step [20], D Loss: 5.6689, G Loss: -3.4599\n",
      "Epoch [13/1000], Step [25], D Loss: 5.6003, G Loss: -2.9271\n",
      "Epoch [13/1000], Step [30], D Loss: 5.7287, G Loss: -3.6096\n",
      "Epoch [13/1000], Step [35], D Loss: 5.6266, G Loss: -3.5611\n",
      "Epoch [13/1000], Step [40], D Loss: 5.6839, G Loss: -3.6080\n",
      "Epoch [13/1000], Step [45], D Loss: 5.6404, G Loss: -3.5736\n",
      "Epoch [13/1000], Step [50], D Loss: 5.5767, G Loss: -3.5421\n",
      "Epoch [13/1000], Step [55], D Loss: 5.4887, G Loss: -3.4070\n",
      "Epoch [13/1000], Step [60], D Loss: 5.5380, G Loss: -3.2716\n",
      "Epoch [13/1000], Step [65], D Loss: 4.9709, G Loss: -3.1331\n",
      "Epoch [13/1000], Step [70], D Loss: 5.3198, G Loss: -3.0146\n",
      "Epoch [13/1000], Step [75], D Loss: 5.2662, G Loss: -2.9867\n",
      "Epoch [14/1000], Step [1], D Loss: 5.2849, G Loss: -3.0402\n",
      "Epoch [14/1000], Step [5], D Loss: 5.2922, G Loss: -3.0838\n",
      "Epoch [14/1000], Step [10], D Loss: 5.3120, G Loss: -3.1148\n",
      "Epoch [14/1000], Step [15], D Loss: 5.2693, G Loss: -3.1260\n",
      "Epoch [14/1000], Step [20], D Loss: 5.4469, G Loss: -3.1194\n",
      "Epoch [14/1000], Step [25], D Loss: 5.6275, G Loss: -3.2407\n",
      "Epoch [14/1000], Step [30], D Loss: 5.6744, G Loss: -3.2693\n",
      "Epoch [14/1000], Step [35], D Loss: 5.6922, G Loss: -3.3377\n",
      "Epoch [14/1000], Step [40], D Loss: 5.8831, G Loss: -3.4151\n",
      "Epoch [14/1000], Step [45], D Loss: 5.8610, G Loss: -3.4485\n",
      "Epoch [14/1000], Step [50], D Loss: 5.9203, G Loss: -3.5246\n",
      "Epoch [14/1000], Step [55], D Loss: 5.2178, G Loss: -2.7220\n",
      "Epoch [14/1000], Step [60], D Loss: 6.0203, G Loss: -3.6844\n",
      "Epoch [14/1000], Step [65], D Loss: 6.0837, G Loss: -3.6156\n",
      "Epoch [14/1000], Step [70], D Loss: 5.8597, G Loss: -3.4670\n",
      "Epoch [14/1000], Step [75], D Loss: 5.6557, G Loss: -3.3687\n",
      "Epoch [15/1000], Step [1], D Loss: 5.5817, G Loss: -3.2716\n",
      "Epoch [15/1000], Step [5], D Loss: 5.5624, G Loss: -3.2334\n",
      "Epoch [15/1000], Step [10], D Loss: 5.3102, G Loss: -3.2410\n",
      "Epoch [15/1000], Step [15], D Loss: 5.5421, G Loss: -3.2657\n",
      "Epoch [15/1000], Step [20], D Loss: 5.2570, G Loss: -3.1858\n",
      "Epoch [15/1000], Step [25], D Loss: 5.7305, G Loss: -3.1464\n",
      "Epoch [15/1000], Step [30], D Loss: 5.4233, G Loss: -3.1098\n",
      "Epoch [15/1000], Step [35], D Loss: 5.5378, G Loss: -3.1868\n",
      "Epoch [15/1000], Step [40], D Loss: 5.6063, G Loss: -3.3125\n",
      "Epoch [15/1000], Step [45], D Loss: 5.7771, G Loss: -3.3650\n",
      "Epoch [15/1000], Step [50], D Loss: 5.7319, G Loss: -3.4097\n",
      "Epoch [15/1000], Step [55], D Loss: 5.7508, G Loss: -3.4334\n",
      "Epoch [15/1000], Step [60], D Loss: 5.8446, G Loss: -3.5019\n",
      "Epoch [15/1000], Step [65], D Loss: 6.0931, G Loss: -3.5713\n",
      "Epoch [15/1000], Step [70], D Loss: 5.8719, G Loss: -3.6164\n",
      "Epoch [15/1000], Step [75], D Loss: 6.0059, G Loss: -3.6689\n",
      "Epoch [16/1000], Step [1], D Loss: 5.9944, G Loss: -3.6331\n",
      "Epoch [16/1000], Step [5], D Loss: 5.8243, G Loss: -3.6588\n",
      "Epoch [16/1000], Step [10], D Loss: 5.9325, G Loss: -3.7168\n",
      "Epoch [16/1000], Step [15], D Loss: 5.8334, G Loss: -3.6378\n",
      "Epoch [16/1000], Step [20], D Loss: 5.6755, G Loss: -3.6607\n",
      "Epoch [16/1000], Step [25], D Loss: 5.9738, G Loss: -3.7600\n",
      "Epoch [16/1000], Step [30], D Loss: 5.9035, G Loss: -3.7504\n",
      "Epoch [16/1000], Step [35], D Loss: 5.8567, G Loss: -3.5451\n",
      "Epoch [16/1000], Step [40], D Loss: 5.6380, G Loss: -3.3366\n",
      "Epoch [16/1000], Step [45], D Loss: 5.5603, G Loss: -3.2872\n",
      "Epoch [16/1000], Step [50], D Loss: 5.5845, G Loss: -3.2621\n",
      "Epoch [16/1000], Step [55], D Loss: 5.6787, G Loss: -3.2537\n",
      "Epoch [16/1000], Step [60], D Loss: 5.8196, G Loss: -3.2742\n",
      "Epoch [16/1000], Step [65], D Loss: 5.9009, G Loss: -3.2476\n",
      "Epoch [16/1000], Step [70], D Loss: 5.3866, G Loss: -3.3175\n",
      "Epoch [16/1000], Step [75], D Loss: 5.3099, G Loss: -3.3627\n",
      "Epoch [17/1000], Step [1], D Loss: 5.4661, G Loss: -3.3758\n",
      "Epoch [17/1000], Step [5], D Loss: 5.3390, G Loss: -3.4116\n",
      "Epoch [17/1000], Step [10], D Loss: 5.3742, G Loss: -3.4278\n",
      "Epoch [17/1000], Step [15], D Loss: 5.6884, G Loss: -3.5586\n",
      "Epoch [17/1000], Step [20], D Loss: 5.6547, G Loss: -3.6390\n",
      "Epoch [17/1000], Step [25], D Loss: 5.6872, G Loss: -3.5654\n",
      "Epoch [17/1000], Step [30], D Loss: 5.4999, G Loss: -3.4292\n",
      "Epoch [17/1000], Step [35], D Loss: 5.5113, G Loss: -3.4181\n",
      "Epoch [17/1000], Step [40], D Loss: 5.6728, G Loss: -3.5642\n",
      "Epoch [17/1000], Step [45], D Loss: 5.6853, G Loss: -3.6408\n",
      "Epoch [17/1000], Step [50], D Loss: 5.5845, G Loss: -3.6130\n",
      "Epoch [17/1000], Step [55], D Loss: 5.6627, G Loss: -3.6212\n",
      "Epoch [17/1000], Step [60], D Loss: 5.7345, G Loss: -3.5607\n",
      "Epoch [17/1000], Step [65], D Loss: 5.5089, G Loss: -3.5147\n",
      "Epoch [17/1000], Step [70], D Loss: 5.5984, G Loss: -3.6684\n",
      "Epoch [17/1000], Step [75], D Loss: 5.6948, G Loss: -3.6836\n",
      "Epoch [18/1000], Step [1], D Loss: 5.6503, G Loss: -3.6824\n",
      "Epoch [18/1000], Step [5], D Loss: 5.7079, G Loss: -3.7305\n",
      "Epoch [18/1000], Step [10], D Loss: 5.7153, G Loss: -3.7651\n",
      "Epoch [18/1000], Step [15], D Loss: 5.7856, G Loss: -3.7972\n",
      "Epoch [18/1000], Step [20], D Loss: 5.8543, G Loss: -3.7980\n",
      "Epoch [18/1000], Step [25], D Loss: 5.7588, G Loss: -3.7411\n",
      "Epoch [18/1000], Step [30], D Loss: 5.7413, G Loss: -3.6669\n",
      "Epoch [18/1000], Step [35], D Loss: 5.4723, G Loss: -3.5870\n",
      "Epoch [18/1000], Step [40], D Loss: 5.3119, G Loss: -3.4483\n",
      "Epoch [18/1000], Step [45], D Loss: 5.2953, G Loss: -3.4120\n",
      "Epoch [18/1000], Step [50], D Loss: 5.3245, G Loss: -3.4103\n",
      "Epoch [18/1000], Step [55], D Loss: 5.3702, G Loss: -3.4116\n",
      "Epoch [18/1000], Step [60], D Loss: 5.3512, G Loss: -3.4085\n",
      "Epoch [18/1000], Step [65], D Loss: 5.4476, G Loss: -3.5373\n",
      "Epoch [18/1000], Step [70], D Loss: 5.6714, G Loss: -3.7156\n",
      "Epoch [18/1000], Step [75], D Loss: 5.5087, G Loss: -3.9596\n",
      "Epoch [19/1000], Step [1], D Loss: 5.9891, G Loss: -4.1300\n",
      "Epoch [19/1000], Step [5], D Loss: 6.0667, G Loss: -4.1942\n",
      "Epoch [19/1000], Step [10], D Loss: 6.0629, G Loss: -4.1151\n",
      "Epoch [19/1000], Step [15], D Loss: 5.9858, G Loss: -3.9943\n",
      "Epoch [19/1000], Step [20], D Loss: 5.7226, G Loss: -3.7895\n",
      "Epoch [19/1000], Step [25], D Loss: 5.6808, G Loss: -3.6348\n",
      "Epoch [19/1000], Step [30], D Loss: 5.3548, G Loss: -3.4647\n",
      "Epoch [19/1000], Step [35], D Loss: 5.3384, G Loss: -3.3642\n",
      "Epoch [19/1000], Step [40], D Loss: 5.4852, G Loss: -3.3844\n",
      "Epoch [19/1000], Step [45], D Loss: 5.5992, G Loss: -3.4423\n",
      "Epoch [19/1000], Step [50], D Loss: 5.5684, G Loss: -3.4365\n",
      "Epoch [19/1000], Step [55], D Loss: 5.5860, G Loss: -3.4216\n",
      "Epoch [19/1000], Step [60], D Loss: 5.8459, G Loss: -3.4979\n",
      "Epoch [19/1000], Step [65], D Loss: 5.9680, G Loss: -3.4580\n",
      "Epoch [19/1000], Step [70], D Loss: 5.5809, G Loss: -3.5027\n",
      "Epoch [19/1000], Step [75], D Loss: 5.5212, G Loss: -3.4166\n",
      "Epoch [20/1000], Step [1], D Loss: 5.5591, G Loss: -3.4316\n",
      "Epoch [20/1000], Step [5], D Loss: 5.5866, G Loss: -3.4453\n",
      "Epoch [20/1000], Step [10], D Loss: 5.5556, G Loss: -3.3882\n",
      "Epoch [20/1000], Step [15], D Loss: 5.3967, G Loss: -3.3884\n",
      "Epoch [20/1000], Step [20], D Loss: 5.5373, G Loss: -3.4076\n",
      "Epoch [20/1000], Step [25], D Loss: 5.4365, G Loss: -3.3627\n",
      "Epoch [20/1000], Step [30], D Loss: 5.2192, G Loss: -3.3154\n",
      "Epoch [20/1000], Step [35], D Loss: 5.0074, G Loss: -3.3277\n",
      "Epoch [20/1000], Step [40], D Loss: 5.0928, G Loss: -3.4503\n",
      "Epoch [20/1000], Step [45], D Loss: 5.3416, G Loss: -3.6798\n",
      "Epoch [20/1000], Step [50], D Loss: 5.6780, G Loss: -3.9484\n",
      "Epoch [20/1000], Step [55], D Loss: 5.6485, G Loss: -4.0936\n",
      "Epoch [20/1000], Step [60], D Loss: 5.6206, G Loss: -4.0576\n",
      "Epoch [20/1000], Step [65], D Loss: 5.7441, G Loss: -4.0319\n",
      "Epoch [20/1000], Step [70], D Loss: 5.7743, G Loss: -3.9880\n",
      "Epoch [20/1000], Step [75], D Loss: 5.5957, G Loss: -3.8683\n",
      "Epoch [21/1000], Step [1], D Loss: 5.5849, G Loss: -3.7974\n",
      "Epoch [21/1000], Step [5], D Loss: 5.5909, G Loss: -3.8589\n",
      "Epoch [21/1000], Step [10], D Loss: 5.7134, G Loss: -3.9773\n",
      "Epoch [21/1000], Step [15], D Loss: 5.6031, G Loss: -4.0319\n",
      "Epoch [21/1000], Step [20], D Loss: 5.5621, G Loss: -3.9396\n",
      "Epoch [21/1000], Step [25], D Loss: 5.5157, G Loss: -3.9646\n",
      "Epoch [21/1000], Step [30], D Loss: 5.6957, G Loss: -4.0217\n",
      "Epoch [21/1000], Step [35], D Loss: 5.6490, G Loss: -3.9510\n",
      "Epoch [21/1000], Step [40], D Loss: 5.7692, G Loss: -3.8459\n",
      "Epoch [21/1000], Step [45], D Loss: 5.4997, G Loss: -3.6884\n",
      "Epoch [21/1000], Step [50], D Loss: 5.3539, G Loss: -3.5972\n",
      "Epoch [21/1000], Step [55], D Loss: 5.5337, G Loss: -3.6757\n",
      "Epoch [21/1000], Step [60], D Loss: 5.6873, G Loss: -3.7468\n",
      "Epoch [21/1000], Step [65], D Loss: 5.6380, G Loss: -3.7333\n",
      "Epoch [21/1000], Step [70], D Loss: 5.6750, G Loss: -3.7528\n",
      "Epoch [21/1000], Step [75], D Loss: 5.6556, G Loss: -3.7400\n",
      "Epoch [22/1000], Step [1], D Loss: 5.6214, G Loss: -3.6561\n",
      "Epoch [22/1000], Step [5], D Loss: 5.5324, G Loss: -3.4992\n",
      "Epoch [22/1000], Step [10], D Loss: 5.2958, G Loss: -3.2778\n",
      "Epoch [22/1000], Step [15], D Loss: 5.2062, G Loss: -3.1346\n",
      "Epoch [22/1000], Step [20], D Loss: 5.0430, G Loss: -3.0448\n",
      "Epoch [22/1000], Step [25], D Loss: 5.1462, G Loss: -3.0617\n",
      "Epoch [22/1000], Step [30], D Loss: 5.3233, G Loss: -3.1773\n",
      "Epoch [22/1000], Step [35], D Loss: 5.2437, G Loss: -3.2373\n",
      "Epoch [22/1000], Step [40], D Loss: 5.1531, G Loss: -3.2987\n",
      "Epoch [22/1000], Step [45], D Loss: 5.0329, G Loss: -3.4253\n",
      "Epoch [22/1000], Step [50], D Loss: 5.2536, G Loss: -3.5557\n",
      "Epoch [22/1000], Step [55], D Loss: 5.4123, G Loss: -3.6695\n",
      "Epoch [22/1000], Step [60], D Loss: 5.2895, G Loss: -3.7018\n",
      "Epoch [22/1000], Step [65], D Loss: 5.2047, G Loss: -3.8251\n",
      "Epoch [22/1000], Step [70], D Loss: 5.4222, G Loss: -3.9858\n",
      "Epoch [22/1000], Step [75], D Loss: 5.6069, G Loss: -4.1767\n",
      "Epoch [23/1000], Step [1], D Loss: 5.5766, G Loss: -4.2372\n",
      "Epoch [23/1000], Step [5], D Loss: 5.4831, G Loss: -4.1645\n",
      "Epoch [23/1000], Step [10], D Loss: 5.6260, G Loss: -4.0880\n",
      "Epoch [23/1000], Step [15], D Loss: 5.2712, G Loss: -3.8717\n",
      "Epoch [23/1000], Step [20], D Loss: 5.1548, G Loss: -3.6934\n",
      "Epoch [23/1000], Step [25], D Loss: 5.3401, G Loss: -3.6833\n",
      "Epoch [23/1000], Step [30], D Loss: 5.3060, G Loss: -3.7193\n",
      "Epoch [23/1000], Step [35], D Loss: 5.1807, G Loss: -3.6493\n",
      "Epoch [23/1000], Step [40], D Loss: 5.3062, G Loss: -3.6118\n",
      "Epoch [23/1000], Step [45], D Loss: 5.3631, G Loss: -3.7231\n",
      "Epoch [23/1000], Step [50], D Loss: 5.5819, G Loss: -3.8331\n",
      "Epoch [23/1000], Step [55], D Loss: 5.6693, G Loss: -3.8154\n",
      "Epoch [23/1000], Step [60], D Loss: 5.4716, G Loss: -3.7387\n",
      "Epoch [23/1000], Step [65], D Loss: 5.3271, G Loss: -3.6561\n",
      "Epoch [23/1000], Step [70], D Loss: 5.1315, G Loss: -3.4990\n",
      "Epoch [23/1000], Step [75], D Loss: 5.2841, G Loss: -3.3918\n",
      "Epoch [24/1000], Step [1], D Loss: 5.0846, G Loss: -3.2457\n",
      "Epoch [24/1000], Step [5], D Loss: 5.1568, G Loss: -3.2171\n",
      "Epoch [24/1000], Step [10], D Loss: 5.1407, G Loss: -3.1912\n",
      "Epoch [24/1000], Step [15], D Loss: 5.0750, G Loss: -3.1262\n",
      "Epoch [24/1000], Step [20], D Loss: 4.9883, G Loss: -3.2460\n",
      "Epoch [24/1000], Step [25], D Loss: 4.9077, G Loss: -3.2980\n",
      "Epoch [24/1000], Step [30], D Loss: 5.0948, G Loss: -3.3866\n",
      "Epoch [24/1000], Step [35], D Loss: 5.2200, G Loss: -3.4708\n",
      "Epoch [24/1000], Step [40], D Loss: 5.0233, G Loss: -3.4858\n",
      "Epoch [24/1000], Step [45], D Loss: 4.9283, G Loss: -3.6080\n",
      "Epoch [24/1000], Step [50], D Loss: 5.2149, G Loss: -3.7187\n",
      "Epoch [24/1000], Step [55], D Loss: 5.2898, G Loss: -3.7651\n",
      "Epoch [24/1000], Step [60], D Loss: 5.2390, G Loss: -3.7078\n",
      "Epoch [24/1000], Step [65], D Loss: 5.3532, G Loss: -3.6822\n",
      "Epoch [24/1000], Step [70], D Loss: 5.3691, G Loss: -3.7042\n",
      "Epoch [24/1000], Step [75], D Loss: 5.3253, G Loss: -3.6645\n",
      "Epoch [25/1000], Step [1], D Loss: 5.4493, G Loss: -3.6559\n",
      "Epoch [25/1000], Step [5], D Loss: 5.5518, G Loss: -3.6578\n",
      "Epoch [25/1000], Step [10], D Loss: 5.5073, G Loss: -3.6097\n",
      "Epoch [25/1000], Step [15], D Loss: 5.3441, G Loss: -3.5672\n",
      "Epoch [25/1000], Step [20], D Loss: 5.2643, G Loss: -3.5679\n",
      "Epoch [25/1000], Step [25], D Loss: 5.3177, G Loss: -3.6094\n",
      "Epoch [25/1000], Step [30], D Loss: 5.5950, G Loss: -3.7758\n",
      "Epoch [25/1000], Step [35], D Loss: 5.5579, G Loss: -3.7909\n",
      "Epoch [25/1000], Step [40], D Loss: 5.4688, G Loss: -3.7207\n",
      "Epoch [25/1000], Step [45], D Loss: 5.4392, G Loss: -3.7699\n",
      "Epoch [25/1000], Step [50], D Loss: 5.5171, G Loss: -3.8246\n",
      "Epoch [25/1000], Step [55], D Loss: 5.4089, G Loss: -3.7271\n",
      "Epoch [25/1000], Step [60], D Loss: 5.3571, G Loss: -3.5795\n",
      "Epoch [25/1000], Step [65], D Loss: 5.3196, G Loss: -3.4462\n",
      "Epoch [25/1000], Step [70], D Loss: 5.1577, G Loss: -3.4837\n",
      "Epoch [25/1000], Step [75], D Loss: 4.9092, G Loss: -3.4568\n",
      "Epoch [26/1000], Step [1], D Loss: 4.7845, G Loss: -3.3682\n",
      "Epoch [26/1000], Step [5], D Loss: 4.6828, G Loss: -3.4290\n",
      "Epoch [26/1000], Step [10], D Loss: 4.9472, G Loss: -3.6119\n",
      "Epoch [26/1000], Step [15], D Loss: 5.2298, G Loss: -3.7062\n",
      "Epoch [26/1000], Step [20], D Loss: 5.2860, G Loss: -3.7143\n",
      "Epoch [26/1000], Step [25], D Loss: 5.3759, G Loss: -3.7525\n",
      "Epoch [26/1000], Step [30], D Loss: 5.3678, G Loss: -3.6869\n",
      "Epoch [26/1000], Step [35], D Loss: 5.0053, G Loss: -3.5591\n",
      "Epoch [26/1000], Step [40], D Loss: 4.8775, G Loss: -3.5106\n",
      "Epoch [26/1000], Step [45], D Loss: 4.7247, G Loss: -3.4145\n",
      "Epoch [26/1000], Step [50], D Loss: 4.8802, G Loss: -3.3996\n",
      "Epoch [26/1000], Step [55], D Loss: 5.1162, G Loss: -3.6041\n",
      "Epoch [26/1000], Step [60], D Loss: 5.0449, G Loss: -3.6964\n",
      "Epoch [26/1000], Step [65], D Loss: 4.9351, G Loss: -3.7082\n",
      "Epoch [26/1000], Step [70], D Loss: 5.0474, G Loss: -3.6750\n",
      "Epoch [26/1000], Step [75], D Loss: 5.2267, G Loss: -3.7254\n",
      "Epoch [27/1000], Step [1], D Loss: 5.2488, G Loss: -3.7671\n",
      "Epoch [27/1000], Step [5], D Loss: 5.1718, G Loss: -3.7801\n",
      "Epoch [27/1000], Step [10], D Loss: 5.0495, G Loss: -3.7484\n",
      "Epoch [27/1000], Step [15], D Loss: 5.0692, G Loss: -3.7151\n",
      "Epoch [27/1000], Step [20], D Loss: 5.1076, G Loss: -3.7089\n",
      "Epoch [27/1000], Step [25], D Loss: 5.2269, G Loss: -3.7050\n",
      "Epoch [27/1000], Step [30], D Loss: 5.1478, G Loss: -3.6406\n",
      "Epoch [27/1000], Step [35], D Loss: 5.1811, G Loss: -3.6103\n",
      "Epoch [27/1000], Step [40], D Loss: 5.2331, G Loss: -3.6679\n",
      "Epoch [27/1000], Step [45], D Loss: 5.4694, G Loss: -3.7055\n",
      "Epoch [27/1000], Step [50], D Loss: 5.3210, G Loss: -3.5608\n",
      "Epoch [27/1000], Step [55], D Loss: 5.2509, G Loss: -3.4705\n",
      "Epoch [27/1000], Step [60], D Loss: 5.0879, G Loss: -3.4058\n",
      "Epoch [27/1000], Step [65], D Loss: 5.0375, G Loss: -3.3615\n",
      "Epoch [27/1000], Step [70], D Loss: 5.1193, G Loss: -3.3912\n",
      "Epoch [27/1000], Step [75], D Loss: 5.1049, G Loss: -3.4672\n",
      "Epoch [28/1000], Step [1], D Loss: 4.9869, G Loss: -3.3914\n",
      "Epoch [28/1000], Step [5], D Loss: 4.8928, G Loss: -3.3391\n",
      "Epoch [28/1000], Step [10], D Loss: 4.8928, G Loss: -3.3137\n",
      "Epoch [28/1000], Step [15], D Loss: 4.8602, G Loss: -3.3370\n",
      "Epoch [28/1000], Step [20], D Loss: 4.9089, G Loss: -3.3699\n",
      "Epoch [28/1000], Step [25], D Loss: 4.8258, G Loss: -3.3997\n",
      "Epoch [28/1000], Step [30], D Loss: 4.8345, G Loss: -3.4978\n",
      "Epoch [28/1000], Step [35], D Loss: 4.9762, G Loss: -3.7273\n",
      "Epoch [28/1000], Step [40], D Loss: 4.9714, G Loss: -3.7214\n",
      "Epoch [28/1000], Step [45], D Loss: 4.7907, G Loss: -3.6350\n",
      "Epoch [28/1000], Step [50], D Loss: 4.7990, G Loss: -3.6829\n",
      "Epoch [28/1000], Step [55], D Loss: 4.9270, G Loss: -3.7876\n",
      "Epoch [28/1000], Step [60], D Loss: 4.8301, G Loss: -3.7060\n",
      "Epoch [28/1000], Step [65], D Loss: 4.9932, G Loss: -3.8012\n",
      "Epoch [28/1000], Step [70], D Loss: 5.1433, G Loss: -3.9066\n",
      "Epoch [28/1000], Step [75], D Loss: 5.1708, G Loss: -3.8813\n",
      "Epoch [29/1000], Step [1], D Loss: 5.1773, G Loss: -3.9433\n",
      "Epoch [29/1000], Step [5], D Loss: 5.1350, G Loss: -3.9340\n",
      "Epoch [29/1000], Step [10], D Loss: 5.0514, G Loss: -3.7959\n",
      "Epoch [29/1000], Step [15], D Loss: 5.0040, G Loss: -3.7492\n",
      "Epoch [29/1000], Step [20], D Loss: 5.0008, G Loss: -3.7053\n",
      "Epoch [29/1000], Step [25], D Loss: 4.8466, G Loss: -3.5478\n",
      "Epoch [29/1000], Step [30], D Loss: 4.6715, G Loss: -3.4669\n",
      "Epoch [29/1000], Step [35], D Loss: 4.6283, G Loss: -3.3521\n",
      "Epoch [29/1000], Step [40], D Loss: 4.6823, G Loss: -3.3549\n",
      "Epoch [29/1000], Step [45], D Loss: 4.7318, G Loss: -3.4081\n",
      "Epoch [29/1000], Step [50], D Loss: 4.7724, G Loss: -3.3891\n",
      "Epoch [29/1000], Step [55], D Loss: 4.7235, G Loss: -3.3159\n",
      "Epoch [29/1000], Step [60], D Loss: 4.5859, G Loss: -3.2925\n",
      "Epoch [29/1000], Step [65], D Loss: 4.4267, G Loss: -3.2405\n",
      "Epoch [29/1000], Step [70], D Loss: 4.4555, G Loss: -3.2217\n",
      "Epoch [29/1000], Step [75], D Loss: 4.5312, G Loss: -3.2111\n",
      "Epoch [30/1000], Step [1], D Loss: 4.5560, G Loss: -3.2549\n",
      "Epoch [30/1000], Step [5], D Loss: 4.6222, G Loss: -3.3095\n",
      "Epoch [30/1000], Step [10], D Loss: 4.7341, G Loss: -3.2745\n",
      "Epoch [30/1000], Step [15], D Loss: 4.9685, G Loss: -3.4386\n",
      "Epoch [30/1000], Step [20], D Loss: 5.0621, G Loss: -3.5770\n",
      "Epoch [30/1000], Step [25], D Loss: 5.0130, G Loss: -3.8518\n",
      "Epoch [30/1000], Step [30], D Loss: 5.1404, G Loss: -3.8585\n",
      "Epoch [30/1000], Step [35], D Loss: 4.9779, G Loss: -3.7531\n",
      "Epoch [30/1000], Step [40], D Loss: 5.0022, G Loss: -3.6539\n",
      "Epoch [30/1000], Step [45], D Loss: 4.9873, G Loss: -3.6620\n",
      "Epoch [30/1000], Step [50], D Loss: 4.9370, G Loss: -3.6527\n",
      "Epoch [30/1000], Step [55], D Loss: 4.9360, G Loss: -3.5715\n",
      "Epoch [30/1000], Step [60], D Loss: 4.7117, G Loss: -3.4757\n",
      "Epoch [30/1000], Step [65], D Loss: 4.5852, G Loss: -3.3334\n",
      "Epoch [30/1000], Step [70], D Loss: 4.5498, G Loss: -3.2451\n",
      "Epoch [30/1000], Step [75], D Loss: 4.8405, G Loss: -3.3058\n",
      "Epoch [31/1000], Step [1], D Loss: 4.6917, G Loss: -3.2765\n",
      "Epoch [31/1000], Step [5], D Loss: 4.4601, G Loss: -3.2643\n",
      "Epoch [31/1000], Step [10], D Loss: 4.5965, G Loss: -3.3534\n",
      "Epoch [31/1000], Step [15], D Loss: 4.4158, G Loss: -3.3805\n",
      "Epoch [31/1000], Step [20], D Loss: 4.5568, G Loss: -3.4566\n",
      "Epoch [31/1000], Step [25], D Loss: 4.7709, G Loss: -3.5915\n",
      "Epoch [31/1000], Step [30], D Loss: 4.9616, G Loss: -3.6300\n",
      "Epoch [31/1000], Step [35], D Loss: 4.8377, G Loss: -3.6067\n",
      "Epoch [31/1000], Step [40], D Loss: 4.7895, G Loss: -3.4817\n",
      "Epoch [31/1000], Step [45], D Loss: 4.9728, G Loss: -3.5171\n",
      "Epoch [31/1000], Step [50], D Loss: 5.0332, G Loss: -3.5969\n",
      "Epoch [31/1000], Step [55], D Loss: 5.0469, G Loss: -3.6913\n",
      "Epoch [31/1000], Step [60], D Loss: 5.0230, G Loss: -3.7650\n",
      "Epoch [31/1000], Step [65], D Loss: 5.0050, G Loss: -3.8118\n",
      "Epoch [31/1000], Step [70], D Loss: 5.0969, G Loss: -3.8494\n",
      "Epoch [31/1000], Step [75], D Loss: 5.1096, G Loss: -3.8557\n",
      "Epoch [32/1000], Step [1], D Loss: 5.0621, G Loss: -3.8161\n",
      "Epoch [32/1000], Step [5], D Loss: 5.1270, G Loss: -3.7988\n",
      "Epoch [32/1000], Step [10], D Loss: 5.0450, G Loss: -3.7549\n",
      "Epoch [32/1000], Step [15], D Loss: 4.9686, G Loss: -3.7284\n",
      "Epoch [32/1000], Step [20], D Loss: 5.1029, G Loss: -3.7309\n",
      "Epoch [32/1000], Step [25], D Loss: 5.0056, G Loss: -3.6955\n",
      "Epoch [32/1000], Step [30], D Loss: 4.8166, G Loss: -3.6012\n",
      "Epoch [32/1000], Step [35], D Loss: 4.9258, G Loss: -3.5470\n",
      "Epoch [32/1000], Step [40], D Loss: 4.8670, G Loss: -3.4426\n",
      "Epoch [32/1000], Step [45], D Loss: 4.8902, G Loss: -3.3650\n",
      "Epoch [32/1000], Step [50], D Loss: 4.7965, G Loss: -3.4356\n",
      "Epoch [32/1000], Step [55], D Loss: 4.7778, G Loss: -3.4655\n",
      "Epoch [32/1000], Step [60], D Loss: 4.6947, G Loss: -3.4136\n",
      "Epoch [32/1000], Step [65], D Loss: 4.8366, G Loss: -3.4625\n",
      "Epoch [32/1000], Step [70], D Loss: 4.6950, G Loss: -3.6440\n",
      "Epoch [32/1000], Step [75], D Loss: 4.7557, G Loss: -3.8004\n",
      "Epoch [33/1000], Step [1], D Loss: 4.8519, G Loss: -3.8863\n",
      "Epoch [33/1000], Step [5], D Loss: 5.0479, G Loss: -3.9499\n",
      "Epoch [33/1000], Step [10], D Loss: 5.0673, G Loss: -3.9484\n",
      "Epoch [33/1000], Step [15], D Loss: 5.2483, G Loss: -3.9816\n",
      "Epoch [33/1000], Step [20], D Loss: 5.2221, G Loss: -3.9914\n",
      "Epoch [33/1000], Step [25], D Loss: 5.2744, G Loss: -3.9906\n",
      "Epoch [33/1000], Step [30], D Loss: 5.0091, G Loss: -3.9797\n",
      "Epoch [33/1000], Step [35], D Loss: 5.0239, G Loss: -3.9197\n",
      "Epoch [33/1000], Step [40], D Loss: 4.8630, G Loss: -3.7765\n",
      "Epoch [33/1000], Step [45], D Loss: 4.9189, G Loss: -3.7643\n",
      "Epoch [33/1000], Step [50], D Loss: 4.7788, G Loss: -3.6958\n",
      "Epoch [33/1000], Step [55], D Loss: 4.7691, G Loss: -3.6992\n",
      "Epoch [33/1000], Step [60], D Loss: 4.8656, G Loss: -3.6450\n",
      "Epoch [33/1000], Step [65], D Loss: 4.7277, G Loss: -3.6209\n",
      "Epoch [33/1000], Step [70], D Loss: 4.8296, G Loss: -3.6593\n",
      "Epoch [33/1000], Step [75], D Loss: 4.6538, G Loss: -3.6274\n",
      "Epoch [34/1000], Step [1], D Loss: 4.8176, G Loss: -3.6328\n",
      "Epoch [34/1000], Step [5], D Loss: 4.8522, G Loss: -3.6172\n",
      "Epoch [34/1000], Step [10], D Loss: 4.7211, G Loss: -3.5218\n",
      "Epoch [34/1000], Step [15], D Loss: 4.8897, G Loss: -3.5314\n",
      "Epoch [34/1000], Step [20], D Loss: 4.6915, G Loss: -3.5267\n",
      "Epoch [34/1000], Step [25], D Loss: 4.9020, G Loss: -3.6037\n",
      "Epoch [34/1000], Step [30], D Loss: 4.9508, G Loss: -3.6760\n",
      "Epoch [34/1000], Step [35], D Loss: 5.0724, G Loss: -3.8474\n",
      "Epoch [34/1000], Step [40], D Loss: 5.0858, G Loss: -3.9138\n",
      "Epoch [34/1000], Step [45], D Loss: 5.1316, G Loss: -3.9282\n",
      "Epoch [34/1000], Step [50], D Loss: 5.3043, G Loss: -3.9702\n",
      "Epoch [34/1000], Step [55], D Loss: 5.2032, G Loss: -3.9260\n",
      "Epoch [34/1000], Step [60], D Loss: 5.1953, G Loss: -3.8460\n",
      "Epoch [34/1000], Step [65], D Loss: 5.0319, G Loss: -3.7102\n",
      "Epoch [34/1000], Step [70], D Loss: 4.8420, G Loss: -3.6063\n",
      "Epoch [34/1000], Step [75], D Loss: 4.9319, G Loss: -3.6109\n",
      "Epoch [35/1000], Step [1], D Loss: 4.8521, G Loss: -3.5519\n",
      "Epoch [35/1000], Step [5], D Loss: 4.8202, G Loss: -3.5122\n",
      "Epoch [35/1000], Step [10], D Loss: 4.8191, G Loss: -3.5074\n",
      "Epoch [35/1000], Step [15], D Loss: 4.6692, G Loss: -3.4918\n",
      "Epoch [35/1000], Step [20], D Loss: 4.6605, G Loss: -3.4790\n",
      "Epoch [35/1000], Step [25], D Loss: 4.6549, G Loss: -3.5813\n",
      "Epoch [35/1000], Step [30], D Loss: 4.8020, G Loss: -3.6644\n",
      "Epoch [35/1000], Step [35], D Loss: 4.9437, G Loss: -3.7926\n",
      "Epoch [35/1000], Step [40], D Loss: 4.9975, G Loss: -3.7956\n",
      "Epoch [35/1000], Step [45], D Loss: 5.0935, G Loss: -3.8406\n",
      "Epoch [35/1000], Step [50], D Loss: 5.1340, G Loss: -3.8608\n",
      "Epoch [35/1000], Step [55], D Loss: 5.0392, G Loss: -3.7709\n",
      "Epoch [35/1000], Step [60], D Loss: 4.9572, G Loss: -3.6799\n",
      "Epoch [35/1000], Step [65], D Loss: 5.0064, G Loss: -3.6348\n",
      "Epoch [35/1000], Step [70], D Loss: 5.0114, G Loss: -3.8523\n",
      "Epoch [35/1000], Step [75], D Loss: 5.2081, G Loss: -3.9773\n",
      "Epoch [36/1000], Step [1], D Loss: 5.1322, G Loss: -4.0444\n",
      "Epoch [36/1000], Step [5], D Loss: 5.1138, G Loss: -4.0263\n",
      "Epoch [36/1000], Step [10], D Loss: 4.8225, G Loss: -3.9317\n",
      "Epoch [36/1000], Step [15], D Loss: 4.8054, G Loss: -3.8042\n",
      "Epoch [36/1000], Step [20], D Loss: 4.5943, G Loss: -3.6339\n",
      "Epoch [36/1000], Step [25], D Loss: 4.6323, G Loss: -3.5327\n",
      "Epoch [36/1000], Step [30], D Loss: 4.5675, G Loss: -3.4338\n",
      "Epoch [36/1000], Step [35], D Loss: 4.6209, G Loss: -3.4161\n",
      "Epoch [36/1000], Step [40], D Loss: 4.6854, G Loss: -3.4475\n",
      "Epoch [36/1000], Step [45], D Loss: 4.5810, G Loss: -3.5236\n",
      "Epoch [36/1000], Step [50], D Loss: 4.5572, G Loss: -3.5977\n",
      "Epoch [36/1000], Step [55], D Loss: 4.6317, G Loss: -3.6246\n",
      "Epoch [36/1000], Step [60], D Loss: 4.4912, G Loss: -3.6405\n",
      "Epoch [36/1000], Step [65], D Loss: 4.6463, G Loss: -3.6913\n",
      "Epoch [36/1000], Step [70], D Loss: 4.8354, G Loss: -3.8086\n",
      "Epoch [36/1000], Step [75], D Loss: 4.9987, G Loss: -3.8944\n",
      "Epoch [37/1000], Step [1], D Loss: 4.9286, G Loss: -3.9357\n",
      "Epoch [37/1000], Step [5], D Loss: 4.9573, G Loss: -3.9215\n",
      "Epoch [37/1000], Step [10], D Loss: 4.7474, G Loss: -3.8374\n",
      "Epoch [37/1000], Step [15], D Loss: 4.7674, G Loss: -3.8030\n",
      "Epoch [37/1000], Step [20], D Loss: 4.8123, G Loss: -3.7916\n",
      "Epoch [37/1000], Step [25], D Loss: 4.6402, G Loss: -3.8000\n",
      "Epoch [37/1000], Step [30], D Loss: 4.7649, G Loss: -3.8101\n",
      "Epoch [37/1000], Step [35], D Loss: 4.8166, G Loss: -3.8243\n",
      "Epoch [37/1000], Step [40], D Loss: 4.8895, G Loss: -3.8634\n",
      "Epoch [37/1000], Step [45], D Loss: 4.8284, G Loss: -3.8220\n",
      "Epoch [37/1000], Step [50], D Loss: 4.8135, G Loss: -3.6894\n",
      "Epoch [37/1000], Step [55], D Loss: 4.6201, G Loss: -3.6284\n",
      "Epoch [37/1000], Step [60], D Loss: 4.6125, G Loss: -3.5860\n",
      "Epoch [37/1000], Step [65], D Loss: 4.6164, G Loss: -3.5357\n",
      "Epoch [37/1000], Step [70], D Loss: 4.4736, G Loss: -3.4855\n",
      "Epoch [37/1000], Step [75], D Loss: 4.5196, G Loss: -3.5480\n",
      "Epoch [38/1000], Step [1], D Loss: 4.4321, G Loss: -3.5883\n",
      "Epoch [38/1000], Step [5], D Loss: 4.3915, G Loss: -3.5570\n",
      "Epoch [38/1000], Step [10], D Loss: 4.4005, G Loss: -3.5209\n",
      "Epoch [38/1000], Step [15], D Loss: 4.3692, G Loss: -3.4954\n",
      "Epoch [38/1000], Step [20], D Loss: 4.4065, G Loss: -3.4815\n",
      "Epoch [38/1000], Step [25], D Loss: 4.5061, G Loss: -3.5337\n",
      "Epoch [38/1000], Step [30], D Loss: 4.7932, G Loss: -3.5772\n",
      "Epoch [38/1000], Step [35], D Loss: 4.6417, G Loss: -3.6196\n",
      "Epoch [38/1000], Step [40], D Loss: 4.6808, G Loss: -3.6005\n",
      "Epoch [38/1000], Step [45], D Loss: 4.8307, G Loss: -3.7235\n",
      "Epoch [38/1000], Step [50], D Loss: 4.9293, G Loss: -3.8679\n",
      "Epoch [38/1000], Step [55], D Loss: 4.9364, G Loss: -3.8832\n",
      "Epoch [38/1000], Step [60], D Loss: 4.8967, G Loss: -3.8189\n",
      "Epoch [38/1000], Step [65], D Loss: 4.6905, G Loss: -3.7029\n",
      "Epoch [38/1000], Step [70], D Loss: 4.6337, G Loss: -3.5950\n",
      "Epoch [38/1000], Step [75], D Loss: 4.5342, G Loss: -3.5113\n",
      "Epoch [39/1000], Step [1], D Loss: 4.5671, G Loss: -3.4750\n",
      "Epoch [39/1000], Step [5], D Loss: 4.5172, G Loss: -3.4249\n",
      "Epoch [39/1000], Step [10], D Loss: 4.6221, G Loss: -3.4903\n",
      "Epoch [39/1000], Step [15], D Loss: 4.6153, G Loss: -3.4472\n",
      "Epoch [39/1000], Step [20], D Loss: 4.4008, G Loss: -3.3437\n",
      "Epoch [39/1000], Step [25], D Loss: 4.3897, G Loss: -3.3031\n",
      "Epoch [39/1000], Step [30], D Loss: 4.4641, G Loss: -3.5218\n",
      "Epoch [39/1000], Step [35], D Loss: 4.7208, G Loss: -3.6686\n",
      "Epoch [39/1000], Step [40], D Loss: 4.7605, G Loss: -3.7199\n",
      "Epoch [39/1000], Step [45], D Loss: 4.6832, G Loss: -3.6414\n",
      "Epoch [39/1000], Step [50], D Loss: 4.8264, G Loss: -3.7251\n",
      "Epoch [39/1000], Step [55], D Loss: 4.9101, G Loss: -3.8993\n",
      "Epoch [39/1000], Step [60], D Loss: 4.8739, G Loss: -3.9429\n",
      "Epoch [39/1000], Step [65], D Loss: 4.9312, G Loss: -3.8927\n",
      "Epoch [39/1000], Step [70], D Loss: 4.9049, G Loss: -3.8506\n",
      "Epoch [39/1000], Step [75], D Loss: 4.6880, G Loss: -3.7247\n",
      "Epoch [40/1000], Step [1], D Loss: 4.6480, G Loss: -3.6025\n",
      "Epoch [40/1000], Step [5], D Loss: 4.4305, G Loss: -3.5108\n",
      "Epoch [40/1000], Step [10], D Loss: 4.4288, G Loss: -3.6348\n",
      "Epoch [40/1000], Step [15], D Loss: 4.7982, G Loss: -3.7723\n",
      "Epoch [40/1000], Step [20], D Loss: 4.9200, G Loss: -3.8754\n",
      "Epoch [40/1000], Step [25], D Loss: 4.8056, G Loss: -3.8943\n",
      "Epoch [40/1000], Step [30], D Loss: 4.8354, G Loss: -3.8442\n",
      "Epoch [40/1000], Step [35], D Loss: 4.9021, G Loss: -3.8482\n",
      "Epoch [40/1000], Step [40], D Loss: 4.8791, G Loss: -3.8080\n",
      "Epoch [40/1000], Step [45], D Loss: 4.8146, G Loss: -3.7028\n",
      "Epoch [40/1000], Step [50], D Loss: 4.7820, G Loss: -3.7171\n",
      "Epoch [40/1000], Step [55], D Loss: 4.6441, G Loss: -3.7046\n",
      "Epoch [40/1000], Step [60], D Loss: 4.6001, G Loss: -3.5978\n",
      "Epoch [40/1000], Step [65], D Loss: 4.4677, G Loss: -3.4537\n",
      "Epoch [40/1000], Step [70], D Loss: 4.3940, G Loss: -3.3530\n",
      "Epoch [40/1000], Step [75], D Loss: 4.3311, G Loss: -3.3537\n",
      "Epoch [41/1000], Step [1], D Loss: 4.3817, G Loss: -3.3453\n",
      "Epoch [41/1000], Step [5], D Loss: 4.5338, G Loss: -3.3463\n",
      "Epoch [41/1000], Step [10], D Loss: 4.5559, G Loss: -3.4225\n",
      "Epoch [41/1000], Step [15], D Loss: 4.5483, G Loss: -3.4016\n",
      "Epoch [41/1000], Step [20], D Loss: 4.4129, G Loss: -3.3911\n",
      "Epoch [41/1000], Step [25], D Loss: 4.3150, G Loss: -3.4699\n",
      "Epoch [41/1000], Step [30], D Loss: 4.3314, G Loss: -3.4826\n",
      "Epoch [41/1000], Step [35], D Loss: 4.3860, G Loss: -3.5546\n",
      "Epoch [41/1000], Step [40], D Loss: 4.4121, G Loss: -3.6324\n",
      "Epoch [41/1000], Step [45], D Loss: 4.5322, G Loss: -3.7669\n",
      "Epoch [41/1000], Step [50], D Loss: 4.6861, G Loss: -3.8540\n",
      "Epoch [41/1000], Step [55], D Loss: 4.6931, G Loss: -3.8526\n",
      "Epoch [41/1000], Step [60], D Loss: 4.7467, G Loss: -3.8364\n",
      "Epoch [41/1000], Step [65], D Loss: 4.5736, G Loss: -3.7934\n",
      "Epoch [41/1000], Step [70], D Loss: 4.6075, G Loss: -3.7855\n",
      "Epoch [41/1000], Step [75], D Loss: 4.6542, G Loss: -3.8571\n",
      "Epoch [42/1000], Step [1], D Loss: 4.6856, G Loss: -3.8627\n",
      "Epoch [42/1000], Step [5], D Loss: 4.6809, G Loss: -3.8642\n",
      "Epoch [42/1000], Step [10], D Loss: 4.6912, G Loss: -3.8133\n",
      "Epoch [42/1000], Step [15], D Loss: 4.6226, G Loss: -3.7727\n",
      "Epoch [42/1000], Step [20], D Loss: 4.5051, G Loss: -3.6935\n",
      "Epoch [42/1000], Step [25], D Loss: 4.5362, G Loss: -3.6269\n",
      "Epoch [42/1000], Step [30], D Loss: 4.4374, G Loss: -3.5286\n",
      "Epoch [42/1000], Step [35], D Loss: 4.2970, G Loss: -3.3838\n",
      "Epoch [42/1000], Step [40], D Loss: 4.2098, G Loss: -3.3003\n",
      "Epoch [42/1000], Step [45], D Loss: 4.1710, G Loss: -3.2828\n",
      "Epoch [42/1000], Step [50], D Loss: 4.0891, G Loss: -3.2563\n",
      "Epoch [42/1000], Step [55], D Loss: 4.3077, G Loss: -3.3669\n",
      "Epoch [42/1000], Step [60], D Loss: 4.4687, G Loss: -3.3690\n",
      "Epoch [42/1000], Step [65], D Loss: 4.4930, G Loss: -3.4039\n",
      "Epoch [42/1000], Step [70], D Loss: 4.5322, G Loss: -3.4360\n",
      "Epoch [42/1000], Step [75], D Loss: 4.4498, G Loss: -3.5687\n",
      "Epoch [43/1000], Step [1], D Loss: 4.4341, G Loss: -3.6648\n",
      "Epoch [43/1000], Step [5], D Loss: 4.5917, G Loss: -3.7730\n",
      "Epoch [43/1000], Step [10], D Loss: 4.6715, G Loss: -3.8354\n",
      "Epoch [43/1000], Step [15], D Loss: 4.7251, G Loss: -3.8436\n",
      "Epoch [43/1000], Step [20], D Loss: 4.7021, G Loss: -3.7516\n",
      "Epoch [43/1000], Step [25], D Loss: 4.6099, G Loss: -3.7293\n",
      "Epoch [43/1000], Step [30], D Loss: 4.5686, G Loss: -3.6910\n",
      "Epoch [43/1000], Step [35], D Loss: 4.5413, G Loss: -3.6753\n",
      "Epoch [43/1000], Step [40], D Loss: 4.5163, G Loss: -3.6342\n",
      "Epoch [43/1000], Step [45], D Loss: 4.3934, G Loss: -3.5291\n",
      "Epoch [43/1000], Step [50], D Loss: 4.2548, G Loss: -3.4382\n",
      "Epoch [43/1000], Step [55], D Loss: 4.2494, G Loss: -3.4170\n",
      "Epoch [43/1000], Step [60], D Loss: 4.2758, G Loss: -3.3298\n",
      "Epoch [43/1000], Step [65], D Loss: 4.3631, G Loss: -3.3148\n",
      "Epoch [43/1000], Step [70], D Loss: 4.4253, G Loss: -3.3830\n",
      "Epoch [43/1000], Step [75], D Loss: 4.3120, G Loss: -3.4210\n",
      "Epoch [44/1000], Step [1], D Loss: 4.4626, G Loss: -3.4677\n",
      "Epoch [44/1000], Step [5], D Loss: 4.3667, G Loss: -3.4944\n",
      "Epoch [44/1000], Step [10], D Loss: 4.4293, G Loss: -3.5181\n",
      "Epoch [44/1000], Step [15], D Loss: 4.3797, G Loss: -3.4863\n",
      "Epoch [44/1000], Step [20], D Loss: 4.2973, G Loss: -3.4332\n",
      "Epoch [44/1000], Step [25], D Loss: 4.3115, G Loss: -3.4751\n",
      "Epoch [44/1000], Step [30], D Loss: 4.3642, G Loss: -3.5629\n",
      "Epoch [44/1000], Step [35], D Loss: 4.2780, G Loss: -3.6201\n",
      "Epoch [44/1000], Step [40], D Loss: 4.3688, G Loss: -3.7371\n",
      "Epoch [44/1000], Step [45], D Loss: 4.5482, G Loss: -3.7607\n",
      "Epoch [44/1000], Step [50], D Loss: 4.6380, G Loss: -3.7938\n",
      "Epoch [44/1000], Step [55], D Loss: 4.7785, G Loss: -3.8647\n",
      "Epoch [44/1000], Step [60], D Loss: 4.6380, G Loss: -3.8079\n",
      "Epoch [44/1000], Step [65], D Loss: 4.6432, G Loss: -3.6674\n",
      "Epoch [44/1000], Step [70], D Loss: 4.3610, G Loss: -3.4939\n",
      "Epoch [44/1000], Step [75], D Loss: 4.2301, G Loss: -3.3761\n",
      "Epoch [45/1000], Step [1], D Loss: 4.2055, G Loss: -3.3461\n",
      "Epoch [45/1000], Step [5], D Loss: 4.1785, G Loss: -3.4527\n",
      "Epoch [45/1000], Step [10], D Loss: 4.2359, G Loss: -3.5330\n",
      "Epoch [45/1000], Step [15], D Loss: 4.3223, G Loss: -3.5460\n",
      "Epoch [45/1000], Step [20], D Loss: 4.3794, G Loss: -3.5231\n",
      "Epoch [45/1000], Step [25], D Loss: 4.3637, G Loss: -3.5320\n",
      "Epoch [45/1000], Step [30], D Loss: 4.3516, G Loss: -3.5291\n",
      "Epoch [45/1000], Step [35], D Loss: 4.3079, G Loss: -3.4830\n",
      "Epoch [45/1000], Step [40], D Loss: 4.3052, G Loss: -3.4248\n",
      "Epoch [45/1000], Step [45], D Loss: 4.1984, G Loss: -3.4704\n",
      "Epoch [45/1000], Step [50], D Loss: 4.2989, G Loss: -3.4916\n",
      "Epoch [45/1000], Step [55], D Loss: 4.3266, G Loss: -3.5663\n",
      "Epoch [45/1000], Step [60], D Loss: 4.6311, G Loss: -3.6740\n",
      "Epoch [45/1000], Step [65], D Loss: 4.6779, G Loss: -3.7628\n",
      "Epoch [45/1000], Step [70], D Loss: 4.6722, G Loss: -3.7567\n",
      "Epoch [45/1000], Step [75], D Loss: 4.6032, G Loss: -3.7652\n",
      "Epoch [46/1000], Step [1], D Loss: 4.5883, G Loss: -3.7046\n",
      "Epoch [46/1000], Step [5], D Loss: 4.5357, G Loss: -3.6310\n",
      "Epoch [46/1000], Step [10], D Loss: 4.4167, G Loss: -3.5327\n",
      "Epoch [46/1000], Step [15], D Loss: 4.3676, G Loss: -3.4791\n",
      "Epoch [46/1000], Step [20], D Loss: 4.2884, G Loss: -3.4565\n",
      "Epoch [46/1000], Step [25], D Loss: 4.2516, G Loss: -3.4339\n",
      "Epoch [46/1000], Step [30], D Loss: 4.3128, G Loss: -3.4348\n",
      "Epoch [46/1000], Step [35], D Loss: 4.3904, G Loss: -3.4760\n",
      "Epoch [46/1000], Step [40], D Loss: 4.3554, G Loss: -3.4936\n",
      "Epoch [46/1000], Step [45], D Loss: 4.2155, G Loss: -3.4278\n",
      "Epoch [46/1000], Step [50], D Loss: 4.1243, G Loss: -3.2940\n",
      "Epoch [46/1000], Step [55], D Loss: 3.8788, G Loss: -3.2370\n",
      "Epoch [46/1000], Step [60], D Loss: 3.9343, G Loss: -3.2912\n",
      "Epoch [46/1000], Step [65], D Loss: 4.2677, G Loss: -3.4311\n",
      "Epoch [46/1000], Step [70], D Loss: 4.4369, G Loss: -3.5843\n",
      "Epoch [46/1000], Step [75], D Loss: 4.6190, G Loss: -3.8429\n",
      "Epoch [47/1000], Step [1], D Loss: 4.7423, G Loss: -3.9349\n",
      "Epoch [47/1000], Step [5], D Loss: 4.7208, G Loss: -3.9788\n",
      "Epoch [47/1000], Step [10], D Loss: 4.8013, G Loss: -3.9755\n",
      "Epoch [47/1000], Step [15], D Loss: 4.7848, G Loss: -3.9465\n",
      "Epoch [47/1000], Step [20], D Loss: 4.6426, G Loss: -3.8015\n",
      "Epoch [47/1000], Step [25], D Loss: 4.4998, G Loss: -3.7450\n",
      "Epoch [47/1000], Step [30], D Loss: 4.4455, G Loss: -3.6357\n",
      "Epoch [47/1000], Step [35], D Loss: 4.2810, G Loss: -3.5347\n",
      "Epoch [47/1000], Step [40], D Loss: 4.3611, G Loss: -3.4999\n",
      "Epoch [47/1000], Step [45], D Loss: 4.4996, G Loss: -3.5238\n",
      "Epoch [47/1000], Step [50], D Loss: 4.4139, G Loss: -3.4493\n",
      "Epoch [47/1000], Step [55], D Loss: 4.1910, G Loss: -3.2749\n",
      "Epoch [47/1000], Step [60], D Loss: 4.1120, G Loss: -3.1749\n",
      "Epoch [47/1000], Step [65], D Loss: 3.9433, G Loss: -3.1889\n",
      "Epoch [47/1000], Step [70], D Loss: 4.0676, G Loss: -3.3579\n",
      "Epoch [47/1000], Step [75], D Loss: 4.2193, G Loss: -3.4947\n",
      "Epoch [48/1000], Step [1], D Loss: 4.2153, G Loss: -3.5257\n",
      "Epoch [48/1000], Step [5], D Loss: 4.1131, G Loss: -3.5121\n",
      "Epoch [48/1000], Step [10], D Loss: 4.0880, G Loss: -3.5332\n",
      "Epoch [48/1000], Step [15], D Loss: 4.2601, G Loss: -3.5855\n",
      "Epoch [48/1000], Step [20], D Loss: 4.1634, G Loss: -3.5981\n",
      "Epoch [48/1000], Step [25], D Loss: 4.3436, G Loss: -3.6485\n",
      "Epoch [48/1000], Step [30], D Loss: 4.5007, G Loss: -3.7286\n",
      "Epoch [48/1000], Step [35], D Loss: 4.5785, G Loss: -3.8104\n",
      "Epoch [48/1000], Step [40], D Loss: 4.6359, G Loss: -3.8623\n",
      "Epoch [48/1000], Step [45], D Loss: 4.6502, G Loss: -3.8689\n",
      "Epoch [48/1000], Step [50], D Loss: 4.7147, G Loss: -3.8720\n",
      "Epoch [48/1000], Step [55], D Loss: 4.5669, G Loss: -3.8044\n",
      "Epoch [48/1000], Step [60], D Loss: 4.4685, G Loss: -3.6859\n",
      "Epoch [48/1000], Step [65], D Loss: 4.3748, G Loss: -3.5746\n",
      "Epoch [48/1000], Step [70], D Loss: 4.2697, G Loss: -3.4413\n",
      "Epoch [48/1000], Step [75], D Loss: 4.2499, G Loss: -3.3384\n",
      "Epoch [49/1000], Step [1], D Loss: 4.2065, G Loss: -3.2241\n",
      "Epoch [49/1000], Step [5], D Loss: 4.1737, G Loss: -3.1813\n",
      "Epoch [49/1000], Step [10], D Loss: 4.1304, G Loss: -3.1617\n",
      "Epoch [49/1000], Step [15], D Loss: 4.0516, G Loss: -3.1632\n",
      "Epoch [49/1000], Step [20], D Loss: 4.2481, G Loss: -3.2879\n",
      "Epoch [49/1000], Step [25], D Loss: 4.2850, G Loss: -3.3108\n",
      "Epoch [49/1000], Step [30], D Loss: 4.0836, G Loss: -3.3335\n",
      "Epoch [49/1000], Step [35], D Loss: 4.3248, G Loss: -3.5614\n",
      "Epoch [49/1000], Step [40], D Loss: 4.2844, G Loss: -3.6646\n",
      "Epoch [49/1000], Step [45], D Loss: 4.4151, G Loss: -3.6814\n",
      "Epoch [49/1000], Step [50], D Loss: 4.4978, G Loss: -3.7323\n",
      "Epoch [49/1000], Step [55], D Loss: 4.5557, G Loss: -3.7230\n",
      "Epoch [49/1000], Step [60], D Loss: 4.3599, G Loss: -3.6674\n",
      "Epoch [49/1000], Step [65], D Loss: 4.4575, G Loss: -3.6726\n",
      "Epoch [49/1000], Step [70], D Loss: 4.3753, G Loss: -3.5917\n",
      "Epoch [49/1000], Step [75], D Loss: 4.2934, G Loss: -3.5256\n",
      "Epoch [50/1000], Step [1], D Loss: 4.2353, G Loss: -3.5733\n",
      "Epoch [50/1000], Step [5], D Loss: 4.1963, G Loss: -3.5942\n",
      "Epoch [50/1000], Step [10], D Loss: 4.3207, G Loss: -3.6289\n",
      "Epoch [50/1000], Step [15], D Loss: 4.3712, G Loss: -3.6509\n",
      "Epoch [50/1000], Step [20], D Loss: 4.3650, G Loss: -3.5872\n",
      "Epoch [50/1000], Step [25], D Loss: 4.1691, G Loss: -3.4311\n",
      "Epoch [50/1000], Step [30], D Loss: 4.1182, G Loss: -3.3671\n",
      "Epoch [50/1000], Step [35], D Loss: 4.1556, G Loss: -3.3665\n",
      "Epoch [50/1000], Step [40], D Loss: 4.0984, G Loss: -3.3195\n",
      "Epoch [50/1000], Step [45], D Loss: 3.9937, G Loss: -3.2555\n",
      "Epoch [50/1000], Step [50], D Loss: 3.9439, G Loss: -3.2201\n",
      "Epoch [50/1000], Step [55], D Loss: 4.1226, G Loss: -3.2402\n",
      "Epoch [50/1000], Step [60], D Loss: 4.0691, G Loss: -3.2721\n",
      "Epoch [50/1000], Step [65], D Loss: 4.1316, G Loss: -3.3029\n",
      "Epoch [50/1000], Step [70], D Loss: 4.1749, G Loss: -3.4001\n",
      "Epoch [50/1000], Step [75], D Loss: 4.2009, G Loss: -3.5650\n",
      "Epoch [51/1000], Step [1], D Loss: 4.2371, G Loss: -3.5682\n",
      "Epoch [51/1000], Step [5], D Loss: 4.1997, G Loss: -3.4781\n",
      "Epoch [51/1000], Step [10], D Loss: 4.0864, G Loss: -3.4047\n",
      "Epoch [51/1000], Step [15], D Loss: 4.1727, G Loss: -3.3796\n",
      "Epoch [51/1000], Step [20], D Loss: 4.0896, G Loss: -3.3625\n",
      "Epoch [51/1000], Step [25], D Loss: 4.1660, G Loss: -3.4321\n",
      "Epoch [51/1000], Step [30], D Loss: 4.2879, G Loss: -3.5785\n",
      "Epoch [51/1000], Step [35], D Loss: 4.4259, G Loss: -3.6685\n",
      "Epoch [51/1000], Step [40], D Loss: 4.3962, G Loss: -3.6758\n",
      "Epoch [51/1000], Step [45], D Loss: 4.5478, G Loss: -3.6656\n",
      "Epoch [51/1000], Step [50], D Loss: 4.3690, G Loss: -3.5973\n",
      "Epoch [51/1000], Step [55], D Loss: 4.2874, G Loss: -3.4874\n",
      "Epoch [51/1000], Step [60], D Loss: 4.1771, G Loss: -3.4399\n",
      "Epoch [51/1000], Step [65], D Loss: 4.0699, G Loss: -3.3603\n",
      "Epoch [51/1000], Step [70], D Loss: 4.1360, G Loss: -3.3127\n",
      "Epoch [51/1000], Step [75], D Loss: 4.0663, G Loss: -3.3311\n",
      "Epoch [52/1000], Step [1], D Loss: 4.1649, G Loss: -3.3957\n",
      "Epoch [52/1000], Step [5], D Loss: 4.1133, G Loss: -3.3816\n",
      "Epoch [52/1000], Step [10], D Loss: 4.2846, G Loss: -3.3709\n",
      "Epoch [52/1000], Step [15], D Loss: 4.1352, G Loss: -3.3510\n",
      "Epoch [52/1000], Step [20], D Loss: 4.0864, G Loss: -3.2797\n",
      "Epoch [52/1000], Step [25], D Loss: 4.0528, G Loss: -3.2101\n",
      "Epoch [52/1000], Step [30], D Loss: 3.9639, G Loss: -3.1756\n",
      "Epoch [52/1000], Step [35], D Loss: 3.9030, G Loss: -3.1752\n",
      "Epoch [52/1000], Step [40], D Loss: 3.9552, G Loss: -3.2057\n",
      "Epoch [52/1000], Step [45], D Loss: 4.0071, G Loss: -3.3011\n",
      "Epoch [52/1000], Step [50], D Loss: 4.1308, G Loss: -3.4619\n",
      "Epoch [52/1000], Step [55], D Loss: 4.1755, G Loss: -3.5500\n",
      "Epoch [52/1000], Step [60], D Loss: 4.1295, G Loss: -3.5935\n",
      "Epoch [52/1000], Step [65], D Loss: 4.1758, G Loss: -3.6258\n",
      "Epoch [52/1000], Step [70], D Loss: 4.2610, G Loss: -3.6011\n",
      "Epoch [52/1000], Step [75], D Loss: 4.1316, G Loss: -3.5595\n",
      "Epoch [53/1000], Step [1], D Loss: 4.1973, G Loss: -3.5497\n",
      "Epoch [53/1000], Step [5], D Loss: 4.1785, G Loss: -3.5463\n",
      "Epoch [53/1000], Step [10], D Loss: 4.3608, G Loss: -3.5383\n",
      "Epoch [53/1000], Step [15], D Loss: 4.1039, G Loss: -3.4555\n",
      "Epoch [53/1000], Step [20], D Loss: 4.1413, G Loss: -3.4135\n",
      "Epoch [53/1000], Step [25], D Loss: 4.1200, G Loss: -3.3818\n",
      "Epoch [53/1000], Step [30], D Loss: 4.1177, G Loss: -3.3671\n",
      "Epoch [53/1000], Step [35], D Loss: 3.9473, G Loss: -3.3297\n",
      "Epoch [53/1000], Step [40], D Loss: 4.0010, G Loss: -3.2615\n",
      "Epoch [53/1000], Step [45], D Loss: 3.8322, G Loss: -3.2007\n",
      "Epoch [53/1000], Step [50], D Loss: 3.8022, G Loss: -3.1160\n",
      "Epoch [53/1000], Step [55], D Loss: 3.7786, G Loss: -3.0777\n",
      "Epoch [53/1000], Step [60], D Loss: 3.7590, G Loss: -3.0731\n",
      "Epoch [53/1000], Step [65], D Loss: 3.8647, G Loss: -3.1828\n",
      "Epoch [53/1000], Step [70], D Loss: 4.0958, G Loss: -3.3577\n",
      "Epoch [53/1000], Step [75], D Loss: 4.0163, G Loss: -3.3951\n",
      "Epoch [54/1000], Step [1], D Loss: 4.0546, G Loss: -3.3721\n",
      "Epoch [54/1000], Step [5], D Loss: 4.0390, G Loss: -3.3503\n",
      "Epoch [54/1000], Step [10], D Loss: 4.1451, G Loss: -3.3640\n",
      "Epoch [54/1000], Step [15], D Loss: 4.0742, G Loss: -3.3536\n",
      "Epoch [54/1000], Step [20], D Loss: 4.1113, G Loss: -3.4488\n",
      "Epoch [54/1000], Step [25], D Loss: 4.1763, G Loss: -3.5287\n",
      "Epoch [54/1000], Step [30], D Loss: 4.1534, G Loss: -3.5143\n",
      "Epoch [54/1000], Step [35], D Loss: 4.1755, G Loss: -3.4981\n",
      "Epoch [54/1000], Step [40], D Loss: 3.9778, G Loss: -3.3444\n",
      "Epoch [54/1000], Step [45], D Loss: 3.9959, G Loss: -3.2422\n",
      "Epoch [54/1000], Step [50], D Loss: 3.9615, G Loss: -3.1796\n",
      "Epoch [54/1000], Step [55], D Loss: 3.9141, G Loss: -3.1397\n",
      "Epoch [54/1000], Step [60], D Loss: 3.6850, G Loss: -3.0602\n",
      "Epoch [54/1000], Step [65], D Loss: 3.7337, G Loss: -3.0794\n",
      "Epoch [54/1000], Step [70], D Loss: 3.8699, G Loss: -3.1800\n",
      "Epoch [54/1000], Step [75], D Loss: 4.0703, G Loss: -3.2341\n",
      "Epoch [55/1000], Step [1], D Loss: 3.8351, G Loss: -3.2648\n",
      "Epoch [55/1000], Step [5], D Loss: 3.8785, G Loss: -3.2522\n",
      "Epoch [55/1000], Step [10], D Loss: 3.9623, G Loss: -3.3135\n",
      "Epoch [55/1000], Step [15], D Loss: 4.0620, G Loss: -3.3148\n",
      "Epoch [55/1000], Step [20], D Loss: 3.9912, G Loss: -3.3046\n",
      "Epoch [55/1000], Step [25], D Loss: 4.0085, G Loss: -3.3187\n",
      "Epoch [55/1000], Step [30], D Loss: 4.0594, G Loss: -3.3230\n",
      "Epoch [55/1000], Step [35], D Loss: 4.0247, G Loss: -3.2483\n",
      "Epoch [55/1000], Step [40], D Loss: 3.9047, G Loss: -3.2124\n",
      "Epoch [55/1000], Step [45], D Loss: 3.9012, G Loss: -3.2833\n",
      "Epoch [55/1000], Step [50], D Loss: 3.9228, G Loss: -3.3178\n",
      "Epoch [55/1000], Step [55], D Loss: 3.9225, G Loss: -3.2733\n",
      "Epoch [55/1000], Step [60], D Loss: 3.9538, G Loss: -3.2255\n",
      "Epoch [55/1000], Step [65], D Loss: 3.9544, G Loss: -3.2146\n",
      "Epoch [55/1000], Step [70], D Loss: 3.8672, G Loss: -3.1531\n",
      "Epoch [55/1000], Step [75], D Loss: 3.8318, G Loss: -3.1287\n",
      "Epoch [56/1000], Step [1], D Loss: 3.7315, G Loss: -3.1421\n",
      "Epoch [56/1000], Step [5], D Loss: 3.8125, G Loss: -3.1485\n",
      "Epoch [56/1000], Step [10], D Loss: 3.9651, G Loss: -3.2284\n",
      "Epoch [56/1000], Step [15], D Loss: 3.9634, G Loss: -3.3515\n",
      "Epoch [56/1000], Step [20], D Loss: 4.2053, G Loss: -3.4179\n",
      "Epoch [56/1000], Step [25], D Loss: 3.9015, G Loss: -3.2997\n",
      "Epoch [56/1000], Step [30], D Loss: 3.9105, G Loss: -3.2312\n",
      "Epoch [56/1000], Step [35], D Loss: 3.9124, G Loss: -3.2197\n",
      "Epoch [56/1000], Step [40], D Loss: 3.9692, G Loss: -3.2326\n",
      "Epoch [56/1000], Step [45], D Loss: 3.6946, G Loss: -3.1196\n",
      "Epoch [56/1000], Step [50], D Loss: 3.7792, G Loss: -3.1109\n",
      "Epoch [56/1000], Step [55], D Loss: 3.7256, G Loss: -3.1630\n",
      "Epoch [56/1000], Step [60], D Loss: 3.8915, G Loss: -3.1667\n",
      "Epoch [56/1000], Step [65], D Loss: 3.7357, G Loss: -3.0629\n",
      "Epoch [56/1000], Step [70], D Loss: 3.6873, G Loss: -2.9871\n",
      "Epoch [56/1000], Step [75], D Loss: 3.7996, G Loss: -2.9916\n",
      "Epoch [57/1000], Step [1], D Loss: 3.7419, G Loss: -2.8990\n",
      "Epoch [57/1000], Step [5], D Loss: 3.5132, G Loss: -2.9070\n",
      "Epoch [57/1000], Step [10], D Loss: 3.7124, G Loss: -3.0921\n",
      "Epoch [57/1000], Step [15], D Loss: 3.6490, G Loss: -3.1473\n",
      "Epoch [57/1000], Step [20], D Loss: 3.8562, G Loss: -3.2036\n",
      "Epoch [57/1000], Step [25], D Loss: 3.9331, G Loss: -3.2791\n",
      "Epoch [57/1000], Step [30], D Loss: 3.7394, G Loss: -3.2398\n",
      "Epoch [57/1000], Step [35], D Loss: 3.7184, G Loss: -3.1911\n",
      "Epoch [57/1000], Step [40], D Loss: 3.6837, G Loss: -3.2171\n",
      "Epoch [57/1000], Step [45], D Loss: 3.7594, G Loss: -3.2422\n",
      "Epoch [57/1000], Step [50], D Loss: 3.7547, G Loss: -3.2757\n",
      "Epoch [57/1000], Step [55], D Loss: 3.8649, G Loss: -3.3165\n",
      "Epoch [57/1000], Step [60], D Loss: 3.7896, G Loss: -3.2494\n",
      "Epoch [57/1000], Step [65], D Loss: 3.7681, G Loss: -3.2075\n",
      "Epoch [57/1000], Step [70], D Loss: 3.6382, G Loss: -3.1759\n",
      "Epoch [57/1000], Step [75], D Loss: 3.7370, G Loss: -3.1403\n",
      "Epoch [58/1000], Step [1], D Loss: 3.8503, G Loss: -3.1496\n",
      "Epoch [58/1000], Step [5], D Loss: 3.8067, G Loss: -3.1375\n",
      "Epoch [58/1000], Step [10], D Loss: 3.7401, G Loss: -3.0735\n",
      "Epoch [58/1000], Step [15], D Loss: 3.5401, G Loss: -2.9374\n",
      "Epoch [58/1000], Step [20], D Loss: 3.5332, G Loss: -2.8290\n",
      "Epoch [58/1000], Step [25], D Loss: 3.5348, G Loss: -2.7998\n",
      "Epoch [58/1000], Step [30], D Loss: 3.6529, G Loss: -2.8538\n",
      "Epoch [58/1000], Step [35], D Loss: 3.5074, G Loss: -2.7923\n",
      "Epoch [58/1000], Step [40], D Loss: 3.6102, G Loss: -2.7919\n",
      "Epoch [58/1000], Step [45], D Loss: 3.5551, G Loss: -2.8791\n",
      "Epoch [58/1000], Step [50], D Loss: 3.6050, G Loss: -2.8976\n",
      "Epoch [58/1000], Step [55], D Loss: 3.6488, G Loss: -3.0213\n",
      "Epoch [58/1000], Step [60], D Loss: 3.8079, G Loss: -3.0995\n",
      "Epoch [58/1000], Step [65], D Loss: 3.8788, G Loss: -3.2966\n",
      "Epoch [58/1000], Step [70], D Loss: 4.0026, G Loss: -3.3708\n",
      "Epoch [58/1000], Step [75], D Loss: 4.0303, G Loss: -3.3581\n",
      "Epoch [59/1000], Step [1], D Loss: 4.1840, G Loss: -3.3572\n",
      "Epoch [59/1000], Step [5], D Loss: 4.0139, G Loss: -3.3163\n",
      "Epoch [59/1000], Step [10], D Loss: 4.0455, G Loss: -3.3588\n",
      "Epoch [59/1000], Step [15], D Loss: 3.8872, G Loss: -3.3093\n",
      "Epoch [59/1000], Step [20], D Loss: 3.8939, G Loss: -3.1932\n",
      "Epoch [59/1000], Step [25], D Loss: 3.6417, G Loss: -3.0398\n",
      "Epoch [59/1000], Step [30], D Loss: 3.5105, G Loss: -2.9673\n",
      "Epoch [59/1000], Step [35], D Loss: 3.7078, G Loss: -2.9246\n",
      "Epoch [59/1000], Step [40], D Loss: 3.6297, G Loss: -2.8860\n",
      "Epoch [59/1000], Step [45], D Loss: 3.7330, G Loss: -2.9346\n",
      "Epoch [59/1000], Step [50], D Loss: 3.8322, G Loss: -3.0022\n",
      "Epoch [59/1000], Step [55], D Loss: 3.6521, G Loss: -3.0526\n",
      "Epoch [59/1000], Step [60], D Loss: 3.7200, G Loss: -3.1004\n",
      "Epoch [59/1000], Step [65], D Loss: 3.7286, G Loss: -3.1078\n",
      "Epoch [59/1000], Step [70], D Loss: 3.9132, G Loss: -3.2198\n",
      "Epoch [59/1000], Step [75], D Loss: 3.9942, G Loss: -3.2401\n",
      "Epoch [60/1000], Step [1], D Loss: 4.0341, G Loss: -3.2383\n",
      "Epoch [60/1000], Step [5], D Loss: 4.0118, G Loss: -3.2393\n",
      "Epoch [60/1000], Step [10], D Loss: 4.0531, G Loss: -3.2892\n",
      "Epoch [60/1000], Step [15], D Loss: 4.0301, G Loss: -3.1940\n",
      "Epoch [60/1000], Step [20], D Loss: 4.0509, G Loss: -3.1645\n",
      "Epoch [60/1000], Step [25], D Loss: 3.8484, G Loss: -3.0606\n",
      "Epoch [60/1000], Step [30], D Loss: 3.7465, G Loss: -3.0904\n",
      "Epoch [60/1000], Step [35], D Loss: 3.7513, G Loss: -3.0285\n",
      "Epoch [60/1000], Step [40], D Loss: 3.8655, G Loss: -3.0022\n",
      "Epoch [60/1000], Step [45], D Loss: 3.7362, G Loss: -2.9269\n",
      "Epoch [60/1000], Step [50], D Loss: 3.5985, G Loss: -2.9860\n",
      "Epoch [60/1000], Step [55], D Loss: 4.0274, G Loss: -3.1306\n",
      "Epoch [60/1000], Step [60], D Loss: 4.1819, G Loss: -3.2592\n",
      "Epoch [60/1000], Step [65], D Loss: 4.2599, G Loss: -3.3206\n",
      "Epoch [60/1000], Step [70], D Loss: 4.1964, G Loss: -3.3385\n",
      "Epoch [60/1000], Step [75], D Loss: 4.2646, G Loss: -3.3357\n",
      "Epoch [61/1000], Step [1], D Loss: 4.2050, G Loss: -3.2832\n",
      "Epoch [61/1000], Step [5], D Loss: 4.0305, G Loss: -3.2143\n",
      "Epoch [61/1000], Step [10], D Loss: 3.9506, G Loss: -3.1830\n",
      "Epoch [61/1000], Step [15], D Loss: 3.9655, G Loss: -3.1811\n",
      "Epoch [61/1000], Step [20], D Loss: 4.1379, G Loss: -3.1941\n",
      "Epoch [61/1000], Step [25], D Loss: 3.9598, G Loss: -3.1363\n",
      "Epoch [61/1000], Step [30], D Loss: 3.9470, G Loss: -3.0306\n",
      "Epoch [61/1000], Step [35], D Loss: 3.8627, G Loss: -2.9551\n",
      "Epoch [61/1000], Step [40], D Loss: 3.6917, G Loss: -2.8491\n",
      "Epoch [61/1000], Step [45], D Loss: 3.5798, G Loss: -2.7563\n",
      "Epoch [61/1000], Step [50], D Loss: 3.5143, G Loss: -2.7995\n",
      "Epoch [61/1000], Step [55], D Loss: 3.6605, G Loss: -2.9919\n",
      "Epoch [61/1000], Step [60], D Loss: 3.7316, G Loss: -3.1344\n",
      "Epoch [61/1000], Step [65], D Loss: 3.8048, G Loss: -3.2675\n",
      "Epoch [61/1000], Step [70], D Loss: 3.8478, G Loss: -3.3756\n",
      "Epoch [61/1000], Step [75], D Loss: 4.0323, G Loss: -3.4601\n",
      "Epoch [62/1000], Step [1], D Loss: 4.2554, G Loss: -3.4965\n",
      "Epoch [62/1000], Step [5], D Loss: 4.1364, G Loss: -3.5162\n",
      "Epoch [62/1000], Step [10], D Loss: 3.9735, G Loss: -3.4233\n",
      "Epoch [62/1000], Step [15], D Loss: 4.0211, G Loss: -3.3679\n",
      "Epoch [62/1000], Step [20], D Loss: 3.9025, G Loss: -3.2931\n",
      "Epoch [62/1000], Step [25], D Loss: 3.8010, G Loss: -3.2020\n",
      "Epoch [62/1000], Step [30], D Loss: 3.7222, G Loss: -3.0551\n",
      "Epoch [62/1000], Step [35], D Loss: 3.6142, G Loss: -2.9524\n",
      "Epoch [62/1000], Step [40], D Loss: 3.3510, G Loss: -2.8150\n",
      "Epoch [62/1000], Step [45], D Loss: 3.3218, G Loss: -2.7605\n",
      "Epoch [62/1000], Step [50], D Loss: 3.3993, G Loss: -2.7732\n",
      "Epoch [62/1000], Step [55], D Loss: 3.3893, G Loss: -2.7858\n",
      "Epoch [62/1000], Step [60], D Loss: 3.3618, G Loss: -2.7487\n",
      "Epoch [62/1000], Step [65], D Loss: 3.3902, G Loss: -2.7552\n",
      "Epoch [62/1000], Step [70], D Loss: 3.4745, G Loss: -2.7837\n",
      "Epoch [62/1000], Step [75], D Loss: 3.4440, G Loss: -2.7628\n",
      "Epoch [63/1000], Step [1], D Loss: 3.5651, G Loss: -2.8017\n",
      "Epoch [63/1000], Step [5], D Loss: 3.4968, G Loss: -2.8915\n",
      "Epoch [63/1000], Step [10], D Loss: 3.7390, G Loss: -3.1113\n",
      "Epoch [63/1000], Step [15], D Loss: 3.9656, G Loss: -3.2114\n",
      "Epoch [63/1000], Step [20], D Loss: 4.1800, G Loss: -3.3120\n",
      "Epoch [63/1000], Step [25], D Loss: 4.1829, G Loss: -3.3247\n",
      "Epoch [63/1000], Step [30], D Loss: 4.0229, G Loss: -3.2448\n",
      "Epoch [63/1000], Step [35], D Loss: 3.8711, G Loss: -3.1784\n",
      "Epoch [63/1000], Step [40], D Loss: 3.7516, G Loss: -3.0780\n",
      "Epoch [63/1000], Step [45], D Loss: 3.7141, G Loss: -2.9680\n",
      "Epoch [63/1000], Step [50], D Loss: 3.6116, G Loss: -2.8647\n",
      "Epoch [63/1000], Step [55], D Loss: 3.5226, G Loss: -2.7561\n",
      "Epoch [63/1000], Step [60], D Loss: 3.3376, G Loss: -2.6694\n",
      "Epoch [63/1000], Step [65], D Loss: 3.3205, G Loss: -2.5698\n",
      "Epoch [63/1000], Step [70], D Loss: 3.2160, G Loss: -2.4868\n",
      "Epoch [63/1000], Step [75], D Loss: 3.2267, G Loss: -2.4662\n",
      "Epoch [64/1000], Step [1], D Loss: 3.4013, G Loss: -2.5764\n",
      "Epoch [64/1000], Step [5], D Loss: 3.3396, G Loss: -2.7102\n",
      "Epoch [64/1000], Step [10], D Loss: 3.4783, G Loss: -2.8834\n",
      "Epoch [64/1000], Step [15], D Loss: 3.6884, G Loss: -3.0325\n",
      "Epoch [64/1000], Step [20], D Loss: 3.7497, G Loss: -3.1233\n",
      "Epoch [64/1000], Step [25], D Loss: 3.8693, G Loss: -3.2486\n",
      "Epoch [64/1000], Step [30], D Loss: 3.8607, G Loss: -3.2284\n",
      "Epoch [64/1000], Step [35], D Loss: 3.7944, G Loss: -3.1539\n",
      "Epoch [64/1000], Step [40], D Loss: 3.9041, G Loss: -3.1702\n",
      "Epoch [64/1000], Step [45], D Loss: 3.7556, G Loss: -3.1342\n",
      "Epoch [64/1000], Step [50], D Loss: 3.6637, G Loss: -3.0894\n",
      "Epoch [64/1000], Step [55], D Loss: 3.5265, G Loss: -3.0322\n",
      "Epoch [64/1000], Step [60], D Loss: 3.5476, G Loss: -2.9735\n",
      "Epoch [64/1000], Step [65], D Loss: 3.4276, G Loss: -2.8976\n",
      "Epoch [64/1000], Step [70], D Loss: 3.4158, G Loss: -2.8509\n",
      "Epoch [64/1000], Step [75], D Loss: 3.4539, G Loss: -2.8060\n",
      "Epoch [65/1000], Step [1], D Loss: 3.4062, G Loss: -2.7652\n",
      "Epoch [65/1000], Step [5], D Loss: 3.3636, G Loss: -2.7478\n",
      "Epoch [65/1000], Step [10], D Loss: 3.3197, G Loss: -2.6738\n",
      "Epoch [65/1000], Step [15], D Loss: 3.1623, G Loss: -2.6062\n",
      "Epoch [65/1000], Step [20], D Loss: 3.3453, G Loss: -2.6572\n",
      "Epoch [65/1000], Step [25], D Loss: 3.2187, G Loss: -2.6794\n",
      "Epoch [65/1000], Step [30], D Loss: 3.3888, G Loss: -2.7360\n",
      "Epoch [65/1000], Step [35], D Loss: 3.3680, G Loss: -2.7635\n",
      "Epoch [65/1000], Step [40], D Loss: 3.6352, G Loss: -2.8639\n",
      "Epoch [65/1000], Step [45], D Loss: 3.5357, G Loss: -2.9674\n",
      "Epoch [65/1000], Step [50], D Loss: 3.4777, G Loss: -2.9910\n",
      "Epoch [65/1000], Step [55], D Loss: 3.4617, G Loss: -2.9355\n",
      "Epoch [65/1000], Step [60], D Loss: 3.4271, G Loss: -2.8688\n",
      "Epoch [65/1000], Step [65], D Loss: 3.5985, G Loss: -2.8443\n",
      "Epoch [65/1000], Step [70], D Loss: 3.6180, G Loss: -2.8650\n",
      "Epoch [65/1000], Step [75], D Loss: 3.4294, G Loss: -2.8278\n",
      "Epoch [66/1000], Step [1], D Loss: 3.3775, G Loss: -2.7874\n",
      "Epoch [66/1000], Step [5], D Loss: 3.5121, G Loss: -2.8184\n",
      "Epoch [66/1000], Step [10], D Loss: 3.4841, G Loss: -2.8076\n",
      "Epoch [66/1000], Step [15], D Loss: 3.4388, G Loss: -2.7572\n",
      "Epoch [66/1000], Step [20], D Loss: 3.6914, G Loss: -2.7168\n",
      "Epoch [66/1000], Step [25], D Loss: 3.4798, G Loss: -2.6488\n",
      "Epoch [66/1000], Step [30], D Loss: 3.4274, G Loss: -2.6688\n",
      "Epoch [66/1000], Step [35], D Loss: 3.6127, G Loss: -2.8333\n",
      "Epoch [66/1000], Step [40], D Loss: 3.6850, G Loss: -2.8304\n",
      "Epoch [66/1000], Step [45], D Loss: 3.7133, G Loss: -2.8821\n",
      "Epoch [66/1000], Step [50], D Loss: 3.8153, G Loss: -2.9357\n",
      "Epoch [66/1000], Step [55], D Loss: 3.6933, G Loss: -2.9316\n",
      "Epoch [66/1000], Step [60], D Loss: 3.8669, G Loss: -2.9576\n",
      "Epoch [66/1000], Step [65], D Loss: 3.8784, G Loss: -2.9438\n",
      "Epoch [66/1000], Step [70], D Loss: 3.6880, G Loss: -2.8818\n",
      "Epoch [66/1000], Step [75], D Loss: 3.5227, G Loss: -2.8643\n",
      "Epoch [67/1000], Step [1], D Loss: 3.5554, G Loss: -2.9147\n",
      "Epoch [67/1000], Step [5], D Loss: 3.7514, G Loss: -3.0120\n",
      "Epoch [67/1000], Step [10], D Loss: 3.9229, G Loss: -3.0636\n",
      "Epoch [67/1000], Step [15], D Loss: 3.5463, G Loss: -2.9901\n",
      "Epoch [67/1000], Step [20], D Loss: 3.6421, G Loss: -2.9528\n",
      "Epoch [67/1000], Step [25], D Loss: 3.6652, G Loss: -2.9289\n",
      "Epoch [67/1000], Step [30], D Loss: 3.6560, G Loss: -2.9253\n",
      "Epoch [67/1000], Step [35], D Loss: 3.5000, G Loss: -2.9291\n",
      "Epoch [67/1000], Step [40], D Loss: 3.6530, G Loss: -3.0056\n",
      "Epoch [67/1000], Step [45], D Loss: 3.8056, G Loss: -3.0966\n",
      "Epoch [67/1000], Step [50], D Loss: 3.8198, G Loss: -3.0818\n",
      "Epoch [67/1000], Step [55], D Loss: 3.6864, G Loss: -3.0276\n",
      "Epoch [67/1000], Step [60], D Loss: 3.7680, G Loss: -3.0510\n",
      "Epoch [67/1000], Step [65], D Loss: 3.7242, G Loss: -3.0654\n",
      "Epoch [67/1000], Step [70], D Loss: 3.6556, G Loss: -3.0112\n",
      "Epoch [67/1000], Step [75], D Loss: 3.5313, G Loss: -2.9032\n",
      "Epoch [68/1000], Step [1], D Loss: 3.4554, G Loss: -2.8522\n",
      "Epoch [68/1000], Step [5], D Loss: 3.4545, G Loss: -2.7872\n",
      "Epoch [68/1000], Step [10], D Loss: 3.5853, G Loss: -2.7667\n",
      "Epoch [68/1000], Step [15], D Loss: 3.4960, G Loss: -2.7255\n",
      "Epoch [68/1000], Step [20], D Loss: 3.5577, G Loss: -2.6942\n",
      "Epoch [68/1000], Step [25], D Loss: 3.6433, G Loss: -2.7056\n",
      "Epoch [68/1000], Step [30], D Loss: 3.6100, G Loss: -2.8359\n",
      "Epoch [68/1000], Step [35], D Loss: 3.7242, G Loss: -2.8763\n",
      "Epoch [68/1000], Step [40], D Loss: 3.8358, G Loss: -3.0069\n",
      "Epoch [68/1000], Step [45], D Loss: 3.9106, G Loss: -3.1095\n",
      "Epoch [68/1000], Step [50], D Loss: 3.7183, G Loss: -3.0721\n",
      "Epoch [68/1000], Step [55], D Loss: 3.8066, G Loss: -3.0647\n",
      "Epoch [68/1000], Step [60], D Loss: 4.0368, G Loss: -3.1310\n",
      "Epoch [68/1000], Step [65], D Loss: 3.9372, G Loss: -3.1089\n",
      "Epoch [68/1000], Step [70], D Loss: 3.8982, G Loss: -3.0353\n",
      "Epoch [68/1000], Step [75], D Loss: 4.0428, G Loss: -3.0292\n",
      "Epoch [69/1000], Step [1], D Loss: 4.0207, G Loss: -3.0170\n",
      "Epoch [69/1000], Step [5], D Loss: 4.0286, G Loss: -3.0915\n",
      "Epoch [69/1000], Step [10], D Loss: 3.9033, G Loss: -3.1062\n",
      "Epoch [69/1000], Step [15], D Loss: 3.7259, G Loss: -3.1094\n",
      "Epoch [69/1000], Step [20], D Loss: 3.6370, G Loss: -3.0211\n",
      "Epoch [69/1000], Step [25], D Loss: 3.7260, G Loss: -3.0421\n",
      "Epoch [69/1000], Step [30], D Loss: 3.7511, G Loss: -3.0692\n",
      "Epoch [69/1000], Step [35], D Loss: 3.7817, G Loss: -3.0441\n",
      "Epoch [69/1000], Step [40], D Loss: 3.6537, G Loss: -2.9890\n",
      "Epoch [69/1000], Step [45], D Loss: 3.8350, G Loss: -2.9894\n",
      "Epoch [69/1000], Step [50], D Loss: 3.6506, G Loss: -2.9050\n",
      "Epoch [69/1000], Step [55], D Loss: 3.7391, G Loss: -2.8625\n",
      "Epoch [69/1000], Step [60], D Loss: 3.7865, G Loss: -3.0151\n",
      "Epoch [69/1000], Step [65], D Loss: 3.9909, G Loss: -3.1237\n",
      "Epoch [69/1000], Step [70], D Loss: 3.8941, G Loss: -3.2031\n",
      "Epoch [69/1000], Step [75], D Loss: 3.8961, G Loss: -3.1975\n",
      "Epoch [70/1000], Step [1], D Loss: 3.8540, G Loss: -3.1636\n",
      "Epoch [70/1000], Step [5], D Loss: 3.6995, G Loss: -3.1082\n",
      "Epoch [70/1000], Step [10], D Loss: 3.7233, G Loss: -3.0681\n",
      "Epoch [70/1000], Step [15], D Loss: 3.6069, G Loss: -3.0338\n",
      "Epoch [70/1000], Step [20], D Loss: 3.7937, G Loss: -3.0510\n",
      "Epoch [70/1000], Step [25], D Loss: 3.7763, G Loss: -3.0318\n",
      "Epoch [70/1000], Step [30], D Loss: 3.7372, G Loss: -3.0342\n",
      "Epoch [70/1000], Step [35], D Loss: 3.5797, G Loss: -2.9296\n",
      "Epoch [70/1000], Step [40], D Loss: 3.5499, G Loss: -2.8268\n",
      "Epoch [70/1000], Step [45], D Loss: 3.5131, G Loss: -2.8215\n",
      "Epoch [70/1000], Step [50], D Loss: 3.6564, G Loss: -2.8578\n",
      "Epoch [70/1000], Step [55], D Loss: 3.7357, G Loss: -2.9419\n",
      "Epoch [70/1000], Step [60], D Loss: 3.5914, G Loss: -2.9408\n",
      "Epoch [70/1000], Step [65], D Loss: 3.5896, G Loss: -2.9833\n",
      "Epoch [70/1000], Step [70], D Loss: 3.6328, G Loss: -3.0398\n",
      "Epoch [70/1000], Step [75], D Loss: 3.8621, G Loss: -3.0809\n",
      "Epoch [71/1000], Step [1], D Loss: 3.8235, G Loss: -3.0957\n",
      "Epoch [71/1000], Step [5], D Loss: 3.9990, G Loss: -3.1283\n",
      "Epoch [71/1000], Step [10], D Loss: 3.9192, G Loss: -3.1111\n",
      "Epoch [71/1000], Step [15], D Loss: 3.8605, G Loss: -3.1038\n",
      "Epoch [71/1000], Step [20], D Loss: 3.8123, G Loss: -3.1056\n",
      "Epoch [71/1000], Step [25], D Loss: 3.8621, G Loss: -3.0644\n",
      "Epoch [71/1000], Step [30], D Loss: 3.6182, G Loss: -2.9703\n",
      "Epoch [71/1000], Step [35], D Loss: 3.6920, G Loss: -2.9412\n",
      "Epoch [71/1000], Step [40], D Loss: 3.6391, G Loss: -2.9232\n",
      "Epoch [71/1000], Step [45], D Loss: 3.6832, G Loss: -2.9111\n",
      "Epoch [71/1000], Step [50], D Loss: 3.6626, G Loss: -2.9498\n",
      "Epoch [71/1000], Step [55], D Loss: 3.4726, G Loss: -2.8659\n",
      "Epoch [71/1000], Step [60], D Loss: 3.6896, G Loss: -2.7910\n",
      "Epoch [71/1000], Step [65], D Loss: 3.5775, G Loss: -2.7624\n",
      "Epoch [71/1000], Step [70], D Loss: 3.6207, G Loss: -2.7950\n",
      "Epoch [71/1000], Step [75], D Loss: 3.5775, G Loss: -2.8617\n",
      "Epoch [72/1000], Step [1], D Loss: 3.7528, G Loss: -2.9250\n",
      "Epoch [72/1000], Step [5], D Loss: 3.9398, G Loss: -3.0833\n",
      "Epoch [72/1000], Step [10], D Loss: 3.9789, G Loss: -3.1399\n",
      "Epoch [72/1000], Step [15], D Loss: 3.7313, G Loss: -3.0976\n",
      "Epoch [72/1000], Step [20], D Loss: 3.7152, G Loss: -3.0658\n",
      "Epoch [72/1000], Step [25], D Loss: 3.7991, G Loss: -3.0898\n",
      "Epoch [72/1000], Step [30], D Loss: 3.6869, G Loss: -3.0552\n",
      "Epoch [72/1000], Step [35], D Loss: 3.7906, G Loss: -3.0828\n",
      "Epoch [72/1000], Step [40], D Loss: 3.7939, G Loss: -3.0582\n",
      "Epoch [72/1000], Step [45], D Loss: 3.7407, G Loss: -2.9939\n",
      "Epoch [72/1000], Step [50], D Loss: 3.7087, G Loss: -2.9239\n",
      "Epoch [72/1000], Step [55], D Loss: 3.5996, G Loss: -2.8001\n",
      "Epoch [72/1000], Step [60], D Loss: 3.5661, G Loss: -2.7849\n",
      "Epoch [72/1000], Step [65], D Loss: 3.6578, G Loss: -2.7977\n",
      "Epoch [72/1000], Step [70], D Loss: 3.5716, G Loss: -2.7494\n",
      "Epoch [72/1000], Step [75], D Loss: 3.7061, G Loss: -2.8288\n",
      "Epoch [73/1000], Step [1], D Loss: 3.4526, G Loss: -2.8145\n",
      "Epoch [73/1000], Step [5], D Loss: 3.5848, G Loss: -2.7868\n",
      "Epoch [73/1000], Step [10], D Loss: 3.6327, G Loss: -2.9011\n",
      "Epoch [73/1000], Step [15], D Loss: 3.6799, G Loss: -2.9424\n",
      "Epoch [73/1000], Step [20], D Loss: 3.6390, G Loss: -2.9109\n",
      "Epoch [73/1000], Step [25], D Loss: 3.5456, G Loss: -2.8581\n",
      "Epoch [73/1000], Step [30], D Loss: 3.4487, G Loss: -2.8179\n",
      "Epoch [73/1000], Step [35], D Loss: 3.3568, G Loss: -2.7645\n",
      "Epoch [73/1000], Step [40], D Loss: 3.5005, G Loss: -2.8408\n",
      "Epoch [73/1000], Step [45], D Loss: 3.5306, G Loss: -2.9782\n",
      "Epoch [73/1000], Step [50], D Loss: 3.6997, G Loss: -3.0681\n",
      "Epoch [73/1000], Step [55], D Loss: 3.8812, G Loss: -3.1620\n",
      "Epoch [73/1000], Step [60], D Loss: 4.0332, G Loss: -3.1948\n",
      "Epoch [73/1000], Step [65], D Loss: 3.8658, G Loss: -3.0968\n",
      "Epoch [73/1000], Step [70], D Loss: 3.6673, G Loss: -2.9560\n",
      "Epoch [73/1000], Step [75], D Loss: 3.7213, G Loss: -2.8365\n",
      "Epoch [74/1000], Step [1], D Loss: 3.7459, G Loss: -2.8192\n",
      "Epoch [74/1000], Step [5], D Loss: 3.6096, G Loss: -2.8416\n",
      "Epoch [74/1000], Step [10], D Loss: 3.6427, G Loss: -2.8657\n",
      "Epoch [74/1000], Step [15], D Loss: 3.6503, G Loss: -2.8356\n",
      "Epoch [74/1000], Step [20], D Loss: 3.6763, G Loss: -2.7632\n",
      "Epoch [74/1000], Step [25], D Loss: 3.6136, G Loss: -2.7213\n",
      "Epoch [74/1000], Step [30], D Loss: 3.3861, G Loss: -2.7162\n",
      "Epoch [74/1000], Step [35], D Loss: 3.4974, G Loss: -2.7801\n",
      "Epoch [74/1000], Step [40], D Loss: 3.4056, G Loss: -2.8326\n",
      "Epoch [74/1000], Step [45], D Loss: 3.6066, G Loss: -2.8616\n",
      "Epoch [74/1000], Step [50], D Loss: 3.7462, G Loss: -2.9730\n",
      "Epoch [74/1000], Step [55], D Loss: 3.6843, G Loss: -2.9897\n",
      "Epoch [74/1000], Step [60], D Loss: 3.7153, G Loss: -3.0088\n",
      "Epoch [74/1000], Step [65], D Loss: 3.8201, G Loss: -3.1219\n",
      "Epoch [74/1000], Step [70], D Loss: 3.8319, G Loss: -3.0572\n",
      "Epoch [74/1000], Step [75], D Loss: 3.8404, G Loss: -3.0412\n",
      "Epoch [75/1000], Step [1], D Loss: 3.7480, G Loss: -2.9894\n",
      "Epoch [75/1000], Step [5], D Loss: 3.7457, G Loss: -2.9290\n",
      "Epoch [75/1000], Step [10], D Loss: 3.6552, G Loss: -2.8792\n",
      "Epoch [75/1000], Step [15], D Loss: 3.5981, G Loss: -2.8756\n",
      "Epoch [75/1000], Step [20], D Loss: 3.4194, G Loss: -2.8660\n",
      "Epoch [75/1000], Step [25], D Loss: 3.4893, G Loss: -2.8451\n",
      "Epoch [75/1000], Step [30], D Loss: 3.3023, G Loss: -2.7544\n",
      "Epoch [75/1000], Step [35], D Loss: 3.3675, G Loss: -2.7274\n",
      "Epoch [75/1000], Step [40], D Loss: 3.5064, G Loss: -2.7588\n",
      "Epoch [75/1000], Step [45], D Loss: 3.4749, G Loss: -2.7383\n",
      "Epoch [75/1000], Step [50], D Loss: 3.2832, G Loss: -2.6775\n",
      "Epoch [75/1000], Step [55], D Loss: 3.2118, G Loss: -2.6991\n",
      "Epoch [75/1000], Step [60], D Loss: 3.3504, G Loss: -2.7123\n",
      "Epoch [75/1000], Step [65], D Loss: 3.5703, G Loss: -2.8264\n",
      "Epoch [75/1000], Step [70], D Loss: 3.5267, G Loss: -2.9121\n",
      "Epoch [75/1000], Step [75], D Loss: 3.7056, G Loss: -2.9793\n",
      "Epoch [76/1000], Step [1], D Loss: 3.7154, G Loss: -2.9962\n",
      "Epoch [76/1000], Step [5], D Loss: 3.6868, G Loss: -2.9728\n",
      "Epoch [76/1000], Step [10], D Loss: 3.7394, G Loss: -2.9391\n",
      "Epoch [76/1000], Step [15], D Loss: 3.7164, G Loss: -2.9134\n",
      "Epoch [76/1000], Step [20], D Loss: 3.6072, G Loss: -2.9103\n",
      "Epoch [76/1000], Step [25], D Loss: 3.6283, G Loss: -2.8804\n",
      "Epoch [76/1000], Step [30], D Loss: 3.5422, G Loss: -2.8594\n",
      "Epoch [76/1000], Step [35], D Loss: 3.5827, G Loss: -2.8149\n",
      "Epoch [76/1000], Step [40], D Loss: 3.4350, G Loss: -2.8008\n",
      "Epoch [76/1000], Step [45], D Loss: 3.4065, G Loss: -2.7354\n",
      "Epoch [76/1000], Step [50], D Loss: 3.3102, G Loss: -2.7155\n",
      "Epoch [76/1000], Step [55], D Loss: 3.2043, G Loss: -2.6824\n",
      "Epoch [76/1000], Step [60], D Loss: 3.2513, G Loss: -2.6266\n",
      "Epoch [76/1000], Step [65], D Loss: 3.2638, G Loss: -2.7275\n",
      "Epoch [76/1000], Step [70], D Loss: 3.4489, G Loss: -2.7719\n",
      "Epoch [76/1000], Step [75], D Loss: 3.3195, G Loss: -2.7785\n",
      "Epoch [77/1000], Step [1], D Loss: 3.4235, G Loss: -2.8082\n",
      "Epoch [77/1000], Step [5], D Loss: 3.5388, G Loss: -2.8405\n",
      "Epoch [77/1000], Step [10], D Loss: 3.4289, G Loss: -2.8426\n",
      "Epoch [77/1000], Step [15], D Loss: 3.4377, G Loss: -2.8007\n",
      "Epoch [77/1000], Step [20], D Loss: 3.5763, G Loss: -2.8089\n",
      "Epoch [77/1000], Step [25], D Loss: 3.5988, G Loss: -2.8652\n",
      "Epoch [77/1000], Step [30], D Loss: 3.4998, G Loss: -2.8909\n",
      "Epoch [77/1000], Step [35], D Loss: 3.4259, G Loss: -2.8588\n",
      "Epoch [77/1000], Step [40], D Loss: 3.4066, G Loss: -2.7584\n",
      "Epoch [77/1000], Step [45], D Loss: 3.2613, G Loss: -2.6829\n",
      "Epoch [77/1000], Step [50], D Loss: 3.3744, G Loss: -2.6651\n",
      "Epoch [77/1000], Step [55], D Loss: 3.2446, G Loss: -2.5471\n",
      "Epoch [77/1000], Step [60], D Loss: 3.3084, G Loss: -2.5116\n",
      "Epoch [77/1000], Step [65], D Loss: 3.3530, G Loss: -2.4842\n",
      "Epoch [77/1000], Step [70], D Loss: 3.1551, G Loss: -2.4826\n",
      "Epoch [77/1000], Step [75], D Loss: 3.0784, G Loss: -2.5095\n",
      "Epoch [78/1000], Step [1], D Loss: 3.1878, G Loss: -2.5531\n",
      "Epoch [78/1000], Step [5], D Loss: 3.1561, G Loss: -2.5976\n",
      "Epoch [78/1000], Step [10], D Loss: 3.2745, G Loss: -2.6329\n",
      "Epoch [78/1000], Step [15], D Loss: 3.5867, G Loss: -2.6408\n",
      "Epoch [78/1000], Step [20], D Loss: 3.3666, G Loss: -2.6500\n",
      "Epoch [78/1000], Step [25], D Loss: 3.5295, G Loss: -2.7457\n",
      "Epoch [78/1000], Step [30], D Loss: 3.4911, G Loss: -2.7687\n",
      "Epoch [78/1000], Step [35], D Loss: 3.3946, G Loss: -2.7639\n",
      "Epoch [78/1000], Step [40], D Loss: 3.5502, G Loss: -2.7685\n",
      "Epoch [78/1000], Step [45], D Loss: 3.5332, G Loss: -2.7904\n",
      "Epoch [78/1000], Step [50], D Loss: 3.5112, G Loss: -2.8371\n",
      "Epoch [78/1000], Step [55], D Loss: 3.3849, G Loss: -2.7440\n",
      "Epoch [78/1000], Step [60], D Loss: 3.3114, G Loss: -2.6491\n",
      "Epoch [78/1000], Step [65], D Loss: 3.3328, G Loss: -2.6206\n",
      "Epoch [78/1000], Step [70], D Loss: 3.1750, G Loss: -2.5528\n",
      "Epoch [78/1000], Step [75], D Loss: 3.1509, G Loss: -2.4495\n",
      "Epoch [79/1000], Step [1], D Loss: 3.0640, G Loss: -2.4440\n",
      "Epoch [79/1000], Step [5], D Loss: 3.0064, G Loss: -2.4873\n",
      "Epoch [79/1000], Step [10], D Loss: 3.3716, G Loss: -2.6307\n",
      "Epoch [79/1000], Step [15], D Loss: 3.3403, G Loss: -2.6702\n",
      "Epoch [79/1000], Step [20], D Loss: 3.3316, G Loss: -2.7415\n",
      "Epoch [79/1000], Step [25], D Loss: 3.5050, G Loss: -2.7676\n",
      "Epoch [79/1000], Step [30], D Loss: 3.5279, G Loss: -2.7707\n",
      "Epoch [79/1000], Step [35], D Loss: 3.5369, G Loss: -2.7358\n",
      "Epoch [79/1000], Step [40], D Loss: 3.3961, G Loss: -2.6789\n",
      "Epoch [79/1000], Step [45], D Loss: 3.4481, G Loss: -2.6803\n",
      "Epoch [79/1000], Step [50], D Loss: 3.3423, G Loss: -2.7079\n",
      "Epoch [79/1000], Step [55], D Loss: 3.3138, G Loss: -2.7688\n",
      "Epoch [79/1000], Step [60], D Loss: 3.4811, G Loss: -2.8366\n",
      "Epoch [79/1000], Step [65], D Loss: 3.7672, G Loss: -2.9302\n",
      "Epoch [79/1000], Step [70], D Loss: 3.6886, G Loss: -2.9647\n",
      "Epoch [79/1000], Step [75], D Loss: 3.7152, G Loss: -2.8815\n",
      "Epoch [80/1000], Step [1], D Loss: 3.5777, G Loss: -2.8185\n",
      "Epoch [80/1000], Step [5], D Loss: 3.5385, G Loss: -2.7954\n",
      "Epoch [80/1000], Step [10], D Loss: 3.4919, G Loss: -2.7172\n",
      "Epoch [80/1000], Step [15], D Loss: 3.5305, G Loss: -2.7270\n",
      "Epoch [80/1000], Step [20], D Loss: 3.5640, G Loss: -2.7137\n",
      "Epoch [80/1000], Step [25], D Loss: 3.5319, G Loss: -2.6678\n",
      "Epoch [80/1000], Step [30], D Loss: 3.3751, G Loss: -2.6703\n",
      "Epoch [80/1000], Step [35], D Loss: 3.4992, G Loss: -2.6254\n",
      "Epoch [80/1000], Step [40], D Loss: 3.4699, G Loss: -2.6559\n",
      "Epoch [80/1000], Step [45], D Loss: 3.4007, G Loss: -2.6233\n",
      "Epoch [80/1000], Step [50], D Loss: 3.3131, G Loss: -2.5822\n",
      "Epoch [80/1000], Step [55], D Loss: 3.3019, G Loss: -2.5661\n",
      "Epoch [80/1000], Step [60], D Loss: 3.2787, G Loss: -2.5632\n",
      "Epoch [80/1000], Step [65], D Loss: 3.3603, G Loss: -2.6470\n",
      "Epoch [80/1000], Step [70], D Loss: 3.4555, G Loss: -2.7345\n",
      "Epoch [80/1000], Step [75], D Loss: 3.4620, G Loss: -2.7724\n",
      "Epoch [81/1000], Step [1], D Loss: 3.5236, G Loss: -2.7829\n",
      "Epoch [81/1000], Step [5], D Loss: 3.4209, G Loss: -2.7860\n",
      "Epoch [81/1000], Step [10], D Loss: 3.2995, G Loss: -2.8195\n",
      "Epoch [81/1000], Step [15], D Loss: 3.4136, G Loss: -2.8661\n",
      "Epoch [81/1000], Step [20], D Loss: 3.5339, G Loss: -2.9511\n",
      "Epoch [81/1000], Step [25], D Loss: 3.5809, G Loss: -3.0022\n",
      "Epoch [81/1000], Step [30], D Loss: 3.5697, G Loss: -3.0022\n",
      "Epoch [81/1000], Step [35], D Loss: 3.5796, G Loss: -2.9508\n",
      "Epoch [81/1000], Step [40], D Loss: 3.4390, G Loss: -2.8196\n",
      "Epoch [81/1000], Step [45], D Loss: 3.5905, G Loss: -2.7437\n",
      "Epoch [81/1000], Step [50], D Loss: 3.5343, G Loss: -2.6713\n",
      "Epoch [81/1000], Step [55], D Loss: 3.3636, G Loss: -2.5993\n",
      "Epoch [81/1000], Step [60], D Loss: 3.2524, G Loss: -2.5471\n",
      "Epoch [81/1000], Step [65], D Loss: 3.3268, G Loss: -2.5022\n",
      "Epoch [81/1000], Step [70], D Loss: 3.2420, G Loss: -2.4620\n",
      "Epoch [81/1000], Step [75], D Loss: 3.3370, G Loss: -2.4116\n",
      "Epoch [82/1000], Step [1], D Loss: 3.1594, G Loss: -2.4318\n",
      "Epoch [82/1000], Step [5], D Loss: 3.2497, G Loss: -2.5041\n",
      "Epoch [82/1000], Step [10], D Loss: 3.5092, G Loss: -2.6699\n",
      "Epoch [82/1000], Step [15], D Loss: 3.5612, G Loss: -2.7227\n",
      "Epoch [82/1000], Step [20], D Loss: 3.5316, G Loss: -2.7572\n",
      "Epoch [82/1000], Step [25], D Loss: 3.5700, G Loss: -2.7381\n",
      "Epoch [82/1000], Step [30], D Loss: 3.6352, G Loss: -2.7820\n",
      "Epoch [82/1000], Step [35], D Loss: 3.6699, G Loss: -2.8204\n",
      "Epoch [82/1000], Step [40], D Loss: 3.6437, G Loss: -2.8385\n",
      "Epoch [82/1000], Step [45], D Loss: 3.5131, G Loss: -2.8638\n",
      "Epoch [82/1000], Step [50], D Loss: 3.6171, G Loss: -2.8062\n",
      "Epoch [82/1000], Step [55], D Loss: 3.5362, G Loss: -2.7174\n",
      "Epoch [82/1000], Step [60], D Loss: 3.2824, G Loss: -2.7186\n",
      "Epoch [82/1000], Step [65], D Loss: 3.3363, G Loss: -2.7343\n",
      "Epoch [82/1000], Step [70], D Loss: 3.4762, G Loss: -2.7528\n",
      "Epoch [82/1000], Step [75], D Loss: 3.3686, G Loss: -2.6537\n",
      "Epoch [83/1000], Step [1], D Loss: 3.2740, G Loss: -2.6267\n",
      "Epoch [83/1000], Step [5], D Loss: 3.2783, G Loss: -2.6752\n",
      "Epoch [83/1000], Step [10], D Loss: 3.2972, G Loss: -2.6491\n",
      "Epoch [83/1000], Step [15], D Loss: 3.3149, G Loss: -2.7420\n",
      "Epoch [83/1000], Step [20], D Loss: 3.3498, G Loss: -2.7667\n",
      "Epoch [83/1000], Step [25], D Loss: 3.4205, G Loss: -2.8151\n",
      "Epoch [83/1000], Step [30], D Loss: 3.2744, G Loss: -2.7689\n",
      "Epoch [83/1000], Step [35], D Loss: 3.3048, G Loss: -2.7466\n",
      "Epoch [83/1000], Step [40], D Loss: 3.3387, G Loss: -2.8112\n",
      "Epoch [83/1000], Step [45], D Loss: 3.3945, G Loss: -2.8313\n",
      "Epoch [83/1000], Step [50], D Loss: 3.4663, G Loss: -2.8081\n",
      "Epoch [83/1000], Step [55], D Loss: 3.5442, G Loss: -2.8396\n",
      "Epoch [83/1000], Step [60], D Loss: 3.3525, G Loss: -2.7584\n",
      "Epoch [83/1000], Step [65], D Loss: 3.4148, G Loss: -2.7081\n",
      "Epoch [83/1000], Step [70], D Loss: 3.4578, G Loss: -2.6934\n",
      "Epoch [83/1000], Step [75], D Loss: 3.3720, G Loss: -2.6710\n",
      "Epoch [84/1000], Step [1], D Loss: 3.3866, G Loss: -2.6229\n",
      "Epoch [84/1000], Step [5], D Loss: 3.2572, G Loss: -2.5872\n",
      "Epoch [84/1000], Step [10], D Loss: 3.4711, G Loss: -2.6007\n",
      "Epoch [84/1000], Step [15], D Loss: 3.5017, G Loss: -2.6413\n",
      "Epoch [84/1000], Step [20], D Loss: 3.3033, G Loss: -2.5793\n",
      "Epoch [84/1000], Step [25], D Loss: 3.2692, G Loss: -2.5060\n",
      "Epoch [84/1000], Step [30], D Loss: 3.1854, G Loss: -2.5462\n",
      "Epoch [84/1000], Step [35], D Loss: 3.2855, G Loss: -2.6433\n",
      "Epoch [84/1000], Step [40], D Loss: 3.4199, G Loss: -2.7350\n",
      "Epoch [84/1000], Step [45], D Loss: 3.3491, G Loss: -2.7141\n",
      "Epoch [84/1000], Step [50], D Loss: 3.3316, G Loss: -2.7680\n",
      "Epoch [84/1000], Step [55], D Loss: 3.5309, G Loss: -2.8019\n",
      "Epoch [84/1000], Step [60], D Loss: 3.7905, G Loss: -2.9715\n",
      "Epoch [84/1000], Step [65], D Loss: 3.7359, G Loss: -2.9930\n",
      "Epoch [84/1000], Step [70], D Loss: 3.6914, G Loss: -2.9649\n",
      "Epoch [84/1000], Step [75], D Loss: 3.6054, G Loss: -2.9571\n",
      "Epoch [85/1000], Step [1], D Loss: 3.6378, G Loss: -2.9048\n",
      "Epoch [85/1000], Step [5], D Loss: 3.4619, G Loss: -2.8379\n",
      "Epoch [85/1000], Step [10], D Loss: 3.4441, G Loss: -2.7863\n",
      "Epoch [85/1000], Step [15], D Loss: 3.4129, G Loss: -2.6828\n",
      "Epoch [85/1000], Step [20], D Loss: 3.4597, G Loss: -2.6831\n",
      "Epoch [85/1000], Step [25], D Loss: 3.4058, G Loss: -2.7220\n",
      "Epoch [85/1000], Step [30], D Loss: 3.5503, G Loss: -2.7512\n",
      "Epoch [85/1000], Step [35], D Loss: 3.4087, G Loss: -2.7306\n",
      "Epoch [85/1000], Step [40], D Loss: 3.3262, G Loss: -2.6485\n",
      "Epoch [85/1000], Step [45], D Loss: 3.3031, G Loss: -2.5902\n",
      "Epoch [85/1000], Step [50], D Loss: 3.2417, G Loss: -2.6429\n",
      "Epoch [85/1000], Step [55], D Loss: 3.4276, G Loss: -2.7341\n",
      "Epoch [85/1000], Step [60], D Loss: 3.4933, G Loss: -2.8119\n",
      "Epoch [85/1000], Step [65], D Loss: 3.5172, G Loss: -2.8817\n",
      "Epoch [85/1000], Step [70], D Loss: 3.4628, G Loss: -2.8843\n",
      "Epoch [85/1000], Step [75], D Loss: 3.7789, G Loss: -2.9208\n",
      "Epoch [86/1000], Step [1], D Loss: 3.8436, G Loss: -2.9435\n",
      "Epoch [86/1000], Step [5], D Loss: 3.6566, G Loss: -2.9291\n",
      "Epoch [86/1000], Step [10], D Loss: 3.6517, G Loss: -2.9245\n",
      "Epoch [86/1000], Step [15], D Loss: 3.5854, G Loss: -2.8913\n",
      "Epoch [86/1000], Step [20], D Loss: 3.5502, G Loss: -2.7992\n",
      "Epoch [86/1000], Step [25], D Loss: 3.4041, G Loss: -2.7341\n",
      "Epoch [86/1000], Step [30], D Loss: 3.4887, G Loss: -2.7963\n",
      "Epoch [86/1000], Step [35], D Loss: 3.5445, G Loss: -2.8231\n",
      "Epoch [86/1000], Step [40], D Loss: 3.4657, G Loss: -2.7242\n",
      "Epoch [86/1000], Step [45], D Loss: 3.3877, G Loss: -2.7016\n",
      "Epoch [86/1000], Step [50], D Loss: 3.4559, G Loss: -2.6663\n",
      "Epoch [86/1000], Step [55], D Loss: 3.3575, G Loss: -2.6267\n",
      "Epoch [86/1000], Step [60], D Loss: 3.4050, G Loss: -2.5893\n",
      "Epoch [86/1000], Step [65], D Loss: 3.2622, G Loss: -2.5775\n",
      "Epoch [86/1000], Step [70], D Loss: 3.2103, G Loss: -2.6152\n",
      "Epoch [86/1000], Step [75], D Loss: 3.3949, G Loss: -2.6458\n",
      "Epoch [87/1000], Step [1], D Loss: 3.3028, G Loss: -2.7273\n",
      "Epoch [87/1000], Step [5], D Loss: 3.4282, G Loss: -2.7849\n",
      "Epoch [87/1000], Step [10], D Loss: 3.3737, G Loss: -2.8307\n",
      "Epoch [87/1000], Step [15], D Loss: 3.5901, G Loss: -2.8914\n",
      "Epoch [87/1000], Step [20], D Loss: 3.4484, G Loss: -2.8320\n",
      "Epoch [87/1000], Step [25], D Loss: 3.5399, G Loss: -2.8023\n",
      "Epoch [87/1000], Step [30], D Loss: 3.4105, G Loss: -2.8045\n",
      "Epoch [87/1000], Step [35], D Loss: 3.3168, G Loss: -2.7945\n",
      "Epoch [87/1000], Step [40], D Loss: 3.3457, G Loss: -2.8180\n",
      "Epoch [87/1000], Step [45], D Loss: 3.4383, G Loss: -2.8013\n",
      "Epoch [87/1000], Step [50], D Loss: 3.3100, G Loss: -2.7589\n",
      "Epoch [87/1000], Step [55], D Loss: 3.2325, G Loss: -2.7113\n",
      "Epoch [87/1000], Step [60], D Loss: 3.2429, G Loss: -2.6222\n",
      "Epoch [87/1000], Step [65], D Loss: 3.2224, G Loss: -2.5660\n",
      "Epoch [87/1000], Step [70], D Loss: 3.2585, G Loss: -2.5085\n",
      "Epoch [87/1000], Step [75], D Loss: 3.1190, G Loss: -2.4751\n",
      "Epoch [88/1000], Step [1], D Loss: 3.1997, G Loss: -2.4785\n",
      "Epoch [88/1000], Step [5], D Loss: 3.2168, G Loss: -2.5522\n",
      "Epoch [88/1000], Step [10], D Loss: 3.1649, G Loss: -2.5933\n",
      "Epoch [88/1000], Step [15], D Loss: 3.4290, G Loss: -2.6342\n",
      "Epoch [88/1000], Step [20], D Loss: 3.3072, G Loss: -2.6059\n",
      "Epoch [88/1000], Step [25], D Loss: 3.0947, G Loss: -2.5908\n",
      "Epoch [88/1000], Step [30], D Loss: 3.1958, G Loss: -2.6221\n",
      "Epoch [88/1000], Step [35], D Loss: 3.3512, G Loss: -2.6791\n",
      "Epoch [88/1000], Step [40], D Loss: 3.4325, G Loss: -2.7345\n",
      "Epoch [88/1000], Step [45], D Loss: 3.3963, G Loss: -2.7684\n",
      "Epoch [88/1000], Step [50], D Loss: 3.3829, G Loss: -2.7843\n",
      "Epoch [88/1000], Step [55], D Loss: 3.5774, G Loss: -2.8538\n",
      "Epoch [88/1000], Step [60], D Loss: 3.5321, G Loss: -2.8878\n",
      "Epoch [88/1000], Step [65], D Loss: 3.4018, G Loss: -2.8328\n",
      "Epoch [88/1000], Step [70], D Loss: 3.3709, G Loss: -2.7780\n",
      "Epoch [88/1000], Step [75], D Loss: 3.3997, G Loss: -2.7355\n",
      "Epoch [89/1000], Step [1], D Loss: 3.4666, G Loss: -2.6503\n",
      "Epoch [89/1000], Step [5], D Loss: 3.2415, G Loss: -2.5975\n",
      "Epoch [89/1000], Step [10], D Loss: 3.3849, G Loss: -2.5753\n",
      "Epoch [89/1000], Step [15], D Loss: 3.2644, G Loss: -2.5802\n",
      "Epoch [89/1000], Step [20], D Loss: 3.2835, G Loss: -2.5883\n",
      "Epoch [89/1000], Step [25], D Loss: 3.1725, G Loss: -2.5916\n",
      "Epoch [89/1000], Step [30], D Loss: 3.1940, G Loss: -2.6470\n",
      "Epoch [89/1000], Step [35], D Loss: 3.3352, G Loss: -2.6974\n",
      "Epoch [89/1000], Step [40], D Loss: 3.2066, G Loss: -2.6504\n",
      "Epoch [89/1000], Step [45], D Loss: 3.1826, G Loss: -2.6468\n",
      "Epoch [89/1000], Step [50], D Loss: 3.3232, G Loss: -2.6425\n",
      "Epoch [89/1000], Step [55], D Loss: 3.1636, G Loss: -2.6506\n",
      "Epoch [89/1000], Step [60], D Loss: 3.3405, G Loss: -2.6448\n",
      "Epoch [89/1000], Step [65], D Loss: 3.2965, G Loss: -2.6561\n",
      "Epoch [89/1000], Step [70], D Loss: 3.3899, G Loss: -2.6893\n",
      "Epoch [89/1000], Step [75], D Loss: 3.3615, G Loss: -2.7714\n",
      "Epoch [90/1000], Step [1], D Loss: 3.4569, G Loss: -2.7921\n",
      "Epoch [90/1000], Step [5], D Loss: 3.4573, G Loss: -2.7630\n",
      "Epoch [90/1000], Step [10], D Loss: 3.4084, G Loss: -2.7276\n",
      "Epoch [90/1000], Step [15], D Loss: 3.2450, G Loss: -2.6209\n",
      "Epoch [90/1000], Step [20], D Loss: 3.2252, G Loss: -2.5707\n",
      "Epoch [90/1000], Step [25], D Loss: 3.2957, G Loss: -2.6025\n",
      "Epoch [90/1000], Step [30], D Loss: 3.3206, G Loss: -2.6246\n",
      "Epoch [90/1000], Step [35], D Loss: 3.3382, G Loss: -2.6022\n",
      "Epoch [90/1000], Step [40], D Loss: 3.4357, G Loss: -2.6801\n",
      "Epoch [90/1000], Step [45], D Loss: 3.4503, G Loss: -2.7221\n",
      "Epoch [90/1000], Step [50], D Loss: 3.4944, G Loss: -2.7284\n",
      "Epoch [90/1000], Step [55], D Loss: 3.4240, G Loss: -2.7174\n",
      "Epoch [90/1000], Step [60], D Loss: 3.2703, G Loss: -2.6446\n",
      "Epoch [90/1000], Step [65], D Loss: 3.2472, G Loss: -2.5870\n",
      "Epoch [90/1000], Step [70], D Loss: 3.3638, G Loss: -2.6533\n",
      "Epoch [90/1000], Step [75], D Loss: 3.3191, G Loss: -2.7291\n",
      "Epoch [91/1000], Step [1], D Loss: 3.3094, G Loss: -2.7729\n",
      "Epoch [91/1000], Step [5], D Loss: 3.2841, G Loss: -2.6935\n",
      "Epoch [91/1000], Step [10], D Loss: 3.3619, G Loss: -2.6725\n",
      "Epoch [91/1000], Step [15], D Loss: 3.4220, G Loss: -2.6487\n",
      "Epoch [91/1000], Step [20], D Loss: 3.3712, G Loss: -2.5998\n",
      "Epoch [91/1000], Step [25], D Loss: 3.3870, G Loss: -2.5876\n",
      "Epoch [91/1000], Step [30], D Loss: 3.3461, G Loss: -2.6027\n",
      "Epoch [91/1000], Step [35], D Loss: 3.2919, G Loss: -2.6508\n",
      "Epoch [91/1000], Step [40], D Loss: 3.2401, G Loss: -2.7459\n",
      "Epoch [91/1000], Step [45], D Loss: 3.4597, G Loss: -2.8616\n",
      "Epoch [91/1000], Step [50], D Loss: 3.4986, G Loss: -2.8417\n",
      "Epoch [91/1000], Step [55], D Loss: 3.4508, G Loss: -2.8178\n",
      "Epoch [91/1000], Step [60], D Loss: 3.5168, G Loss: -2.8095\n",
      "Epoch [91/1000], Step [65], D Loss: 3.6302, G Loss: -2.7910\n",
      "Epoch [91/1000], Step [70], D Loss: 3.3855, G Loss: -2.7082\n",
      "Epoch [91/1000], Step [75], D Loss: 3.2700, G Loss: -2.6673\n",
      "Epoch [92/1000], Step [1], D Loss: 3.3338, G Loss: -2.6637\n",
      "Epoch [92/1000], Step [5], D Loss: 3.4149, G Loss: -2.6239\n",
      "Epoch [92/1000], Step [10], D Loss: 3.3525, G Loss: -2.6519\n",
      "Epoch [92/1000], Step [15], D Loss: 3.3222, G Loss: -2.6344\n",
      "Epoch [92/1000], Step [20], D Loss: 3.4038, G Loss: -2.7033\n",
      "Epoch [92/1000], Step [25], D Loss: 3.3904, G Loss: -2.7672\n",
      "Epoch [92/1000], Step [30], D Loss: 3.4804, G Loss: -2.8525\n",
      "Epoch [92/1000], Step [35], D Loss: 3.5381, G Loss: -2.8659\n",
      "Epoch [92/1000], Step [40], D Loss: 3.5253, G Loss: -2.8236\n",
      "Epoch [92/1000], Step [45], D Loss: 3.4388, G Loss: -2.7265\n",
      "Epoch [92/1000], Step [50], D Loss: 3.2181, G Loss: -2.6825\n",
      "Epoch [92/1000], Step [55], D Loss: 3.3113, G Loss: -2.7134\n",
      "Epoch [92/1000], Step [60], D Loss: 3.3935, G Loss: -2.7821\n",
      "Epoch [92/1000], Step [65], D Loss: 3.5469, G Loss: -2.7655\n",
      "Epoch [92/1000], Step [70], D Loss: 3.5357, G Loss: -2.7934\n",
      "Epoch [92/1000], Step [75], D Loss: 3.6161, G Loss: -2.7505\n",
      "Epoch [93/1000], Step [1], D Loss: 3.5553, G Loss: -2.7275\n",
      "Epoch [93/1000], Step [5], D Loss: 3.3388, G Loss: -2.7039\n",
      "Epoch [93/1000], Step [10], D Loss: 3.3799, G Loss: -2.7352\n",
      "Epoch [93/1000], Step [15], D Loss: 3.2522, G Loss: -2.7253\n",
      "Epoch [93/1000], Step [20], D Loss: 3.3685, G Loss: -2.6936\n",
      "Epoch [93/1000], Step [25], D Loss: 3.1538, G Loss: -2.6092\n",
      "Epoch [93/1000], Step [30], D Loss: 3.0531, G Loss: -2.5798\n",
      "Epoch [93/1000], Step [35], D Loss: 3.0793, G Loss: -2.6244\n",
      "Epoch [93/1000], Step [40], D Loss: 3.1628, G Loss: -2.6559\n",
      "Epoch [93/1000], Step [45], D Loss: 3.0491, G Loss: -2.6399\n",
      "Epoch [93/1000], Step [50], D Loss: 3.1777, G Loss: -2.6110\n",
      "Epoch [93/1000], Step [55], D Loss: 3.1933, G Loss: -2.5695\n",
      "Epoch [93/1000], Step [60], D Loss: 3.1576, G Loss: -2.5722\n",
      "Epoch [93/1000], Step [65], D Loss: 3.3700, G Loss: -2.7556\n",
      "Epoch [93/1000], Step [70], D Loss: 3.5478, G Loss: -2.9136\n",
      "Epoch [93/1000], Step [75], D Loss: 3.3885, G Loss: -3.0268\n",
      "Epoch [94/1000], Step [1], D Loss: 3.5554, G Loss: -3.0231\n",
      "Epoch [94/1000], Step [5], D Loss: 3.4213, G Loss: -2.9572\n",
      "Epoch [94/1000], Step [10], D Loss: 3.4834, G Loss: -2.8934\n",
      "Epoch [94/1000], Step [15], D Loss: 3.4629, G Loss: -2.8375\n",
      "Epoch [94/1000], Step [20], D Loss: 3.3072, G Loss: -2.7468\n",
      "Epoch [94/1000], Step [25], D Loss: 3.3742, G Loss: -2.7134\n",
      "Epoch [94/1000], Step [30], D Loss: 3.2201, G Loss: -2.6695\n",
      "Epoch [94/1000], Step [35], D Loss: 3.1591, G Loss: -2.5904\n",
      "Epoch [94/1000], Step [40], D Loss: 3.1087, G Loss: -2.4743\n",
      "Epoch [94/1000], Step [45], D Loss: 3.2776, G Loss: -2.4546\n",
      "Epoch [94/1000], Step [50], D Loss: 3.0488, G Loss: -2.4251\n",
      "Epoch [94/1000], Step [55], D Loss: 3.1012, G Loss: -2.4618\n",
      "Epoch [94/1000], Step [60], D Loss: 3.0881, G Loss: -2.4357\n",
      "Epoch [94/1000], Step [65], D Loss: 3.0350, G Loss: -2.4570\n",
      "Epoch [94/1000], Step [70], D Loss: 3.1118, G Loss: -2.4891\n",
      "Epoch [94/1000], Step [75], D Loss: 2.9514, G Loss: -2.4385\n",
      "Epoch [95/1000], Step [1], D Loss: 3.0162, G Loss: -2.5078\n",
      "Epoch [95/1000], Step [5], D Loss: 3.1159, G Loss: -2.6143\n",
      "Epoch [95/1000], Step [10], D Loss: 3.1398, G Loss: -2.7051\n",
      "Epoch [95/1000], Step [15], D Loss: 3.4192, G Loss: -2.8048\n",
      "Epoch [95/1000], Step [20], D Loss: 3.5162, G Loss: -2.9626\n",
      "Epoch [95/1000], Step [25], D Loss: 3.3172, G Loss: -2.9882\n",
      "Epoch [95/1000], Step [30], D Loss: 3.4973, G Loss: -3.0038\n",
      "Epoch [95/1000], Step [35], D Loss: 3.3901, G Loss: -2.9163\n",
      "Epoch [95/1000], Step [40], D Loss: 3.2859, G Loss: -2.8249\n",
      "Epoch [95/1000], Step [45], D Loss: 3.1243, G Loss: -2.7157\n",
      "Epoch [95/1000], Step [50], D Loss: 3.0817, G Loss: -2.6449\n",
      "Epoch [95/1000], Step [55], D Loss: 3.1265, G Loss: -2.6188\n",
      "Epoch [95/1000], Step [60], D Loss: 3.1801, G Loss: -2.5799\n",
      "Epoch [95/1000], Step [65], D Loss: 3.0319, G Loss: -2.4945\n",
      "Epoch [95/1000], Step [70], D Loss: 2.8013, G Loss: -2.4083\n",
      "Epoch [95/1000], Step [75], D Loss: 2.8968, G Loss: -2.3723\n",
      "Epoch [96/1000], Step [1], D Loss: 2.9159, G Loss: -2.3973\n",
      "Epoch [96/1000], Step [5], D Loss: 3.0550, G Loss: -2.3954\n",
      "Epoch [96/1000], Step [10], D Loss: 2.9924, G Loss: -2.4223\n",
      "Epoch [96/1000], Step [15], D Loss: 3.0865, G Loss: -2.4892\n",
      "Epoch [96/1000], Step [20], D Loss: 3.1390, G Loss: -2.5425\n",
      "Epoch [96/1000], Step [25], D Loss: 3.2034, G Loss: -2.6251\n",
      "Epoch [96/1000], Step [30], D Loss: 3.2906, G Loss: -2.7632\n",
      "Epoch [96/1000], Step [35], D Loss: 3.4651, G Loss: -2.7688\n",
      "Epoch [96/1000], Step [40], D Loss: 3.3958, G Loss: -2.7794\n",
      "Epoch [96/1000], Step [45], D Loss: 3.4473, G Loss: -2.8164\n",
      "Epoch [96/1000], Step [50], D Loss: 3.4113, G Loss: -2.7800\n",
      "Epoch [96/1000], Step [55], D Loss: 3.3909, G Loss: -2.7591\n",
      "Epoch [96/1000], Step [60], D Loss: 3.3834, G Loss: -2.7008\n",
      "Epoch [96/1000], Step [65], D Loss: 3.2531, G Loss: -2.5772\n",
      "Epoch [96/1000], Step [70], D Loss: 3.1722, G Loss: -2.4626\n",
      "Epoch [96/1000], Step [75], D Loss: 3.0074, G Loss: -2.4399\n",
      "Epoch [97/1000], Step [1], D Loss: 3.0679, G Loss: -2.4639\n",
      "Epoch [97/1000], Step [5], D Loss: 3.0493, G Loss: -2.4930\n",
      "Epoch [97/1000], Step [10], D Loss: 2.8868, G Loss: -2.5433\n",
      "Epoch [97/1000], Step [15], D Loss: 3.0643, G Loss: -2.5627\n",
      "Epoch [97/1000], Step [20], D Loss: 3.0115, G Loss: -2.5539\n",
      "Epoch [97/1000], Step [25], D Loss: 2.9488, G Loss: -2.4925\n",
      "Epoch [97/1000], Step [30], D Loss: 3.0395, G Loss: -2.4701\n",
      "Epoch [97/1000], Step [35], D Loss: 3.1519, G Loss: -2.5727\n",
      "Epoch [97/1000], Step [40], D Loss: 3.1847, G Loss: -2.6747\n",
      "Epoch [97/1000], Step [45], D Loss: 3.1788, G Loss: -2.7065\n",
      "Epoch [97/1000], Step [50], D Loss: 3.2143, G Loss: -2.7203\n",
      "Epoch [97/1000], Step [55], D Loss: 3.0078, G Loss: -2.7354\n",
      "Epoch [97/1000], Step [60], D Loss: 3.2664, G Loss: -2.7283\n",
      "Epoch [97/1000], Step [65], D Loss: 3.1487, G Loss: -2.6432\n",
      "Epoch [97/1000], Step [70], D Loss: 3.1392, G Loss: -2.5633\n",
      "Epoch [97/1000], Step [75], D Loss: 3.0611, G Loss: -2.5315\n",
      "Epoch [98/1000], Step [1], D Loss: 3.0424, G Loss: -2.5148\n",
      "Epoch [98/1000], Step [5], D Loss: 3.0625, G Loss: -2.4535\n",
      "Epoch [98/1000], Step [10], D Loss: 2.8109, G Loss: -2.4011\n",
      "Epoch [98/1000], Step [15], D Loss: 2.9576, G Loss: -2.3946\n",
      "Epoch [98/1000], Step [20], D Loss: 2.9593, G Loss: -2.3717\n",
      "Epoch [98/1000], Step [25], D Loss: 2.9203, G Loss: -2.3356\n",
      "Epoch [98/1000], Step [30], D Loss: 2.9159, G Loss: -2.3055\n",
      "Epoch [98/1000], Step [35], D Loss: 2.8870, G Loss: -2.3795\n",
      "Epoch [98/1000], Step [40], D Loss: 2.9353, G Loss: -2.4279\n",
      "Epoch [98/1000], Step [45], D Loss: 2.9206, G Loss: -2.4221\n",
      "Epoch [98/1000], Step [50], D Loss: 2.8565, G Loss: -2.4564\n",
      "Epoch [98/1000], Step [55], D Loss: 3.1930, G Loss: -2.5474\n",
      "Epoch [98/1000], Step [60], D Loss: 3.0845, G Loss: -2.5898\n",
      "Epoch [98/1000], Step [65], D Loss: 3.1230, G Loss: -2.6011\n",
      "Epoch [98/1000], Step [70], D Loss: 3.1147, G Loss: -2.6664\n",
      "Epoch [98/1000], Step [75], D Loss: 3.2296, G Loss: -2.6801\n",
      "Epoch [99/1000], Step [1], D Loss: 3.0031, G Loss: -2.6285\n",
      "Epoch [99/1000], Step [5], D Loss: 3.0928, G Loss: -2.6076\n",
      "Epoch [99/1000], Step [10], D Loss: 2.9671, G Loss: -2.4932\n",
      "Epoch [99/1000], Step [15], D Loss: 2.9291, G Loss: -2.4699\n",
      "Epoch [99/1000], Step [20], D Loss: 2.9175, G Loss: -2.4529\n",
      "Epoch [99/1000], Step [25], D Loss: 3.0117, G Loss: -2.4374\n",
      "Epoch [99/1000], Step [30], D Loss: 2.9103, G Loss: -2.3835\n",
      "Epoch [99/1000], Step [35], D Loss: 3.0008, G Loss: -2.3439\n",
      "Epoch [99/1000], Step [40], D Loss: 2.7880, G Loss: -2.3331\n",
      "Epoch [99/1000], Step [45], D Loss: 2.9657, G Loss: -2.3221\n",
      "Epoch [99/1000], Step [50], D Loss: 2.8235, G Loss: -2.3298\n",
      "Epoch [99/1000], Step [55], D Loss: 2.9474, G Loss: -2.3378\n",
      "Epoch [99/1000], Step [60], D Loss: 2.9557, G Loss: -2.3318\n",
      "Epoch [99/1000], Step [65], D Loss: 2.8812, G Loss: -2.2988\n",
      "Epoch [99/1000], Step [70], D Loss: 2.7243, G Loss: -2.2606\n",
      "Epoch [99/1000], Step [75], D Loss: 2.9283, G Loss: -2.2904\n",
      "Epoch [100/1000], Step [1], D Loss: 2.9934, G Loss: -2.3922\n",
      "Epoch [100/1000], Step [5], D Loss: 3.0517, G Loss: -2.4627\n",
      "Epoch [100/1000], Step [10], D Loss: 3.2274, G Loss: -2.5530\n",
      "Epoch [100/1000], Step [15], D Loss: 3.1886, G Loss: -2.5477\n",
      "Epoch [100/1000], Step [20], D Loss: 3.0138, G Loss: -2.4889\n",
      "Epoch [100/1000], Step [25], D Loss: 3.1554, G Loss: -2.5188\n",
      "Epoch [100/1000], Step [30], D Loss: 3.0276, G Loss: -2.4893\n",
      "Epoch [100/1000], Step [35], D Loss: 2.9103, G Loss: -2.5209\n",
      "Epoch [100/1000], Step [40], D Loss: 2.9957, G Loss: -2.4962\n",
      "Epoch [100/1000], Step [45], D Loss: 2.9328, G Loss: -2.4656\n",
      "Epoch [100/1000], Step [50], D Loss: 2.9358, G Loss: -2.4320\n",
      "Epoch [100/1000], Step [55], D Loss: 2.7430, G Loss: -2.3337\n",
      "Epoch [100/1000], Step [60], D Loss: 2.8436, G Loss: -2.2915\n",
      "Epoch [100/1000], Step [65], D Loss: 2.8599, G Loss: -2.2944\n",
      "Epoch [100/1000], Step [70], D Loss: 2.7032, G Loss: -2.2624\n",
      "Epoch [100/1000], Step [75], D Loss: 2.6591, G Loss: -2.1955\n",
      "Epoch [101/1000], Step [1], D Loss: 2.7006, G Loss: -2.2007\n",
      "Epoch [101/1000], Step [5], D Loss: 2.8630, G Loss: -2.1691\n",
      "Epoch [101/1000], Step [10], D Loss: 2.6557, G Loss: -2.1577\n",
      "Epoch [101/1000], Step [15], D Loss: 2.7372, G Loss: -2.1604\n",
      "Epoch [101/1000], Step [20], D Loss: 2.8751, G Loss: -2.2722\n",
      "Epoch [101/1000], Step [25], D Loss: 2.9701, G Loss: -2.4206\n",
      "Epoch [101/1000], Step [30], D Loss: 2.8925, G Loss: -2.4534\n",
      "Epoch [101/1000], Step [35], D Loss: 2.8999, G Loss: -2.4543\n",
      "Epoch [101/1000], Step [40], D Loss: 3.0222, G Loss: -2.4867\n",
      "Epoch [101/1000], Step [45], D Loss: 3.0854, G Loss: -2.4568\n",
      "Epoch [101/1000], Step [50], D Loss: 2.9767, G Loss: -2.4398\n",
      "Epoch [101/1000], Step [55], D Loss: 2.8581, G Loss: -2.4478\n",
      "Epoch [101/1000], Step [60], D Loss: 2.9612, G Loss: -2.4049\n",
      "Epoch [101/1000], Step [65], D Loss: 2.7597, G Loss: -2.3197\n",
      "Epoch [101/1000], Step [70], D Loss: 2.7400, G Loss: -2.2829\n",
      "Epoch [101/1000], Step [75], D Loss: 2.6344, G Loss: -2.2011\n",
      "Epoch [102/1000], Step [1], D Loss: 2.5786, G Loss: -2.1429\n",
      "Epoch [102/1000], Step [5], D Loss: 2.5419, G Loss: -2.1080\n",
      "Epoch [102/1000], Step [10], D Loss: 2.6224, G Loss: -2.0868\n",
      "Epoch [102/1000], Step [15], D Loss: 2.6601, G Loss: -2.0952\n",
      "Epoch [102/1000], Step [20], D Loss: 2.7159, G Loss: -2.1026\n",
      "Epoch [102/1000], Step [25], D Loss: 2.8331, G Loss: -2.2037\n",
      "Epoch [102/1000], Step [30], D Loss: 2.9897, G Loss: -2.2621\n",
      "Epoch [102/1000], Step [35], D Loss: 2.8022, G Loss: -2.2660\n",
      "Epoch [102/1000], Step [40], D Loss: 2.8572, G Loss: -2.3270\n",
      "Epoch [102/1000], Step [45], D Loss: 2.9080, G Loss: -2.4141\n",
      "Epoch [102/1000], Step [50], D Loss: 2.8698, G Loss: -2.3858\n",
      "Epoch [102/1000], Step [55], D Loss: 2.7868, G Loss: -2.3030\n",
      "Epoch [102/1000], Step [60], D Loss: 2.7389, G Loss: -2.2701\n",
      "Epoch [102/1000], Step [65], D Loss: 2.7893, G Loss: -2.1677\n",
      "Epoch [102/1000], Step [70], D Loss: 2.8079, G Loss: -2.1740\n",
      "Epoch [102/1000], Step [75], D Loss: 2.6667, G Loss: -2.1836\n",
      "Epoch [103/1000], Step [1], D Loss: 2.7227, G Loss: -2.1865\n",
      "Epoch [103/1000], Step [5], D Loss: 2.8345, G Loss: -2.1866\n",
      "Epoch [103/1000], Step [10], D Loss: 2.7867, G Loss: -2.2116\n",
      "Epoch [103/1000], Step [15], D Loss: 2.6599, G Loss: -2.1376\n",
      "Epoch [103/1000], Step [20], D Loss: 2.6095, G Loss: -2.0734\n",
      "Epoch [103/1000], Step [25], D Loss: 2.6978, G Loss: -2.1014\n",
      "Epoch [103/1000], Step [30], D Loss: 2.8255, G Loss: -2.2012\n",
      "Epoch [103/1000], Step [35], D Loss: 2.6588, G Loss: -2.2011\n",
      "Epoch [103/1000], Step [40], D Loss: 2.6129, G Loss: -2.2157\n",
      "Epoch [103/1000], Step [45], D Loss: 2.6822, G Loss: -2.2111\n",
      "Epoch [103/1000], Step [50], D Loss: 3.0019, G Loss: -2.2742\n",
      "Epoch [103/1000], Step [55], D Loss: 3.1626, G Loss: -2.3100\n",
      "Epoch [103/1000], Step [60], D Loss: 3.0459, G Loss: -2.3399\n",
      "Epoch [103/1000], Step [65], D Loss: 2.7533, G Loss: -2.2453\n",
      "Epoch [103/1000], Step [70], D Loss: 2.7643, G Loss: -2.2250\n",
      "Epoch [103/1000], Step [75], D Loss: 2.5883, G Loss: -2.1652\n",
      "Epoch [104/1000], Step [1], D Loss: 2.6831, G Loss: -2.1155\n",
      "Epoch [104/1000], Step [5], D Loss: 2.6672, G Loss: -2.1648\n",
      "Epoch [104/1000], Step [10], D Loss: 2.6239, G Loss: -2.1995\n",
      "Epoch [104/1000], Step [15], D Loss: 2.7569, G Loss: -2.1996\n",
      "Epoch [104/1000], Step [20], D Loss: 2.7642, G Loss: -2.1291\n",
      "Epoch [104/1000], Step [25], D Loss: 2.6186, G Loss: -2.0824\n",
      "Epoch [104/1000], Step [30], D Loss: 2.6149, G Loss: -1.9625\n",
      "Epoch [104/1000], Step [35], D Loss: 2.5558, G Loss: -1.8761\n",
      "Epoch [104/1000], Step [40], D Loss: 2.4301, G Loss: -1.9001\n",
      "Epoch [104/1000], Step [45], D Loss: 2.5325, G Loss: -2.0267\n",
      "Epoch [104/1000], Step [50], D Loss: 2.6939, G Loss: -2.1640\n",
      "Epoch [104/1000], Step [55], D Loss: 2.6225, G Loss: -2.2098\n",
      "Epoch [104/1000], Step [60], D Loss: 2.6744, G Loss: -2.2604\n",
      "Epoch [104/1000], Step [65], D Loss: 2.8115, G Loss: -2.1793\n",
      "Epoch [104/1000], Step [70], D Loss: 2.6540, G Loss: -2.1993\n",
      "Epoch [104/1000], Step [75], D Loss: 2.7283, G Loss: -2.2108\n",
      "Epoch [105/1000], Step [1], D Loss: 2.8507, G Loss: -2.2175\n",
      "Epoch [105/1000], Step [5], D Loss: 2.9321, G Loss: -2.2296\n",
      "Epoch [105/1000], Step [10], D Loss: 2.7849, G Loss: -2.2129\n",
      "Epoch [105/1000], Step [15], D Loss: 2.7100, G Loss: -2.1398\n",
      "Epoch [105/1000], Step [20], D Loss: 2.7469, G Loss: -2.1080\n",
      "Epoch [105/1000], Step [25], D Loss: 2.5863, G Loss: -1.9923\n",
      "Epoch [105/1000], Step [30], D Loss: 2.4723, G Loss: -1.9347\n",
      "Epoch [105/1000], Step [35], D Loss: 2.5252, G Loss: -1.9513\n",
      "Epoch [105/1000], Step [40], D Loss: 2.4298, G Loss: -1.8891\n",
      "Epoch [105/1000], Step [45], D Loss: 2.5000, G Loss: -1.8446\n",
      "Epoch [105/1000], Step [50], D Loss: 2.3196, G Loss: -1.9147\n",
      "Epoch [105/1000], Step [55], D Loss: 2.5484, G Loss: -1.9276\n",
      "Epoch [105/1000], Step [60], D Loss: 2.6466, G Loss: -2.0076\n",
      "Epoch [105/1000], Step [65], D Loss: 2.5582, G Loss: -2.0606\n",
      "Epoch [105/1000], Step [70], D Loss: 2.6962, G Loss: -2.0855\n",
      "Epoch [105/1000], Step [75], D Loss: 2.4994, G Loss: -2.0865\n",
      "Epoch [106/1000], Step [1], D Loss: 2.7907, G Loss: -2.1659\n",
      "Epoch [106/1000], Step [5], D Loss: 2.6279, G Loss: -2.1426\n",
      "Epoch [106/1000], Step [10], D Loss: 2.7093, G Loss: -2.1304\n",
      "Epoch [106/1000], Step [15], D Loss: 2.5889, G Loss: -2.0452\n",
      "Epoch [106/1000], Step [20], D Loss: 2.4715, G Loss: -2.0131\n",
      "Epoch [106/1000], Step [25], D Loss: 2.5106, G Loss: -2.0471\n",
      "Epoch [106/1000], Step [30], D Loss: 2.6257, G Loss: -2.0551\n",
      "Epoch [106/1000], Step [35], D Loss: 2.5839, G Loss: -1.9955\n",
      "Epoch [106/1000], Step [40], D Loss: 2.6314, G Loss: -1.9683\n",
      "Epoch [106/1000], Step [45], D Loss: 2.5510, G Loss: -1.9721\n",
      "Epoch [106/1000], Step [50], D Loss: 2.3070, G Loss: -1.8809\n",
      "Epoch [106/1000], Step [55], D Loss: 2.3142, G Loss: -1.8651\n",
      "Epoch [106/1000], Step [60], D Loss: 2.2402, G Loss: -1.8728\n",
      "Epoch [106/1000], Step [65], D Loss: 2.3924, G Loss: -1.8673\n",
      "Epoch [106/1000], Step [70], D Loss: 2.3368, G Loss: -1.8713\n",
      "Epoch [106/1000], Step [75], D Loss: 2.4279, G Loss: -1.9251\n",
      "Epoch [107/1000], Step [1], D Loss: 2.4481, G Loss: -1.9317\n",
      "Epoch [107/1000], Step [5], D Loss: 2.5120, G Loss: -1.9384\n",
      "Epoch [107/1000], Step [10], D Loss: 2.5059, G Loss: -1.9367\n",
      "Epoch [107/1000], Step [15], D Loss: 2.6077, G Loss: -1.9557\n",
      "Epoch [107/1000], Step [20], D Loss: 2.4544, G Loss: -1.9456\n",
      "Epoch [107/1000], Step [25], D Loss: 2.5732, G Loss: -2.0427\n",
      "Epoch [107/1000], Step [30], D Loss: 2.6005, G Loss: -2.0989\n",
      "Epoch [107/1000], Step [35], D Loss: 2.6353, G Loss: -2.0760\n",
      "Epoch [107/1000], Step [40], D Loss: 2.5240, G Loss: -1.9858\n",
      "Epoch [107/1000], Step [45], D Loss: 2.5469, G Loss: -1.8608\n",
      "Epoch [107/1000], Step [50], D Loss: 2.4991, G Loss: -1.7642\n",
      "Epoch [107/1000], Step [55], D Loss: 2.2287, G Loss: -1.7046\n",
      "Epoch [107/1000], Step [60], D Loss: 2.5096, G Loss: -1.7700\n",
      "Epoch [107/1000], Step [65], D Loss: 2.3607, G Loss: -1.7448\n",
      "Epoch [107/1000], Step [70], D Loss: 2.4647, G Loss: -1.7580\n",
      "Epoch [107/1000], Step [75], D Loss: 2.3918, G Loss: -1.7774\n",
      "Epoch [108/1000], Step [1], D Loss: 2.4037, G Loss: -1.7907\n",
      "Epoch [108/1000], Step [5], D Loss: 2.4085, G Loss: -1.8447\n",
      "Epoch [108/1000], Step [10], D Loss: 2.3444, G Loss: -1.9163\n",
      "Epoch [108/1000], Step [15], D Loss: 2.4983, G Loss: -1.9139\n",
      "Epoch [108/1000], Step [20], D Loss: 2.6791, G Loss: -2.0466\n",
      "Epoch [108/1000], Step [25], D Loss: 2.6109, G Loss: -2.0244\n",
      "Epoch [108/1000], Step [30], D Loss: 2.5828, G Loss: -2.0118\n",
      "Epoch [108/1000], Step [35], D Loss: 2.5699, G Loss: -1.9971\n",
      "Epoch [108/1000], Step [40], D Loss: 2.5427, G Loss: -1.9261\n",
      "Epoch [108/1000], Step [45], D Loss: 2.2853, G Loss: -1.7951\n",
      "Epoch [108/1000], Step [50], D Loss: 2.3104, G Loss: -1.7005\n",
      "Epoch [108/1000], Step [55], D Loss: 2.3012, G Loss: -1.6285\n",
      "Epoch [108/1000], Step [60], D Loss: 2.1480, G Loss: -1.5968\n",
      "Epoch [108/1000], Step [65], D Loss: 2.2168, G Loss: -1.6189\n",
      "Epoch [108/1000], Step [70], D Loss: 2.4005, G Loss: -1.6405\n",
      "Epoch [108/1000], Step [75], D Loss: 2.2409, G Loss: -1.6711\n",
      "Epoch [109/1000], Step [1], D Loss: 2.3119, G Loss: -1.6895\n",
      "Epoch [109/1000], Step [5], D Loss: 2.1212, G Loss: -1.6513\n",
      "Epoch [109/1000], Step [10], D Loss: 2.1909, G Loss: -1.6224\n",
      "Epoch [109/1000], Step [15], D Loss: 2.5780, G Loss: -1.7087\n",
      "Epoch [109/1000], Step [20], D Loss: 2.5058, G Loss: -1.7796\n",
      "Epoch [109/1000], Step [25], D Loss: 2.4746, G Loss: -1.8472\n",
      "Epoch [109/1000], Step [30], D Loss: 2.5796, G Loss: -1.8992\n",
      "Epoch [109/1000], Step [35], D Loss: 2.5257, G Loss: -1.8518\n",
      "Epoch [109/1000], Step [40], D Loss: 2.3436, G Loss: -1.7632\n",
      "Epoch [109/1000], Step [45], D Loss: 2.3285, G Loss: -1.7058\n",
      "Epoch [109/1000], Step [50], D Loss: 2.3079, G Loss: -1.6888\n",
      "Epoch [109/1000], Step [55], D Loss: 2.2566, G Loss: -1.7333\n",
      "Epoch [109/1000], Step [60], D Loss: 2.1968, G Loss: -1.6697\n",
      "Epoch [109/1000], Step [65], D Loss: 2.1752, G Loss: -1.6521\n",
      "Epoch [109/1000], Step [70], D Loss: 2.0852, G Loss: -1.5604\n",
      "Epoch [109/1000], Step [75], D Loss: 2.1432, G Loss: -1.5036\n",
      "Epoch [110/1000], Step [1], D Loss: 2.0332, G Loss: -1.4846\n",
      "Epoch [110/1000], Step [5], D Loss: 2.3839, G Loss: -1.5653\n",
      "Epoch [110/1000], Step [10], D Loss: 2.3524, G Loss: -1.6868\n",
      "Epoch [110/1000], Step [15], D Loss: 2.3388, G Loss: -1.7551\n",
      "Epoch [110/1000], Step [20], D Loss: 2.2131, G Loss: -1.7949\n",
      "Epoch [110/1000], Step [25], D Loss: 2.3528, G Loss: -1.7919\n",
      "Epoch [110/1000], Step [30], D Loss: 2.2880, G Loss: -1.7766\n",
      "Epoch [110/1000], Step [35], D Loss: 2.2036, G Loss: -1.7296\n",
      "Epoch [110/1000], Step [40], D Loss: 2.3472, G Loss: -1.6820\n",
      "Epoch [110/1000], Step [45], D Loss: 2.2576, G Loss: -1.6332\n",
      "Epoch [110/1000], Step [50], D Loss: 2.2801, G Loss: -1.6346\n",
      "Epoch [110/1000], Step [55], D Loss: 2.2118, G Loss: -1.5694\n",
      "Epoch [110/1000], Step [60], D Loss: 1.9339, G Loss: -1.5047\n",
      "Epoch [110/1000], Step [65], D Loss: 1.8989, G Loss: -1.4455\n",
      "Epoch [110/1000], Step [70], D Loss: 1.7109, G Loss: -1.3373\n",
      "Epoch [110/1000], Step [75], D Loss: 1.9811, G Loss: -1.3218\n",
      "Epoch [111/1000], Step [1], D Loss: 1.9180, G Loss: -1.3622\n",
      "Epoch [111/1000], Step [5], D Loss: 1.8666, G Loss: -1.3407\n",
      "Epoch [111/1000], Step [10], D Loss: 2.0893, G Loss: -1.4168\n",
      "Epoch [111/1000], Step [15], D Loss: 1.9508, G Loss: -1.5066\n",
      "Epoch [111/1000], Step [20], D Loss: 1.9622, G Loss: -1.5779\n",
      "Epoch [111/1000], Step [25], D Loss: 2.0818, G Loss: -1.6465\n",
      "Epoch [111/1000], Step [30], D Loss: 2.1691, G Loss: -1.6851\n",
      "Epoch [111/1000], Step [35], D Loss: 1.9928, G Loss: -1.6583\n",
      "Epoch [111/1000], Step [40], D Loss: 2.0181, G Loss: -1.6161\n",
      "Epoch [111/1000], Step [45], D Loss: 1.8376, G Loss: -1.5237\n",
      "Epoch [111/1000], Step [50], D Loss: 1.9322, G Loss: -1.4306\n",
      "Epoch [111/1000], Step [55], D Loss: 1.9449, G Loss: -1.3995\n",
      "Epoch [111/1000], Step [60], D Loss: 1.9770, G Loss: -1.3451\n",
      "Epoch [111/1000], Step [65], D Loss: 1.8218, G Loss: -1.2931\n",
      "Epoch [111/1000], Step [70], D Loss: 1.9655, G Loss: -1.2895\n",
      "Epoch [111/1000], Step [75], D Loss: 1.8178, G Loss: -1.2823\n",
      "Epoch [112/1000], Step [1], D Loss: 1.9119, G Loss: -1.2876\n",
      "Epoch [112/1000], Step [5], D Loss: 1.9745, G Loss: -1.3762\n",
      "Epoch [112/1000], Step [10], D Loss: 2.0183, G Loss: -1.4939\n",
      "Epoch [112/1000], Step [15], D Loss: 2.1913, G Loss: -1.5119\n",
      "Epoch [112/1000], Step [20], D Loss: 1.9682, G Loss: -1.4842\n",
      "Epoch [112/1000], Step [25], D Loss: 2.2217, G Loss: -1.5486\n",
      "Epoch [112/1000], Step [30], D Loss: 1.9423, G Loss: -1.4747\n",
      "Epoch [112/1000], Step [35], D Loss: 1.7752, G Loss: -1.4181\n",
      "Epoch [112/1000], Step [40], D Loss: 1.6702, G Loss: -1.3389\n",
      "Epoch [112/1000], Step [45], D Loss: 1.6479, G Loss: -1.3021\n",
      "Epoch [112/1000], Step [50], D Loss: 1.6534, G Loss: -1.2748\n",
      "Epoch [112/1000], Step [55], D Loss: 1.5255, G Loss: -1.1754\n",
      "Epoch [112/1000], Step [60], D Loss: 1.3287, G Loss: -1.0793\n",
      "Epoch [112/1000], Step [65], D Loss: 1.4053, G Loss: -1.0501\n",
      "Epoch [112/1000], Step [70], D Loss: 1.6109, G Loss: -1.0722\n",
      "Epoch [112/1000], Step [75], D Loss: 1.5614, G Loss: -1.1147\n",
      "Epoch [113/1000], Step [1], D Loss: 1.8117, G Loss: -1.2556\n",
      "Epoch [113/1000], Step [5], D Loss: 1.5901, G Loss: -1.3288\n",
      "Epoch [113/1000], Step [10], D Loss: 1.8389, G Loss: -1.4588\n",
      "Epoch [113/1000], Step [15], D Loss: 1.5802, G Loss: -1.4246\n",
      "Epoch [113/1000], Step [20], D Loss: 1.4977, G Loss: -1.3210\n",
      "Epoch [113/1000], Step [25], D Loss: 1.3929, G Loss: -1.1889\n",
      "Epoch [113/1000], Step [30], D Loss: 1.4544, G Loss: -1.0788\n",
      "Epoch [113/1000], Step [35], D Loss: 1.3123, G Loss: -1.0321\n",
      "Epoch [113/1000], Step [40], D Loss: 1.2889, G Loss: -0.9989\n",
      "Epoch [113/1000], Step [45], D Loss: 1.4303, G Loss: -1.0472\n",
      "Epoch [113/1000], Step [50], D Loss: 1.6414, G Loss: -1.1093\n",
      "Epoch [113/1000], Step [55], D Loss: 1.4379, G Loss: -1.0964\n",
      "Epoch [113/1000], Step [60], D Loss: 1.4331, G Loss: -1.1085\n",
      "Epoch [113/1000], Step [65], D Loss: 1.5111, G Loss: -1.1393\n",
      "Epoch [113/1000], Step [70], D Loss: 1.7237, G Loss: -1.2432\n",
      "Epoch [113/1000], Step [75], D Loss: 1.8588, G Loss: -1.2812\n",
      "Epoch [114/1000], Step [1], D Loss: 1.5791, G Loss: -1.2127\n",
      "Epoch [114/1000], Step [5], D Loss: 1.5029, G Loss: -1.1557\n",
      "Epoch [114/1000], Step [10], D Loss: 1.2281, G Loss: -1.0374\n",
      "Epoch [114/1000], Step [15], D Loss: 1.1418, G Loss: -1.0034\n",
      "Epoch [114/1000], Step [20], D Loss: 1.2598, G Loss: -0.9834\n",
      "Epoch [114/1000], Step [25], D Loss: 1.2032, G Loss: -0.9095\n",
      "Epoch [114/1000], Step [30], D Loss: 1.2189, G Loss: -0.9120\n",
      "Epoch [114/1000], Step [35], D Loss: 1.2968, G Loss: -0.8623\n",
      "Epoch [114/1000], Step [40], D Loss: 1.1775, G Loss: -0.8458\n",
      "Epoch [114/1000], Step [45], D Loss: 1.2028, G Loss: -0.8455\n",
      "Epoch [114/1000], Step [50], D Loss: 1.3520, G Loss: -0.8925\n",
      "Epoch [114/1000], Step [55], D Loss: 1.5378, G Loss: -1.0298\n",
      "Epoch [114/1000], Step [60], D Loss: 1.2966, G Loss: -1.0818\n",
      "Epoch [114/1000], Step [65], D Loss: 1.4086, G Loss: -1.0615\n",
      "Epoch [114/1000], Step [70], D Loss: 1.2239, G Loss: -0.9679\n",
      "Epoch [114/1000], Step [75], D Loss: 1.0591, G Loss: -0.8609\n",
      "Epoch [115/1000], Step [1], D Loss: 1.0382, G Loss: -0.7952\n",
      "Epoch [115/1000], Step [5], D Loss: 1.0252, G Loss: -0.7804\n",
      "Epoch [115/1000], Step [10], D Loss: 1.0449, G Loss: -0.8118\n",
      "Epoch [115/1000], Step [15], D Loss: 0.9874, G Loss: -0.7625\n",
      "Epoch [115/1000], Step [20], D Loss: 1.0015, G Loss: -0.7352\n",
      "Epoch [115/1000], Step [25], D Loss: 1.0299, G Loss: -0.7394\n",
      "Epoch [115/1000], Step [30], D Loss: 1.3774, G Loss: -0.7918\n",
      "Epoch [115/1000], Step [35], D Loss: 1.4099, G Loss: -0.8299\n",
      "Epoch [115/1000], Step [40], D Loss: 1.0485, G Loss: -0.8042\n",
      "Epoch [115/1000], Step [45], D Loss: 1.0430, G Loss: -0.8364\n",
      "Epoch [115/1000], Step [50], D Loss: 1.3098, G Loss: -0.8226\n",
      "Epoch [115/1000], Step [55], D Loss: 1.0335, G Loss: -0.7623\n",
      "Epoch [115/1000], Step [60], D Loss: 1.1369, G Loss: -0.7562\n",
      "Epoch [115/1000], Step [65], D Loss: 0.8568, G Loss: -0.6519\n",
      "Epoch [115/1000], Step [70], D Loss: 0.8673, G Loss: -0.6770\n",
      "Epoch [115/1000], Step [75], D Loss: 0.7305, G Loss: -0.5987\n",
      "Epoch [116/1000], Step [1], D Loss: 0.6713, G Loss: -0.5599\n",
      "Epoch [116/1000], Step [5], D Loss: 0.6302, G Loss: -0.5450\n",
      "Epoch [116/1000], Step [10], D Loss: 0.7646, G Loss: -0.5692\n",
      "Epoch [116/1000], Step [15], D Loss: 0.8681, G Loss: -0.6266\n",
      "Epoch [116/1000], Step [20], D Loss: 0.8106, G Loss: -0.6630\n",
      "Epoch [116/1000], Step [25], D Loss: 0.8620, G Loss: -0.6549\n",
      "Epoch [116/1000], Step [30], D Loss: 0.8917, G Loss: -0.5985\n",
      "Epoch [116/1000], Step [35], D Loss: 0.9199, G Loss: -0.5656\n",
      "Epoch [116/1000], Step [40], D Loss: 1.0083, G Loss: -0.5696\n",
      "Epoch [116/1000], Step [45], D Loss: 0.9649, G Loss: -0.5842\n",
      "Epoch [116/1000], Step [50], D Loss: 0.9378, G Loss: -0.5456\n",
      "Epoch [116/1000], Step [55], D Loss: 0.8216, G Loss: -0.4831\n",
      "Epoch [116/1000], Step [60], D Loss: 0.7071, G Loss: -0.3999\n",
      "Epoch [116/1000], Step [65], D Loss: 0.7485, G Loss: -0.3601\n",
      "Epoch [116/1000], Step [70], D Loss: 0.6000, G Loss: -0.3652\n",
      "Epoch [116/1000], Step [75], D Loss: 0.7504, G Loss: -0.4158\n",
      "Epoch [117/1000], Step [1], D Loss: 0.8541, G Loss: -0.4788\n",
      "Epoch [117/1000], Step [5], D Loss: 0.9348, G Loss: -0.5398\n",
      "Epoch [117/1000], Step [10], D Loss: 0.9563, G Loss: -0.6153\n",
      "Epoch [117/1000], Step [15], D Loss: 1.1127, G Loss: -0.6563\n",
      "Epoch [117/1000], Step [20], D Loss: 1.0873, G Loss: -0.6568\n",
      "Epoch [117/1000], Step [25], D Loss: 0.9752, G Loss: -0.6000\n",
      "Epoch [117/1000], Step [30], D Loss: 1.0017, G Loss: -0.5839\n",
      "Epoch [117/1000], Step [35], D Loss: 0.8278, G Loss: -0.5157\n",
      "Epoch [117/1000], Step [40], D Loss: 0.9429, G Loss: -0.5046\n",
      "Epoch [117/1000], Step [45], D Loss: 0.7478, G Loss: -0.4762\n",
      "Epoch [117/1000], Step [50], D Loss: 0.5351, G Loss: -0.4051\n",
      "Epoch [117/1000], Step [55], D Loss: 0.5961, G Loss: -0.3099\n",
      "Epoch [117/1000], Step [60], D Loss: 0.4974, G Loss: -0.2878\n",
      "Epoch [117/1000], Step [65], D Loss: 0.5557, G Loss: -0.2442\n",
      "Epoch [117/1000], Step [70], D Loss: 0.5913, G Loss: -0.2802\n",
      "Epoch [117/1000], Step [75], D Loss: 0.8147, G Loss: -0.3097\n",
      "Epoch [118/1000], Step [1], D Loss: 0.6828, G Loss: -0.3161\n",
      "Epoch [118/1000], Step [5], D Loss: 0.8123, G Loss: -0.3603\n",
      "Epoch [118/1000], Step [10], D Loss: 0.7078, G Loss: -0.3922\n",
      "Epoch [118/1000], Step [15], D Loss: 0.6697, G Loss: -0.4079\n",
      "Epoch [118/1000], Step [20], D Loss: 0.6515, G Loss: -0.4134\n",
      "Epoch [118/1000], Step [25], D Loss: 0.7728, G Loss: -0.4071\n",
      "Epoch [118/1000], Step [30], D Loss: 0.6366, G Loss: -0.3810\n",
      "Epoch [118/1000], Step [35], D Loss: 0.7661, G Loss: -0.3709\n",
      "Epoch [118/1000], Step [40], D Loss: 0.7007, G Loss: -0.2840\n",
      "Epoch [118/1000], Step [45], D Loss: 0.5666, G Loss: -0.1979\n",
      "Epoch [118/1000], Step [50], D Loss: 0.6069, G Loss: -0.1193\n",
      "Epoch [118/1000], Step [55], D Loss: 0.7099, G Loss: -0.1186\n",
      "Epoch [118/1000], Step [60], D Loss: 0.6396, G Loss: -0.0899\n",
      "Epoch [118/1000], Step [65], D Loss: 0.6822, G Loss: -0.1032\n",
      "Epoch [118/1000], Step [70], D Loss: 0.7784, G Loss: -0.1330\n",
      "Epoch [118/1000], Step [75], D Loss: 0.7981, G Loss: -0.1169\n",
      "Epoch [119/1000], Step [1], D Loss: 0.7841, G Loss: -0.1283\n",
      "Epoch [119/1000], Step [5], D Loss: 0.5506, G Loss: -0.1832\n",
      "Epoch [119/1000], Step [10], D Loss: 0.6849, G Loss: -0.2688\n",
      "Epoch [119/1000], Step [15], D Loss: 0.6821, G Loss: -0.3295\n",
      "Epoch [119/1000], Step [20], D Loss: 0.6972, G Loss: -0.4299\n",
      "Epoch [119/1000], Step [25], D Loss: 0.6070, G Loss: -0.4194\n",
      "Epoch [119/1000], Step [30], D Loss: 0.5561, G Loss: -0.3199\n",
      "Epoch [119/1000], Step [35], D Loss: 0.4028, G Loss: -0.2144\n",
      "Epoch [119/1000], Step [40], D Loss: 0.3645, G Loss: -0.1631\n",
      "Epoch [119/1000], Step [45], D Loss: 0.2456, G Loss: -0.0875\n",
      "Epoch [119/1000], Step [50], D Loss: 0.1057, G Loss: 0.0206\n",
      "Epoch [119/1000], Step [55], D Loss: 0.4572, G Loss: 0.0487\n",
      "Epoch [119/1000], Step [60], D Loss: 0.3588, G Loss: 0.0634\n",
      "Epoch [119/1000], Step [65], D Loss: 0.4063, G Loss: 0.0479\n",
      "Epoch [119/1000], Step [70], D Loss: 0.2889, G Loss: 0.0133\n",
      "Epoch [119/1000], Step [75], D Loss: 0.5780, G Loss: -0.0627\n",
      "Epoch [120/1000], Step [1], D Loss: 0.5335, G Loss: -0.1538\n",
      "Epoch [120/1000], Step [5], D Loss: 0.4727, G Loss: -0.2056\n",
      "Epoch [120/1000], Step [10], D Loss: 0.4235, G Loss: -0.1847\n",
      "Epoch [120/1000], Step [15], D Loss: 0.3665, G Loss: -0.0889\n",
      "Epoch [120/1000], Step [20], D Loss: 0.3620, G Loss: 0.0020\n",
      "Epoch [120/1000], Step [25], D Loss: 0.4609, G Loss: 0.0452\n",
      "Epoch [120/1000], Step [30], D Loss: 0.6240, G Loss: 0.0692\n",
      "Epoch [120/1000], Step [35], D Loss: 0.4393, G Loss: 0.1390\n",
      "Epoch [120/1000], Step [40], D Loss: 0.4325, G Loss: 0.1840\n",
      "Epoch [120/1000], Step [45], D Loss: 0.4545, G Loss: 0.1903\n",
      "Epoch [120/1000], Step [50], D Loss: 0.3897, G Loss: 0.1617\n",
      "Epoch [120/1000], Step [55], D Loss: 0.3931, G Loss: 0.1211\n",
      "Epoch [120/1000], Step [60], D Loss: 0.3250, G Loss: 0.0580\n",
      "Epoch [120/1000], Step [65], D Loss: 0.3294, G Loss: 0.0016\n",
      "Epoch [120/1000], Step [70], D Loss: 0.1180, G Loss: -0.0187\n",
      "Epoch [120/1000], Step [75], D Loss: 0.3410, G Loss: -0.0063\n",
      "Epoch [121/1000], Step [1], D Loss: 0.1340, G Loss: 0.0204\n",
      "Epoch [121/1000], Step [5], D Loss: 0.1709, G Loss: 0.0168\n",
      "Epoch [121/1000], Step [10], D Loss: 0.0826, G Loss: 0.0462\n",
      "Epoch [121/1000], Step [15], D Loss: 0.4358, G Loss: 0.1107\n",
      "Epoch [121/1000], Step [20], D Loss: 0.4821, G Loss: 0.1682\n",
      "Epoch [121/1000], Step [25], D Loss: 0.1623, G Loss: 0.1776\n",
      "Epoch [121/1000], Step [30], D Loss: 0.2990, G Loss: 0.2043\n",
      "Epoch [121/1000], Step [35], D Loss: 0.5669, G Loss: 0.2445\n",
      "Epoch [121/1000], Step [40], D Loss: 0.8349, G Loss: 0.2419\n",
      "Epoch [121/1000], Step [45], D Loss: 0.9125, G Loss: 0.2346\n",
      "Epoch [121/1000], Step [50], D Loss: 1.0089, G Loss: 0.2271\n",
      "Epoch [121/1000], Step [55], D Loss: 0.8655, G Loss: 0.2469\n",
      "Epoch [121/1000], Step [60], D Loss: 0.3895, G Loss: 0.1627\n",
      "Epoch [121/1000], Step [65], D Loss: 0.3107, G Loss: 0.1620\n",
      "Epoch [121/1000], Step [70], D Loss: 0.3795, G Loss: 0.1964\n",
      "Epoch [121/1000], Step [75], D Loss: 0.3100, G Loss: 0.2549\n",
      "Early stopping at epoch 121\n",
      "\n",
      "Generating synthetic samples for Class 2...\n",
      "\n",
      "Generating synthetic samples for Class 3...\n",
      "\n",
      "Generating synthetic samples for Class 4...\n",
      "\n",
      "Evaluating synthetic data for Class 2...\n",
      "Silhouette Score for Class 2: 0.9901\n",
      "\n",
      "Evaluating synthetic data for Class 3...\n",
      "Silhouette Score for Class 3: 0.9805\n",
      "\n",
      "Evaluating synthetic data for Class 4...\n",
      "Silhouette Score for Class 4: 0.9865\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  \n",
    "\n",
    "\n",
    "# \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "z_dim = 100\n",
    "cond_dim = 5  #  \n",
    "bert_dim = 768\n",
    "hidden_dim = 256\n",
    "batch_size = 128\n",
    "num_epochs_gan = 1000  #   1000  Early Stopping\n",
    "lr_g = 1e-4  # Learning rate  Generator\n",
    "lr_d = 1e-5  # Learning rate  Discriminator\n",
    "lambda_gp = 10  #  Gradient Penalty\n",
    "n_critic = 3  #    Discriminator\n",
    "sim_weight = 0.01  #   0.01\n",
    "num_samples_per_class = 500  #   500  t-SNE\n",
    "output_dir = \"F:\\\\payan-nameh\\\\faz2 . 1404.04.02\\\\Date\\\\RNALocate\\\\\"\n",
    "\n",
    "#  )  \n",
    "X_bert = np.load(f\"{output_dir}X_bert.npy\")\n",
    "df_rnalocate = pd.read_csv(f\"{output_dir}rnalocate_dataset.csv\")\n",
    "labels_initial = df_rnalocate['label'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_bert = scaler.fit_transform(X_bert)\n",
    "X_bert_tensor = torch.FloatTensor(X_bert)\n",
    "labels_tensor = torch.LongTensor(labels_initial)\n",
    "dataset = TensorDataset(X_bert_tensor, labels_tensor)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class_counts = np.bincount(labels_initial, minlength=cond_dim)\n",
    "majority_class_size = np.max(class_counts)\n",
    "threshold = 0.3 * majority_class_size\n",
    "rare_classes = np.where(class_counts < threshold)[0].tolist()\n",
    "print(f\"Rare classes identified: {rare_classes} with counts: {class_counts[rare_classes]}\")\n",
    "\n",
    "#  )    Conditional GAN (WGAN-GP)\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        return self.norm(x + attn_output)\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, cond_dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.label_embed = nn.Linear(cond_dim, embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    def forward(self, x, labels):\n",
    "        one_hot = torch.zeros(labels.size(0), cond_dim).to(device)\n",
    "        one_hot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "        cond_embed = self.label_embed(one_hot)\n",
    "        cond_embed = cond_embed.unsqueeze(1)\n",
    "        attn_output, _ = self.attn(x, cond_embed, cond_embed)\n",
    "        return self.norm(x + attn_output)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(z_dim + cond_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.res1 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.attn1 = AttentionBlock(embed_dim=32, num_heads=4)\n",
    "        self.cross_attn = CrossAttentionBlock(embed_dim=32, cond_dim=cond_dim, num_heads=4)\n",
    "        self.attn2 = AttentionBlock(embed_dim=32, num_heads=4)\n",
    "        self.fc_post_attn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.res2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.out = spectral_norm(nn.Linear(hidden_dim, bert_dim))\n",
    "    def forward(self, z, labels):\n",
    "        class_onehot = F.one_hot(labels, num_classes=cond_dim).float().to(device)\n",
    "        gen_input = torch.cat((z, class_onehot), dim=1)\n",
    "        x = self.fc1(gen_input)\n",
    "        x = x + self.res1(x)\n",
    "        B = x.size(0)\n",
    "        x = x.view(B, 8, 32)\n",
    "        x = self.attn1(x)\n",
    "        x = self.cross_attn(x, labels)\n",
    "        x = self.attn2(x)\n",
    "        x = x.view(B, -1)\n",
    "        x = self.fc_post_attn(x)\n",
    "        x = x + self.res2(x)\n",
    "        return torch.tanh(self.out(x))\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(bert_dim + cond_dim, hidden_dim)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            spectral_norm(nn.Linear(hidden_dim, hidden_dim // 2)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            spectral_norm(nn.Linear(hidden_dim // 2, 1))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples, labels):\n",
    "    alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    class_onehot = F.one_hot(labels, num_classes=cond_dim).float().to(device)\n",
    "    disc_input_interpolates = torch.cat((interpolates, class_onehot), dim=1)\n",
    "    d_interpolates = D(disc_input_interpolates)\n",
    "    fake = torch.ones(real_samples.size(0), 1).to(device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates, inputs=interpolates, grad_outputs=fake,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# )  \n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "g_optimizer = optim.AdamW(generator.parameters(), lr=lr_g, betas=(0.5, 0.9))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.9))\n",
    "scheduler_g = ReduceLROnPlateau(g_optimizer, 'min', patience=100, factor=0.5)\n",
    "\n",
    "d_losses, g_losses = [], []\n",
    "best_g_loss = float('inf')\n",
    "patience, early_stop_counter = 100, 0\n",
    "\n",
    "for epoch in range(num_epochs_gan):\n",
    "    epoch_g_losses = []\n",
    "    for i, (real_bert, labels) in enumerate(train_loader):\n",
    "        batch_size = real_bert.size(0)\n",
    "        real_bert, labels = real_bert.to(device), labels.to(device)\n",
    "\n",
    "        #  Discriminator\n",
    "        for _ in range(n_critic):\n",
    "            d_optimizer.zero_grad()\n",
    "            labels_batch = labels[:real_bert.size(0)]  #   real_bert\n",
    "            class_onehot = F.one_hot(labels_batch, num_classes=cond_dim).float().to(device)\n",
    "            disc_input_real = torch.cat((real_bert, class_onehot), dim=1)\n",
    "            real_validity = discriminator(disc_input_real)\n",
    "            z = torch.randn(real_bert.size(0), z_dim).to(device)\n",
    "            fake_bert = generator(z, labels_batch)\n",
    "            disc_input_fake = torch.cat((fake_bert.detach(), class_onehot), dim=1)\n",
    "            fake_validity = discriminator(disc_input_fake)\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_bert, fake_bert.detach(), labels_batch)\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "        #  Generator\n",
    "        g_optimizer.zero_grad()\n",
    "        labels_batch = labels[:real_bert.size(0)]\n",
    "        class_onehot = F.one_hot(labels_batch, num_classes=cond_dim).float().to(device)\n",
    "        z = torch.randn(real_bert.size(0), z_dim).to(device)\n",
    "        fake_bert = generator(z, labels_batch)\n",
    "        disc_input_fake = torch.cat((fake_bert, class_onehot), dim=1)\n",
    "        fake_validity = discriminator(disc_input_fake)\n",
    "        cos_sim = nn.CosineSimilarity(dim=1)\n",
    "        sim_loss = 1 - cos_sim(fake_bert, real_bert[:real_bert.size(0)]).mean()\n",
    "        g_loss = -torch.mean(fake_validity) + sim_weight * sim_loss\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        d_losses.append(d_loss.item())\n",
    "        g_losses.append(g_loss.item())\n",
    "        epoch_g_losses.append(g_loss.item())\n",
    "\n",
    "        if (i + 1) % 5 == 0 or i == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs_gan}], Step [{i+1}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    scheduler_g.step(np.mean(epoch_g_losses))\n",
    "\n",
    "    if np.mean(epoch_g_losses) < best_g_loss:\n",
    "        best_g_loss = np.mean(epoch_g_losses)\n",
    "        early_stop_counter = 0\n",
    "        torch.save(generator.state_dict(), f\"{output_dir}gan_generator_attention.pt\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "#   \n",
    "generator.load_state_dict(torch.load(f\"{output_dir}gan_generator_attention.pt\"))\n",
    "generator.eval()\n",
    "\n",
    "#       \n",
    "generated_data = {}\n",
    "for cls in rare_classes:\n",
    "    print(f\"\\nGenerating synthetic samples for Class {cls}...\")\n",
    "    z = torch.randn(num_samples_per_class, z_dim).to(device)\n",
    "    labels_c = torch.full((num_samples_per_class,), cls, dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        fake_bert = generator(z, labels_c)\n",
    "    generated_data[cls] = fake_bert.cpu().numpy()\n",
    "    np.save(f\"{output_dir}generated_class_{cls}.npy\", fake_bert.cpu().numpy())\n",
    "\n",
    "#  t-SNE  Silhouette\n",
    "for cls in rare_classes:\n",
    "    print(f\"\\nEvaluating synthetic data for Class {cls}...\")\n",
    "    real_class_idx = np.where(labels_initial == cls)[0][:500]\n",
    "    real_samples = X_bert[real_class_idx]\n",
    "    fake_samples = generated_data.get(cls, np.zeros((500, bert_dim)))[:500]\n",
    "\n",
    "    min_len = min(len(real_samples), len(fake_samples))\n",
    "    real_samples = real_samples[:min_len]\n",
    "    fake_samples = fake_samples[:min_len]\n",
    "    X_vis = np.concatenate([real_samples, fake_samples])\n",
    "    labels_vis = np.concatenate([np.zeros(min_len), np.ones(min_len)])\n",
    "\n",
    "    X_tsne = TSNE(n_components=2, random_state=42, n_jobs=-1).fit_transform(X_vis)\n",
    "    plt.figure()\n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels_vis, palette=[\"blue\", \"red\"], alpha=0.5)\n",
    "    plt.title(f\"t-SNE: Real vs Generated Data for Class {cls}\")\n",
    "    plt.savefig(f\"{output_dir}t-SNE_class_{cls}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    silhouette_avg = silhouette_score(X_vis, labels_vis)\n",
    "    print(f\"Silhouette Score for Class {cls}: {silhouette_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6379d4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_bert_train: (8935, 768)\n",
      "Shape of X_handcrafted_train: (8935, 490)\n",
      "Shape of X_train_dual: (8935, 1258), dual_dim: 1258\n",
      "Shape of X_train_pca: (8935, 512)\n",
      "Rare classes identified: [2, 3, 4] with counts: [252 853 511]\n",
      "Epoch [1/30], Step [1], D Loss: 5.7015, G Loss: 0.0961, Grad norm G: 2.4276, Grad norm D: 11.1357\n",
      "Epoch [1/30], Step [5], D Loss: 5.3089, G Loss: -0.4708, Grad norm G: 2.8066, Grad norm D: 9.2777\n",
      "Epoch [1/30], Step [10], D Loss: 4.7086, G Loss: -0.6403, Grad norm G: 3.2278, Grad norm D: 7.8916\n",
      "Epoch [1/30], Step [15], D Loss: 3.4936, G Loss: 0.1192, Grad norm G: 2.5751, Grad norm D: 8.3166\n",
      "Epoch [1/30], Step [20], D Loss: 3.4598, G Loss: 0.3925, Grad norm G: 2.5058, Grad norm D: 8.1565\n",
      "Epoch [1/30], Step [25], D Loss: 2.2182, G Loss: 0.5235, Grad norm G: 3.2109, Grad norm D: 7.9312\n",
      "Epoch [1/30], Step [30], D Loss: 1.4801, G Loss: 0.6953, Grad norm G: 3.4236, Grad norm D: 8.8393\n",
      "Epoch [1/30], Step [35], D Loss: 2.0730, G Loss: 0.2519, Grad norm G: 3.7987, Grad norm D: 6.6671\n",
      "Epoch [1/30], Step [40], D Loss: 1.8748, G Loss: -0.0674, Grad norm G: 3.9424, Grad norm D: 8.0269\n",
      "Epoch [1/30], Step [45], D Loss: 2.1375, G Loss: -0.1670, Grad norm G: 4.2285, Grad norm D: 9.5512\n",
      "Epoch [1/30], Step [50], D Loss: 2.4752, G Loss: -0.8268, Grad norm G: 4.6628, Grad norm D: 7.4034\n",
      "Epoch [1/30], Step [55], D Loss: 1.3223, G Loss: -0.5865, Grad norm G: 4.3292, Grad norm D: 8.8335\n",
      "Epoch [1/30], Step [60], D Loss: 2.4601, G Loss: 0.3157, Grad norm G: 2.4180, Grad norm D: 9.3663\n",
      "Epoch [1/30], Step [65], D Loss: 0.4259, G Loss: 0.5753, Grad norm G: 3.7730, Grad norm D: 11.3395\n",
      "Epoch [1/30], Step [70], D Loss: 0.9599, G Loss: 0.3646, Grad norm G: 3.6792, Grad norm D: 7.3145\n",
      "Epoch [1/30], Step [75], D Loss: 2.3132, G Loss: 0.3064, Grad norm G: 3.4888, Grad norm D: 7.1881\n",
      "Epoch [1/30], Step [80], D Loss: 1.0447, G Loss: 0.5173, Grad norm G: 3.2149, Grad norm D: 6.4069\n",
      "Epoch [1/30], Step [85], D Loss: 1.7778, G Loss: 0.5843, Grad norm G: 3.4432, Grad norm D: 9.4588\n",
      "Epoch [1/30], Step [90], D Loss: 1.0461, G Loss: -0.2294, Grad norm G: 3.5682, Grad norm D: 9.6901\n",
      "Epoch [1/30], Step [95], D Loss: 1.4188, G Loss: -0.3611, Grad norm G: 3.2325, Grad norm D: 7.9014\n",
      "Epoch [1/30], Step [100], D Loss: 2.0298, G Loss: -0.7585, Grad norm G: 4.0234, Grad norm D: 7.1703\n",
      "Epoch [1/30], Step [105], D Loss: 1.3670, G Loss: -0.8192, Grad norm G: 4.0294, Grad norm D: 7.1701\n",
      "Epoch [1/30], Step [110], D Loss: 0.9700, G Loss: -0.6898, Grad norm G: 4.2162, Grad norm D: 8.5143\n",
      "Epoch [1/30], Step [115], D Loss: 1.5194, G Loss: -0.7749, Grad norm G: 4.2912, Grad norm D: 6.7549\n",
      "Epoch [1/30], Step [120], D Loss: 0.5110, G Loss: -0.4441, Grad norm G: 4.0394, Grad norm D: 6.9405\n",
      "Epoch [1/30], Step [125], D Loss: 0.1691, G Loss: 0.0384, Grad norm G: 3.8961, Grad norm D: 8.8043\n",
      "Epoch [1/30], Step [130], D Loss: -0.5503, G Loss: 0.0031, Grad norm G: 4.2253, Grad norm D: 8.4222\n",
      "Epoch [1/30], Step [135], D Loss: 0.5596, G Loss: -0.1961, Grad norm G: 4.5167, Grad norm D: 7.8605\n",
      "Epoch [2/30], Step [1], D Loss: 0.7628, G Loss: -0.3763, Grad norm G: 4.7673, Grad norm D: 4.9119\n",
      "Epoch [2/30], Step [5], D Loss: 0.2996, G Loss: -0.1860, Grad norm G: 4.5281, Grad norm D: 5.3938\n",
      "Epoch [2/30], Step [10], D Loss: 0.1084, G Loss: -0.1094, Grad norm G: 4.4397, Grad norm D: 7.1496\n",
      "Epoch [2/30], Step [15], D Loss: 1.1640, G Loss: -0.5313, Grad norm G: 4.4007, Grad norm D: 7.0166\n",
      "Epoch [2/30], Step [20], D Loss: 1.1841, G Loss: -0.8977, Grad norm G: 4.2516, Grad norm D: 5.3701\n",
      "Epoch [2/30], Step [25], D Loss: 1.0591, G Loss: -0.8956, Grad norm G: 3.8830, Grad norm D: 7.2649\n",
      "Epoch [2/30], Step [30], D Loss: 1.6789, G Loss: -1.2088, Grad norm G: 4.4284, Grad norm D: 5.1994\n",
      "Epoch [2/30], Step [35], D Loss: 1.3978, G Loss: -1.3195, Grad norm G: 4.4317, Grad norm D: 5.1397\n",
      "Epoch [2/30], Step [40], D Loss: 0.9313, G Loss: -1.3117, Grad norm G: 4.0383, Grad norm D: 7.9261\n",
      "Epoch [2/30], Step [45], D Loss: 0.5668, G Loss: -1.6129, Grad norm G: 5.3697, Grad norm D: 6.8140\n",
      "Epoch [2/30], Step [50], D Loss: 0.9602, G Loss: -1.4959, Grad norm G: 4.6284, Grad norm D: 5.0339\n",
      "Epoch [2/30], Step [55], D Loss: 0.9014, G Loss: -1.3531, Grad norm G: 4.3229, Grad norm D: 5.6840\n",
      "Epoch [2/30], Step [60], D Loss: 0.7327, G Loss: -1.1412, Grad norm G: 4.3451, Grad norm D: 6.7041\n",
      "Epoch [2/30], Step [65], D Loss: 0.4764, G Loss: -1.6106, Grad norm G: 4.8136, Grad norm D: 5.6465\n",
      "Epoch [2/30], Step [70], D Loss: 0.8121, G Loss: -1.2897, Grad norm G: 4.2388, Grad norm D: 5.5321\n",
      "Epoch [2/30], Step [75], D Loss: 0.8648, G Loss: -1.0539, Grad norm G: 4.2102, Grad norm D: 4.5347\n",
      "Epoch [2/30], Step [80], D Loss: 0.8153, G Loss: -1.4069, Grad norm G: 4.5608, Grad norm D: 5.1421\n",
      "Epoch [2/30], Step [85], D Loss: 0.9573, G Loss: -1.3129, Grad norm G: 4.4230, Grad norm D: 5.4367\n",
      "Epoch [2/30], Step [90], D Loss: 0.6845, G Loss: -1.1333, Grad norm G: 3.9908, Grad norm D: 5.8339\n",
      "Epoch [2/30], Step [95], D Loss: 0.8334, G Loss: -1.1059, Grad norm G: 4.2604, Grad norm D: 4.1848\n",
      "Epoch [2/30], Step [100], D Loss: 0.9716, G Loss: -1.5534, Grad norm G: 4.6877, Grad norm D: 4.4188\n",
      "Epoch [2/30], Step [105], D Loss: 0.8229, G Loss: -1.6730, Grad norm G: 4.5801, Grad norm D: 5.1064\n",
      "Epoch [2/30], Step [110], D Loss: 0.2584, G Loss: -1.7075, Grad norm G: 4.5953, Grad norm D: 5.9205\n",
      "Epoch [2/30], Step [115], D Loss: 1.2098, G Loss: -1.3601, Grad norm G: 4.3905, Grad norm D: 5.2568\n",
      "Epoch [2/30], Step [120], D Loss: 1.1643, G Loss: -1.5707, Grad norm G: 4.8538, Grad norm D: 3.9808\n",
      "Epoch [2/30], Step [125], D Loss: 1.4316, G Loss: -1.7702, Grad norm G: 5.2495, Grad norm D: 4.2183\n",
      "Epoch [2/30], Step [130], D Loss: 1.2459, G Loss: -1.5495, Grad norm G: 5.2270, Grad norm D: 4.7798\n",
      "Epoch [2/30], Step [135], D Loss: 0.8534, G Loss: -1.0723, Grad norm G: 4.9838, Grad norm D: 5.8999\n",
      "Epoch [3/30], Step [1], D Loss: 0.8201, G Loss: -0.6786, Grad norm G: 4.8033, Grad norm D: 6.7352\n",
      "Epoch [3/30], Step [5], D Loss: 0.8642, G Loss: -0.6526, Grad norm G: 5.1150, Grad norm D: 5.0723\n",
      "Epoch [3/30], Step [10], D Loss: 1.2746, G Loss: -0.7122, Grad norm G: 5.2696, Grad norm D: 5.1076\n",
      "Epoch [3/30], Step [15], D Loss: 1.5563, G Loss: -0.9425, Grad norm G: 4.9435, Grad norm D: 5.0013\n",
      "Epoch [3/30], Step [20], D Loss: 1.4673, G Loss: -0.6168, Grad norm G: 4.1813, Grad norm D: 5.1924\n",
      "Epoch [3/30], Step [25], D Loss: 0.7870, G Loss: -0.4418, Grad norm G: 4.1228, Grad norm D: 5.0233\n",
      "Epoch [3/30], Step [30], D Loss: 1.0612, G Loss: -0.0216, Grad norm G: 3.3941, Grad norm D: 4.9624\n",
      "Epoch [3/30], Step [35], D Loss: 0.6787, G Loss: -0.3766, Grad norm G: 4.3969, Grad norm D: 4.5550\n",
      "Epoch [3/30], Step [40], D Loss: 0.7796, G Loss: -0.7602, Grad norm G: 4.8050, Grad norm D: 4.7624\n",
      "Epoch [3/30], Step [45], D Loss: 1.2308, G Loss: -0.8502, Grad norm G: 5.2086, Grad norm D: 5.1976\n",
      "Epoch [3/30], Step [50], D Loss: 1.2938, G Loss: -1.0153, Grad norm G: 5.3769, Grad norm D: 5.2711\n",
      "Epoch [3/30], Step [55], D Loss: 0.9678, G Loss: -1.0247, Grad norm G: 5.2734, Grad norm D: 5.0978\n",
      "Epoch [3/30], Step [60], D Loss: 1.4894, G Loss: -1.2093, Grad norm G: 5.1064, Grad norm D: 4.3846\n",
      "Epoch [3/30], Step [65], D Loss: 1.2335, G Loss: -0.9855, Grad norm G: 5.1616, Grad norm D: 4.7144\n",
      "Epoch [3/30], Step [70], D Loss: 0.8747, G Loss: -0.5346, Grad norm G: 5.2364, Grad norm D: 5.1869\n",
      "Epoch [3/30], Step [75], D Loss: 0.9618, G Loss: -0.1015, Grad norm G: 4.9283, Grad norm D: 5.1677\n",
      "Epoch [3/30], Step [80], D Loss: 0.7359, G Loss: -0.1780, Grad norm G: 5.0599, Grad norm D: 5.6769\n",
      "Epoch [3/30], Step [85], D Loss: 0.6992, G Loss: -0.1973, Grad norm G: 4.7826, Grad norm D: 5.0101\n",
      "Epoch [3/30], Step [90], D Loss: 0.8218, G Loss: -0.1379, Grad norm G: 4.7224, Grad norm D: 4.6629\n",
      "Epoch [3/30], Step [95], D Loss: 0.3963, G Loss: 0.1692, Grad norm G: 4.9750, Grad norm D: 6.4715\n",
      "Epoch [3/30], Step [100], D Loss: 1.0382, G Loss: -0.1032, Grad norm G: 4.9443, Grad norm D: 4.1358\n",
      "Epoch [3/30], Step [105], D Loss: 1.0977, G Loss: -0.3431, Grad norm G: 5.3744, Grad norm D: 4.2922\n",
      "Epoch [3/30], Step [110], D Loss: 1.5253, G Loss: -0.4255, Grad norm G: 5.4633, Grad norm D: 4.5493\n",
      "Epoch [3/30], Step [115], D Loss: 1.1907, G Loss: -0.1848, Grad norm G: 5.4169, Grad norm D: 5.1031\n",
      "Epoch [3/30], Step [120], D Loss: 0.8678, G Loss: 0.3443, Grad norm G: 5.3481, Grad norm D: 5.5612\n",
      "Epoch [3/30], Step [125], D Loss: 0.9521, G Loss: 0.4873, Grad norm G: 5.4260, Grad norm D: 5.7395\n",
      "Epoch [3/30], Step [130], D Loss: 1.1980, G Loss: 0.2534, Grad norm G: 4.9550, Grad norm D: 4.8118\n",
      "Epoch [3/30], Step [135], D Loss: 1.1997, G Loss: 0.4159, Grad norm G: 4.8609, Grad norm D: 4.2921\n",
      "Epoch [4/30], Step [1], D Loss: 1.0978, G Loss: 0.5878, Grad norm G: 4.7390, Grad norm D: 4.6423\n",
      "Epoch [4/30], Step [5], D Loss: 0.8880, G Loss: 0.9574, Grad norm G: 4.6708, Grad norm D: 4.8738\n",
      "Epoch [4/30], Step [10], D Loss: 0.6484, G Loss: 1.1492, Grad norm G: 4.9477, Grad norm D: 4.8138\n",
      "Epoch [4/30], Step [15], D Loss: 0.6151, G Loss: 0.9815, Grad norm G: 5.8948, Grad norm D: 4.8983\n",
      "Epoch [4/30], Step [20], D Loss: 1.5254, G Loss: 0.3220, Grad norm G: 5.7348, Grad norm D: 3.9766\n",
      "Epoch [4/30], Step [25], D Loss: 1.7572, G Loss: 0.0763, Grad norm G: 5.4614, Grad norm D: 4.1235\n",
      "Epoch [4/30], Step [30], D Loss: 1.8537, G Loss: 0.2982, Grad norm G: 5.2682, Grad norm D: 4.0458\n",
      "Epoch [4/30], Step [35], D Loss: 1.6628, G Loss: 0.8172, Grad norm G: 5.0782, Grad norm D: 4.3101\n",
      "Epoch [4/30], Step [40], D Loss: 1.1273, G Loss: 0.9627, Grad norm G: 5.5840, Grad norm D: 5.3364\n",
      "Epoch [4/30], Step [45], D Loss: 1.8529, G Loss: 0.6985, Grad norm G: 5.3872, Grad norm D: 4.2591\n",
      "Epoch [4/30], Step [50], D Loss: 1.4220, G Loss: 0.6486, Grad norm G: 5.5553, Grad norm D: 4.1666\n",
      "Epoch [4/30], Step [55], D Loss: 1.5374, G Loss: 0.6484, Grad norm G: 5.5845, Grad norm D: 4.3560\n",
      "Epoch [4/30], Step [60], D Loss: 1.3730, G Loss: 0.7022, Grad norm G: 5.5219, Grad norm D: 4.3896\n",
      "Epoch [4/30], Step [65], D Loss: 1.1903, G Loss: 1.0897, Grad norm G: 5.3059, Grad norm D: 4.5653\n",
      "Epoch [4/30], Step [70], D Loss: 1.1150, G Loss: 1.2951, Grad norm G: 5.1198, Grad norm D: 4.6425\n",
      "Epoch [4/30], Step [75], D Loss: 1.0095, G Loss: 1.1894, Grad norm G: 5.1003, Grad norm D: 4.6539\n",
      "Epoch [4/30], Step [80], D Loss: 1.2080, G Loss: 1.1329, Grad norm G: 4.9670, Grad norm D: 4.7136\n",
      "Epoch [4/30], Step [85], D Loss: 1.3135, G Loss: 1.0411, Grad norm G: 4.9592, Grad norm D: 4.6528\n",
      "Epoch [4/30], Step [90], D Loss: 1.5894, G Loss: 0.7644, Grad norm G: 4.8604, Grad norm D: 4.2073\n",
      "Epoch [4/30], Step [95], D Loss: 1.8333, G Loss: 0.2594, Grad norm G: 4.8229, Grad norm D: 4.0534\n",
      "Epoch [4/30], Step [100], D Loss: 2.5131, G Loss: -0.2379, Grad norm G: 4.6104, Grad norm D: 3.8199\n",
      "Epoch [4/30], Step [105], D Loss: 2.4510, G Loss: -0.4336, Grad norm G: 4.3501, Grad norm D: 4.1944\n",
      "Epoch [4/30], Step [110], D Loss: 2.4618, G Loss: -0.5155, Grad norm G: 4.3429, Grad norm D: 4.3614\n",
      "Epoch [4/30], Step [115], D Loss: 2.0812, G Loss: -0.4661, Grad norm G: 4.3132, Grad norm D: 4.7230\n",
      "Epoch [4/30], Step [120], D Loss: 2.2715, G Loss: -0.6567, Grad norm G: 4.6000, Grad norm D: 4.6933\n",
      "Epoch [4/30], Step [125], D Loss: 2.3531, G Loss: -0.8471, Grad norm G: 4.7550, Grad norm D: 4.5627\n",
      "Epoch [4/30], Step [130], D Loss: 2.0538, G Loss: -0.7533, Grad norm G: 4.7911, Grad norm D: 4.9529\n",
      "Epoch [4/30], Step [135], D Loss: 1.8220, G Loss: -0.5102, Grad norm G: 4.7954, Grad norm D: 4.9807\n",
      "Epoch [5/30], Step [1], D Loss: 1.3143, G Loss: -0.1417, Grad norm G: 4.8501, Grad norm D: 4.9692\n",
      "Epoch [5/30], Step [5], D Loss: 1.3441, G Loss: 0.1741, Grad norm G: 4.7456, Grad norm D: 4.8484\n",
      "Epoch [5/30], Step [10], D Loss: 1.1861, G Loss: 0.2878, Grad norm G: 4.6334, Grad norm D: 4.5715\n",
      "Epoch [5/30], Step [15], D Loss: 1.1120, G Loss: 0.3511, Grad norm G: 4.4916, Grad norm D: 4.3717\n",
      "Epoch [5/30], Step [20], D Loss: 1.1329, G Loss: 0.1781, Grad norm G: 4.4789, Grad norm D: 4.2346\n",
      "Epoch [5/30], Step [25], D Loss: 1.5541, G Loss: 0.0202, Grad norm G: 4.2581, Grad norm D: 4.1390\n",
      "Epoch [5/30], Step [30], D Loss: 1.7054, G Loss: -0.3988, Grad norm G: 4.1602, Grad norm D: 3.8134\n",
      "Epoch [5/30], Step [35], D Loss: 2.1452, G Loss: -0.9132, Grad norm G: 3.9633, Grad norm D: 4.3595\n",
      "Epoch [5/30], Step [40], D Loss: 2.6811, G Loss: -1.2241, Grad norm G: 3.5777, Grad norm D: 4.5596\n",
      "Epoch [5/30], Step [45], D Loss: 2.7037, G Loss: -1.5866, Grad norm G: 3.8923, Grad norm D: 4.4586\n",
      "Epoch [5/30], Step [50], D Loss: 2.9211, G Loss: -1.7088, Grad norm G: 3.8469, Grad norm D: 4.4207\n",
      "Epoch [5/30], Step [55], D Loss: 2.2395, G Loss: -1.7788, Grad norm G: 4.0507, Grad norm D: 5.0955\n",
      "Epoch [5/30], Step [60], D Loss: 1.7564, G Loss: -1.6207, Grad norm G: 4.1030, Grad norm D: 5.2044\n",
      "Epoch [5/30], Step [65], D Loss: 1.9032, G Loss: -1.7021, Grad norm G: 4.4250, Grad norm D: 5.0187\n",
      "Epoch [5/30], Step [70], D Loss: 1.8871, G Loss: -1.6540, Grad norm G: 4.5061, Grad norm D: 4.3006\n",
      "Epoch [5/30], Step [75], D Loss: 1.8249, G Loss: -1.6337, Grad norm G: 4.5695, Grad norm D: 4.2313\n",
      "Epoch [5/30], Step [80], D Loss: 1.3769, G Loss: -1.4204, Grad norm G: 4.5783, Grad norm D: 4.4307\n",
      "Epoch [5/30], Step [85], D Loss: 0.8131, G Loss: -0.9185, Grad norm G: 4.4720, Grad norm D: 4.6271\n",
      "Epoch [5/30], Step [90], D Loss: -0.1183, G Loss: -0.2680, Grad norm G: 4.6603, Grad norm D: 5.4638\n",
      "Epoch [5/30], Step [95], D Loss: 0.2410, G Loss: -0.2742, Grad norm G: 4.6181, Grad norm D: 3.9620\n",
      "Epoch [5/30], Step [100], D Loss: 0.6983, G Loss: -0.6723, Grad norm G: 5.3261, Grad norm D: 3.8308\n",
      "Epoch [5/30], Step [105], D Loss: 1.1603, G Loss: -0.9240, Grad norm G: 5.2930, Grad norm D: 3.6962\n",
      "Epoch [5/30], Step [110], D Loss: 1.4228, G Loss: -1.1605, Grad norm G: 4.8858, Grad norm D: 3.9517\n",
      "Epoch [5/30], Step [115], D Loss: 1.4242, G Loss: -1.1674, Grad norm G: 4.4985, Grad norm D: 4.2596\n",
      "Epoch [5/30], Step [120], D Loss: 1.5589, G Loss: -1.2816, Grad norm G: 4.6229, Grad norm D: 3.5889\n",
      "Epoch [5/30], Step [125], D Loss: 1.4351, G Loss: -1.4110, Grad norm G: 4.6760, Grad norm D: 3.6814\n",
      "Epoch [5/30], Step [130], D Loss: 1.4242, G Loss: -1.3746, Grad norm G: 4.7398, Grad norm D: 3.8573\n",
      "Epoch [5/30], Step [135], D Loss: 1.2785, G Loss: -1.1012, Grad norm G: 4.2802, Grad norm D: 3.8991\n",
      "Epoch [6/30], Step [1], D Loss: 0.8339, G Loss: -0.6156, Grad norm G: 4.1637, Grad norm D: 3.8204\n",
      "Epoch [6/30], Step [5], D Loss: 0.4664, G Loss: -0.3668, Grad norm G: 4.1493, Grad norm D: 3.7545\n",
      "Epoch [6/30], Step [10], D Loss: 0.1845, G Loss: -0.2834, Grad norm G: 4.4788, Grad norm D: 3.7245\n",
      "Epoch [6/30], Step [15], D Loss: 0.5090, G Loss: -0.4436, Grad norm G: 4.9267, Grad norm D: 3.7879\n",
      "Epoch [6/30], Step [20], D Loss: 0.9233, G Loss: -0.7848, Grad norm G: 5.1761, Grad norm D: 3.7193\n",
      "Epoch [6/30], Step [25], D Loss: 1.1068, G Loss: -0.9258, Grad norm G: 5.2713, Grad norm D: 3.5897\n",
      "Epoch [6/30], Step [30], D Loss: 1.2404, G Loss: -0.9845, Grad norm G: 5.0183, Grad norm D: 3.5840\n",
      "Epoch [6/30], Step [35], D Loss: 1.4101, G Loss: -0.9610, Grad norm G: 5.0831, Grad norm D: 3.6462\n",
      "Epoch [6/30], Step [40], D Loss: 1.2206, G Loss: -0.8993, Grad norm G: 4.8988, Grad norm D: 3.8070\n",
      "Epoch [6/30], Step [45], D Loss: 1.3127, G Loss: -0.8371, Grad norm G: 4.9095, Grad norm D: 3.8068\n",
      "Epoch [6/30], Step [50], D Loss: 0.6838, G Loss: -0.6471, Grad norm G: 4.8369, Grad norm D: 4.0606\n",
      "Epoch [6/30], Step [55], D Loss: 0.4999, G Loss: -0.3043, Grad norm G: 4.7123, Grad norm D: 3.9642\n",
      "Epoch [6/30], Step [60], D Loss: 0.0926, G Loss: 0.1267, Grad norm G: 4.4781, Grad norm D: 4.1980\n",
      "Epoch [6/30], Step [65], D Loss: 0.1113, G Loss: 0.2060, Grad norm G: 4.7395, Grad norm D: 3.8228\n",
      "Epoch [6/30], Step [70], D Loss: 0.2773, G Loss: 0.1140, Grad norm G: 4.9931, Grad norm D: 3.6986\n",
      "Epoch [6/30], Step [75], D Loss: 0.3613, G Loss: -0.2052, Grad norm G: 5.0871, Grad norm D: 3.5940\n",
      "Epoch [6/30], Step [80], D Loss: 0.6542, G Loss: -0.2604, Grad norm G: 4.9257, Grad norm D: 3.6078\n",
      "Epoch [6/30], Step [85], D Loss: 0.6161, G Loss: -0.4301, Grad norm G: 4.6996, Grad norm D: 4.0059\n",
      "Epoch [6/30], Step [90], D Loss: 0.8892, G Loss: -0.6706, Grad norm G: 4.8656, Grad norm D: 3.4321\n",
      "Epoch [6/30], Step [95], D Loss: 0.9721, G Loss: -1.0102, Grad norm G: 5.1109, Grad norm D: 3.5154\n",
      "Epoch [6/30], Step [100], D Loss: 1.3882, G Loss: -1.2904, Grad norm G: 5.1636, Grad norm D: 3.7871\n",
      "Epoch [6/30], Step [105], D Loss: 0.9982, G Loss: -1.0809, Grad norm G: 4.9933, Grad norm D: 4.1506\n",
      "Epoch [6/30], Step [110], D Loss: 0.6540, G Loss: -0.5428, Grad norm G: 4.8187, Grad norm D: 4.8551\n",
      "Epoch [6/30], Step [115], D Loss: 0.3664, G Loss: -0.2228, Grad norm G: 4.8048, Grad norm D: 4.0460\n",
      "Epoch [6/30], Step [120], D Loss: 0.5575, G Loss: -0.5622, Grad norm G: 5.3051, Grad norm D: 3.6758\n",
      "Epoch [6/30], Step [125], D Loss: 0.6700, G Loss: -0.6805, Grad norm G: 5.5241, Grad norm D: 3.7865\n",
      "Epoch [6/30], Step [130], D Loss: 0.5357, G Loss: -0.4486, Grad norm G: 5.3389, Grad norm D: 3.9807\n",
      "Epoch [6/30], Step [135], D Loss: 0.0117, G Loss: 0.0403, Grad norm G: 5.1121, Grad norm D: 4.1156\n",
      "Epoch [7/30], Step [1], D Loss: 0.3638, G Loss: 0.0903, Grad norm G: 5.0246, Grad norm D: 3.8783\n",
      "Epoch [7/30], Step [5], D Loss: 0.5195, G Loss: -0.2716, Grad norm G: 5.3334, Grad norm D: 3.6179\n",
      "Epoch [7/30], Step [10], D Loss: 0.9212, G Loss: -0.5729, Grad norm G: 5.2679, Grad norm D: 3.6182\n",
      "Epoch [7/30], Step [15], D Loss: 0.7321, G Loss: -0.3563, Grad norm G: 4.9320, Grad norm D: 4.0804\n",
      "Epoch [7/30], Step [20], D Loss: 0.4570, G Loss: 0.0327, Grad norm G: 4.6086, Grad norm D: 4.7844\n",
      "Epoch [7/30], Step [25], D Loss: 0.3786, G Loss: -0.1928, Grad norm G: 4.9214, Grad norm D: 3.6476\n",
      "Epoch [7/30], Step [30], D Loss: 0.6532, G Loss: -0.5293, Grad norm G: 5.2447, Grad norm D: 3.6440\n",
      "Epoch [7/30], Step [35], D Loss: 0.3843, G Loss: -0.4345, Grad norm G: 5.0465, Grad norm D: 4.2074\n",
      "Epoch [7/30], Step [40], D Loss: -0.1952, G Loss: 0.0517, Grad norm G: 4.8298, Grad norm D: 5.2141\n",
      "Epoch [7/30], Step [45], D Loss: 0.0263, G Loss: 0.1188, Grad norm G: 4.8701, Grad norm D: 3.6412\n",
      "Epoch [7/30], Step [50], D Loss: -0.0891, G Loss: -0.0969, Grad norm G: 5.1542, Grad norm D: 3.7737\n",
      "Epoch [7/30], Step [55], D Loss: 0.1879, G Loss: -0.0217, Grad norm G: 5.0383, Grad norm D: 4.2160\n",
      "Epoch [7/30], Step [60], D Loss: 0.0023, G Loss: -0.0372, Grad norm G: 5.1607, Grad norm D: 5.5063\n",
      "Epoch [7/30], Step [65], D Loss: 0.5423, G Loss: -0.3552, Grad norm G: 5.2215, Grad norm D: 3.5535\n",
      "Epoch [7/30], Step [70], D Loss: 0.4755, G Loss: -0.5727, Grad norm G: 5.2744, Grad norm D: 3.6387\n",
      "Epoch [7/30], Step [75], D Loss: 0.5519, G Loss: -0.6418, Grad norm G: 5.1488, Grad norm D: 4.0591\n",
      "Epoch [7/30], Step [80], D Loss: -0.1209, G Loss: 0.1154, Grad norm G: 4.9688, Grad norm D: 5.8820\n",
      "Epoch [7/30], Step [85], D Loss: -0.0308, G Loss: 0.0634, Grad norm G: 4.8947, Grad norm D: 3.5115\n",
      "Epoch [7/30], Step [90], D Loss: 0.2496, G Loss: -0.2661, Grad norm G: 5.4097, Grad norm D: 3.3966\n",
      "Epoch [7/30], Step [95], D Loss: 0.6822, G Loss: -0.7207, Grad norm G: 5.5646, Grad norm D: 3.6567\n",
      "Epoch [7/30], Step [100], D Loss: 0.4114, G Loss: -0.4663, Grad norm G: 5.2608, Grad norm D: 4.3366\n",
      "Epoch [7/30], Step [105], D Loss: -0.6770, G Loss: 0.3262, Grad norm G: 4.8005, Grad norm D: 4.8372\n",
      "Epoch [7/30], Step [110], D Loss: -0.3852, G Loss: 0.3774, Grad norm G: 5.0466, Grad norm D: 3.5266\n",
      "Epoch [7/30], Step [115], D Loss: 0.2237, G Loss: 0.0792, Grad norm G: 5.5191, Grad norm D: 3.4327\n",
      "Epoch [7/30], Step [120], D Loss: 0.3195, G Loss: -0.1455, Grad norm G: 5.3867, Grad norm D: 3.7728\n",
      "Epoch [7/30], Step [125], D Loss: -0.0716, G Loss: 0.4140, Grad norm G: 5.3106, Grad norm D: 5.0324\n",
      "Epoch [7/30], Step [130], D Loss: 0.3502, G Loss: 0.0595, Grad norm G: 5.0005, Grad norm D: 3.5243\n",
      "Epoch [7/30], Step [135], D Loss: 0.8048, G Loss: -0.5117, Grad norm G: 5.2870, Grad norm D: 3.3263\n",
      "Epoch [8/30], Step [1], D Loss: 0.9091, G Loss: -0.7839, Grad norm G: 5.1390, Grad norm D: 3.7478\n",
      "Epoch [8/30], Step [5], D Loss: 0.2599, G Loss: -0.5002, Grad norm G: 4.9725, Grad norm D: 4.2496\n",
      "Epoch [8/30], Step [10], D Loss: -0.5867, G Loss: 0.1498, Grad norm G: 5.1478, Grad norm D: 6.2572\n",
      "Epoch [8/30], Step [15], D Loss: 0.2826, G Loss: -0.4013, Grad norm G: 5.1845, Grad norm D: 3.3280\n",
      "Epoch [8/30], Step [20], D Loss: 0.5663, G Loss: -0.8033, Grad norm G: 5.4473, Grad norm D: 3.5674\n",
      "Epoch [8/30], Step [25], D Loss: 0.3249, G Loss: -0.7457, Grad norm G: 5.2231, Grad norm D: 4.3577\n",
      "Epoch [8/30], Step [30], D Loss: -0.2016, G Loss: -0.0109, Grad norm G: 4.6614, Grad norm D: 4.3453\n",
      "Epoch [8/30], Step [35], D Loss: -0.2855, G Loss: 0.3544, Grad norm G: 4.6801, Grad norm D: 3.8560\n",
      "Epoch [8/30], Step [40], D Loss: 0.0592, G Loss: -0.0065, Grad norm G: 5.2647, Grad norm D: 3.4657\n",
      "Epoch [8/30], Step [45], D Loss: 0.2115, G Loss: -0.2718, Grad norm G: 5.3704, Grad norm D: 3.7575\n",
      "Epoch [8/30], Step [50], D Loss: 0.1367, G Loss: -0.0304, Grad norm G: 5.4174, Grad norm D: 5.0032\n",
      "Epoch [8/30], Step [55], D Loss: 0.5139, G Loss: -0.1370, Grad norm G: 5.0928, Grad norm D: 3.4652\n",
      "Epoch [8/30], Step [60], D Loss: 0.5792, G Loss: -0.4730, Grad norm G: 5.2355, Grad norm D: 3.5365\n",
      "Epoch [8/30], Step [65], D Loss: 0.6062, G Loss: -0.5341, Grad norm G: 5.1758, Grad norm D: 4.1603\n",
      "Epoch [8/30], Step [70], D Loss: -0.2371, G Loss: -0.1287, Grad norm G: 5.3847, Grad norm D: 5.2510\n",
      "Epoch [8/30], Step [75], D Loss: -0.1492, G Loss: 0.1171, Grad norm G: 4.9402, Grad norm D: 3.7475\n",
      "Epoch [8/30], Step [80], D Loss: 0.0331, G Loss: -0.2170, Grad norm G: 5.2946, Grad norm D: 3.5597\n",
      "Epoch [8/30], Step [85], D Loss: 0.2278, G Loss: -0.4126, Grad norm G: 5.2945, Grad norm D: 3.8942\n",
      "Epoch [8/30], Step [90], D Loss: 0.0609, G Loss: -0.2551, Grad norm G: 5.0826, Grad norm D: 4.9634\n",
      "Epoch [8/30], Step [95], D Loss: 0.0882, G Loss: -0.2409, Grad norm G: 5.3180, Grad norm D: 3.7663\n",
      "Epoch [8/30], Step [100], D Loss: 0.2258, G Loss: -0.4020, Grad norm G: 5.4188, Grad norm D: 3.6915\n",
      "Epoch [8/30], Step [105], D Loss: 0.2229, G Loss: -0.4119, Grad norm G: 5.4561, Grad norm D: 4.3324\n",
      "Epoch [8/30], Step [110], D Loss: 0.0170, G Loss: -0.0459, Grad norm G: 5.0596, Grad norm D: 4.1861\n",
      "Epoch [8/30], Step [115], D Loss: 0.0361, G Loss: -0.0042, Grad norm G: 5.3402, Grad norm D: 3.6698\n",
      "Epoch [8/30], Step [120], D Loss: 0.1657, G Loss: -0.0840, Grad norm G: 5.3216, Grad norm D: 3.6709\n",
      "Epoch [8/30], Step [125], D Loss: 0.2396, G Loss: -0.2256, Grad norm G: 5.3226, Grad norm D: 3.8343\n",
      "Epoch [8/30], Step [130], D Loss: 0.3792, G Loss: -0.1628, Grad norm G: 5.0891, Grad norm D: 4.1131\n",
      "Epoch [8/30], Step [135], D Loss: 0.2139, G Loss: -0.1003, Grad norm G: 5.1657, Grad norm D: 3.7349\n",
      "Epoch [9/30], Step [1], D Loss: 0.3723, G Loss: -0.2096, Grad norm G: 5.2302, Grad norm D: 4.1482\n",
      "Epoch [9/30], Step [5], D Loss: 0.2295, G Loss: -0.2449, Grad norm G: 5.3269, Grad norm D: 4.3720\n",
      "Epoch [9/30], Step [10], D Loss: 0.4097, G Loss: -0.3645, Grad norm G: 5.1939, Grad norm D: 3.9031\n",
      "Epoch [9/30], Step [15], D Loss: 0.2365, G Loss: -0.4213, Grad norm G: 5.2743, Grad norm D: 3.9819\n",
      "Epoch [9/30], Step [20], D Loss: 0.2101, G Loss: -0.3299, Grad norm G: 5.3009, Grad norm D: 4.1285\n",
      "Epoch [9/30], Step [25], D Loss: 0.0942, G Loss: -0.2007, Grad norm G: 5.3000, Grad norm D: 4.2759\n",
      "Epoch [9/30], Step [30], D Loss: -0.0877, G Loss: -0.1808, Grad norm G: 5.4912, Grad norm D: 4.0616\n",
      "Epoch [9/30], Step [35], D Loss: 0.0079, G Loss: -0.2022, Grad norm G: 5.5517, Grad norm D: 3.9958\n",
      "Epoch [9/30], Step [40], D Loss: 0.1972, G Loss: -0.1874, Grad norm G: 5.5688, Grad norm D: 4.3121\n",
      "Epoch [9/30], Step [45], D Loss: 0.0888, G Loss: -0.2230, Grad norm G: 5.5907, Grad norm D: 4.1948\n",
      "Epoch [9/30], Step [50], D Loss: 0.0542, G Loss: -0.1166, Grad norm G: 5.4816, Grad norm D: 3.7787\n",
      "Epoch [9/30], Step [55], D Loss: 0.4053, G Loss: -0.2333, Grad norm G: 5.3793, Grad norm D: 3.9174\n",
      "Epoch [9/30], Step [60], D Loss: 0.4132, G Loss: -0.2370, Grad norm G: 5.3769, Grad norm D: 4.7217\n",
      "Epoch [9/30], Step [65], D Loss: 0.2475, G Loss: -0.2262, Grad norm G: 5.3069, Grad norm D: 3.8596\n",
      "Epoch [9/30], Step [70], D Loss: 0.3569, G Loss: -0.2855, Grad norm G: 5.3660, Grad norm D: 3.6509\n",
      "Epoch [9/30], Step [75], D Loss: 0.0104, G Loss: -0.2400, Grad norm G: 5.3391, Grad norm D: 3.9724\n",
      "Epoch [9/30], Step [80], D Loss: -0.0868, G Loss: 0.0684, Grad norm G: 5.2641, Grad norm D: 4.9822\n",
      "Epoch [9/30], Step [85], D Loss: 0.0012, G Loss: 0.1132, Grad norm G: 5.3059, Grad norm D: 3.9412\n",
      "Epoch [9/30], Step [90], D Loss: 0.0714, G Loss: -0.0871, Grad norm G: 5.4115, Grad norm D: 3.6925\n",
      "Epoch [9/30], Step [95], D Loss: 0.0330, G Loss: -0.3124, Grad norm G: 5.4378, Grad norm D: 3.9379\n",
      "Epoch [9/30], Step [100], D Loss: -0.0450, G Loss: -0.2528, Grad norm G: 5.5232, Grad norm D: 4.7348\n",
      "Epoch [9/30], Step [105], D Loss: 0.2158, G Loss: -0.2993, Grad norm G: 5.3624, Grad norm D: 3.7171\n",
      "Epoch [9/30], Step [110], D Loss: 0.2582, G Loss: -0.3553, Grad norm G: 5.4817, Grad norm D: 3.8001\n",
      "Epoch [9/30], Step [115], D Loss: 0.1165, G Loss: -0.3739, Grad norm G: 5.5517, Grad norm D: 4.3154\n",
      "Epoch [9/30], Step [120], D Loss: 0.2064, G Loss: -0.0501, Grad norm G: 5.3900, Grad norm D: 4.6575\n",
      "Epoch [9/30], Step [125], D Loss: -0.0649, G Loss: 0.0895, Grad norm G: 5.3740, Grad norm D: 3.8542\n",
      "Epoch [9/30], Step [130], D Loss: -0.1213, G Loss: 0.0621, Grad norm G: 5.4368, Grad norm D: 3.8507\n",
      "Epoch [9/30], Step [135], D Loss: 0.0365, G Loss: -0.0531, Grad norm G: 5.2686, Grad norm D: 4.0135\n",
      "Epoch [10/30], Step [1], D Loss: 0.2540, G Loss: -0.1357, Grad norm G: 5.3097, Grad norm D: 4.1653\n",
      "Epoch [10/30], Step [5], D Loss: 0.2194, G Loss: -0.1567, Grad norm G: 5.3534, Grad norm D: 3.7834\n",
      "Epoch [10/30], Step [10], D Loss: 0.1140, G Loss: -0.2484, Grad norm G: 5.3378, Grad norm D: 4.1400\n",
      "Epoch [10/30], Step [15], D Loss: 0.0530, G Loss: -0.4125, Grad norm G: 5.3794, Grad norm D: 4.2452\n",
      "Epoch [10/30], Step [20], D Loss: 0.3014, G Loss: -0.3237, Grad norm G: 5.2587, Grad norm D: 4.2372\n",
      "Epoch [10/30], Step [25], D Loss: -0.1545, G Loss: -0.0920, Grad norm G: 5.2033, Grad norm D: 4.0867\n",
      "Epoch [10/30], Step [30], D Loss: -0.2304, G Loss: 0.0430, Grad norm G: 5.3335, Grad norm D: 4.1197\n",
      "Epoch [10/30], Step [35], D Loss: -0.0753, G Loss: -0.0585, Grad norm G: 5.5018, Grad norm D: 4.1966\n",
      "Epoch [10/30], Step [40], D Loss: 0.0641, G Loss: -0.1449, Grad norm G: 5.4674, Grad norm D: 3.9866\n",
      "Epoch [10/30], Step [45], D Loss: 0.0497, G Loss: -0.0393, Grad norm G: 5.4819, Grad norm D: 4.1239\n",
      "Epoch [10/30], Step [50], D Loss: -0.0695, G Loss: 0.0118, Grad norm G: 5.4504, Grad norm D: 4.1214\n",
      "Epoch [10/30], Step [55], D Loss: 0.1018, G Loss: -0.1266, Grad norm G: 5.4982, Grad norm D: 4.1960\n",
      "Epoch [10/30], Step [60], D Loss: 0.3983, G Loss: -0.2099, Grad norm G: 5.4128, Grad norm D: 4.2426\n",
      "Epoch [10/30], Step [65], D Loss: 0.1464, G Loss: -0.1354, Grad norm G: 5.4286, Grad norm D: 4.1957\n",
      "Epoch [10/30], Step [70], D Loss: 0.0916, G Loss: -0.1301, Grad norm G: 5.4569, Grad norm D: 4.1128\n",
      "Epoch [10/30], Step [75], D Loss: 0.1329, G Loss: -0.1624, Grad norm G: 5.4608, Grad norm D: 3.9909\n",
      "Epoch [10/30], Step [80], D Loss: -0.2252, G Loss: -0.0475, Grad norm G: 5.5080, Grad norm D: 4.3831\n",
      "Epoch [10/30], Step [85], D Loss: -0.3432, G Loss: 0.1643, Grad norm G: 5.5199, Grad norm D: 4.4818\n",
      "Epoch [10/30], Step [90], D Loss: -0.0239, G Loss: -0.0116, Grad norm G: 5.5645, Grad norm D: 3.7820\n",
      "Epoch [10/30], Step [95], D Loss: 0.0648, G Loss: -0.2679, Grad norm G: 5.5689, Grad norm D: 3.9042\n",
      "Epoch [10/30], Step [100], D Loss: 0.3345, G Loss: -0.3104, Grad norm G: 5.3547, Grad norm D: 4.0956\n",
      "Epoch [10/30], Step [105], D Loss: 0.3939, G Loss: -0.2788, Grad norm G: 5.3784, Grad norm D: 3.9728\n",
      "Epoch [10/30], Step [110], D Loss: 0.0046, G Loss: -0.2271, Grad norm G: 5.4773, Grad norm D: 4.1946\n",
      "Epoch [10/30], Step [115], D Loss: -0.1604, G Loss: -0.2146, Grad norm G: 5.6035, Grad norm D: 4.3831\n",
      "Epoch [10/30], Step [120], D Loss: -0.1038, G Loss: -0.0571, Grad norm G: 5.5614, Grad norm D: 4.2557\n",
      "Epoch [10/30], Step [125], D Loss: -0.2439, G Loss: 0.0997, Grad norm G: 5.6092, Grad norm D: 4.1041\n",
      "Epoch [10/30], Step [130], D Loss: -0.0853, G Loss: 0.1530, Grad norm G: 5.5677, Grad norm D: 4.1455\n",
      "Epoch [10/30], Step [135], D Loss: 0.0489, G Loss: -0.0564, Grad norm G: 5.5779, Grad norm D: 4.3187\n",
      "Epoch [11/30], Step [1], D Loss: 0.2019, G Loss: -0.1672, Grad norm G: 5.5057, Grad norm D: 4.1185\n",
      "Epoch [11/30], Step [5], D Loss: 0.0097, G Loss: -0.0529, Grad norm G: 5.3229, Grad norm D: 4.0997\n",
      "Epoch [11/30], Step [10], D Loss: 0.2577, G Loss: -0.0985, Grad norm G: 5.3932, Grad norm D: 3.8782\n",
      "Epoch [11/30], Step [15], D Loss: 0.4098, G Loss: -0.3899, Grad norm G: 5.3757, Grad norm D: 3.8500\n",
      "Epoch [11/30], Step [20], D Loss: 0.3618, G Loss: -0.4200, Grad norm G: 5.1592, Grad norm D: 3.9690\n",
      "Epoch [11/30], Step [25], D Loss: 0.3656, G Loss: -0.2571, Grad norm G: 5.2174, Grad norm D: 3.9826\n",
      "Epoch [11/30], Step [30], D Loss: -0.1227, G Loss: 0.0624, Grad norm G: 5.3168, Grad norm D: 4.1905\n",
      "Epoch [11/30], Step [35], D Loss: -0.1660, G Loss: 0.2424, Grad norm G: 5.5603, Grad norm D: 3.9937\n",
      "Epoch [11/30], Step [40], D Loss: -0.1948, G Loss: 0.2117, Grad norm G: 5.5765, Grad norm D: 3.9738\n",
      "Epoch [11/30], Step [45], D Loss: -0.1168, G Loss: 0.2840, Grad norm G: 5.6199, Grad norm D: 3.9394\n",
      "Epoch [11/30], Step [50], D Loss: -0.1453, G Loss: 0.2910, Grad norm G: 5.6273, Grad norm D: 4.0283\n",
      "Epoch [11/30], Step [55], D Loss: 0.2511, G Loss: 0.0370, Grad norm G: 5.5218, Grad norm D: 3.7467\n",
      "Epoch [11/30], Step [60], D Loss: 0.3276, G Loss: -0.2249, Grad norm G: 5.3556, Grad norm D: 4.3897\n",
      "Epoch [11/30], Step [65], D Loss: 0.4792, G Loss: -0.1781, Grad norm G: 5.2960, Grad norm D: 4.2009\n",
      "Epoch [11/30], Step [70], D Loss: 0.2880, G Loss: -0.1304, Grad norm G: 5.2415, Grad norm D: 4.0953\n",
      "Epoch [11/30], Step [75], D Loss: 0.3139, G Loss: -0.1250, Grad norm G: 5.3294, Grad norm D: 4.0467\n",
      "Epoch [11/30], Step [80], D Loss: 0.0526, G Loss: -0.1057, Grad norm G: 5.4852, Grad norm D: 4.2264\n",
      "Epoch [11/30], Step [85], D Loss: -0.1835, G Loss: 0.1175, Grad norm G: 5.4036, Grad norm D: 4.3234\n",
      "Epoch [11/30], Step [90], D Loss: -0.0862, G Loss: 0.2767, Grad norm G: 5.4593, Grad norm D: 4.2082\n",
      "Epoch [11/30], Step [95], D Loss: -0.2140, G Loss: 0.2424, Grad norm G: 5.5283, Grad norm D: 3.9902\n",
      "Epoch [11/30], Step [100], D Loss: -0.2943, G Loss: 0.1846, Grad norm G: 5.5218, Grad norm D: 3.9460\n",
      "Epoch [11/30], Step [105], D Loss: 0.0710, G Loss: 0.1269, Grad norm G: 5.4760, Grad norm D: 4.0806\n",
      "Epoch [11/30], Step [110], D Loss: 0.1308, G Loss: 0.1822, Grad norm G: 5.4153, Grad norm D: 3.9151\n",
      "Epoch [11/30], Step [115], D Loss: 0.3410, G Loss: -0.0276, Grad norm G: 5.3378, Grad norm D: 3.8317\n",
      "Epoch [11/30], Step [120], D Loss: 0.2129, G Loss: -0.0495, Grad norm G: 5.2544, Grad norm D: 4.0617\n",
      "Epoch [11/30], Step [125], D Loss: 0.1519, G Loss: -0.0411, Grad norm G: 5.2751, Grad norm D: 4.1505\n",
      "Epoch [11/30], Step [130], D Loss: 0.0434, G Loss: 0.1458, Grad norm G: 5.3032, Grad norm D: 3.9409\n",
      "Epoch [11/30], Step [135], D Loss: -0.1696, G Loss: 0.2417, Grad norm G: 5.4425, Grad norm D: 4.1558\n",
      "Epoch [12/30], Step [1], D Loss: -0.1965, G Loss: 0.3779, Grad norm G: 5.4992, Grad norm D: 4.2726\n",
      "Epoch [12/30], Step [5], D Loss: -0.0711, G Loss: 0.4098, Grad norm G: 5.4441, Grad norm D: 3.9958\n",
      "Epoch [12/30], Step [10], D Loss: -0.1311, G Loss: 0.3284, Grad norm G: 5.4186, Grad norm D: 3.8749\n",
      "Epoch [12/30], Step [15], D Loss: 0.0437, G Loss: 0.2360, Grad norm G: 5.5156, Grad norm D: 4.2064\n",
      "Epoch [12/30], Step [20], D Loss: 0.2853, G Loss: -0.0513, Grad norm G: 5.4111, Grad norm D: 3.5990\n",
      "Epoch [12/30], Step [25], D Loss: 0.5217, G Loss: -0.2318, Grad norm G: 5.2897, Grad norm D: 3.8517\n",
      "Epoch [12/30], Step [30], D Loss: 0.1955, G Loss: -0.0593, Grad norm G: 5.2490, Grad norm D: 4.2609\n",
      "Epoch [12/30], Step [35], D Loss: 0.1487, G Loss: 0.1432, Grad norm G: 5.3318, Grad norm D: 4.1820\n",
      "Epoch [12/30], Step [40], D Loss: 0.0556, G Loss: 0.2202, Grad norm G: 5.4274, Grad norm D: 3.8475\n",
      "Epoch [12/30], Step [45], D Loss: -0.1560, G Loss: 0.2669, Grad norm G: 5.5989, Grad norm D: 4.2028\n",
      "Epoch [12/30], Step [50], D Loss: -0.1644, G Loss: 0.4473, Grad norm G: 5.6120, Grad norm D: 4.1495\n",
      "Epoch [12/30], Step [55], D Loss: -0.2090, G Loss: 0.2975, Grad norm G: 5.6247, Grad norm D: 3.7414\n",
      "Epoch [12/30], Step [60], D Loss: -0.0237, G Loss: 0.2493, Grad norm G: 5.4758, Grad norm D: 3.7963\n",
      "Epoch [12/30], Step [65], D Loss: 0.0760, G Loss: 0.1723, Grad norm G: 5.2954, Grad norm D: 4.2233\n",
      "Epoch [12/30], Step [70], D Loss: 0.2136, G Loss: 0.0020, Grad norm G: 5.2070, Grad norm D: 3.8487\n",
      "Epoch [12/30], Step [75], D Loss: 0.2904, G Loss: -0.1630, Grad norm G: 5.2067, Grad norm D: 4.0275\n",
      "Epoch [12/30], Step [80], D Loss: 0.1946, G Loss: -0.2890, Grad norm G: 5.3621, Grad norm D: 4.2217\n",
      "Epoch [12/30], Step [85], D Loss: 0.3010, G Loss: -0.3405, Grad norm G: 5.3849, Grad norm D: 4.0526\n",
      "Epoch [12/30], Step [90], D Loss: -0.2145, G Loss: -0.0234, Grad norm G: 5.4857, Grad norm D: 4.4963\n",
      "Epoch [12/30], Step [95], D Loss: -0.2548, G Loss: 0.2556, Grad norm G: 5.5289, Grad norm D: 4.2325\n",
      "Epoch [12/30], Step [100], D Loss: -0.2858, G Loss: 0.3551, Grad norm G: 5.6524, Grad norm D: 3.8865\n",
      "Epoch [12/30], Step [105], D Loss: -0.2902, G Loss: 0.4870, Grad norm G: 5.7244, Grad norm D: 4.1436\n",
      "Epoch [12/30], Step [110], D Loss: -0.3675, G Loss: 0.4965, Grad norm G: 5.7754, Grad norm D: 4.2413\n",
      "Epoch [12/30], Step [115], D Loss: -0.0312, G Loss: 0.3329, Grad norm G: 5.5990, Grad norm D: 3.8384\n",
      "Epoch [12/30], Step [120], D Loss: 0.1905, G Loss: 0.0825, Grad norm G: 5.4627, Grad norm D: 3.8579\n",
      "Epoch [12/30], Step [125], D Loss: 0.5024, G Loss: -0.1128, Grad norm G: 5.3021, Grad norm D: 4.2130\n",
      "Epoch [12/30], Step [130], D Loss: 0.5910, G Loss: -0.2285, Grad norm G: 5.1995, Grad norm D: 3.9957\n",
      "Epoch [12/30], Step [135], D Loss: 0.4429, G Loss: -0.1242, Grad norm G: 5.2282, Grad norm D: 3.9670\n",
      "Epoch [13/30], Step [1], D Loss: 0.1704, G Loss: 0.1216, Grad norm G: 5.2563, Grad norm D: 4.4595\n",
      "Epoch [13/30], Step [5], D Loss: -0.0800, G Loss: 0.2468, Grad norm G: 5.2568, Grad norm D: 3.9876\n",
      "Epoch [13/30], Step [10], D Loss: -0.2429, G Loss: 0.3800, Grad norm G: 5.4666, Grad norm D: 3.8762\n",
      "Epoch [13/30], Step [15], D Loss: -0.7545, G Loss: 0.7369, Grad norm G: 5.5052, Grad norm D: 4.3791\n",
      "Epoch [13/30], Step [20], D Loss: -0.5988, G Loss: 0.7723, Grad norm G: 5.5423, Grad norm D: 4.0191\n",
      "Epoch [13/30], Step [25], D Loss: 0.2348, G Loss: 0.2298, Grad norm G: 5.6461, Grad norm D: 3.3492\n",
      "Epoch [13/30], Step [30], D Loss: 0.3164, G Loss: -0.2028, Grad norm G: 5.4662, Grad norm D: 3.8064\n",
      "Epoch [13/30], Step [35], D Loss: 0.4163, G Loss: -0.2735, Grad norm G: 5.3069, Grad norm D: 4.3681\n",
      "Epoch [13/30], Step [40], D Loss: 0.7041, G Loss: -0.2843, Grad norm G: 5.2176, Grad norm D: 3.7695\n",
      "Epoch [13/30], Step [45], D Loss: 0.1307, G Loss: -0.1470, Grad norm G: 5.2209, Grad norm D: 4.1585\n",
      "Epoch [13/30], Step [50], D Loss: 0.0160, G Loss: 0.2915, Grad norm G: 5.2108, Grad norm D: 4.6488\n",
      "Epoch [13/30], Step [55], D Loss: -0.2498, G Loss: 0.5241, Grad norm G: 5.3747, Grad norm D: 3.9279\n",
      "Epoch [13/30], Step [60], D Loss: -0.6099, G Loss: 0.7422, Grad norm G: 5.4502, Grad norm D: 3.9293\n",
      "Epoch [13/30], Step [65], D Loss: -0.5507, G Loss: 0.7235, Grad norm G: 5.6399, Grad norm D: 4.3404\n",
      "Epoch [13/30], Step [70], D Loss: -0.1262, G Loss: 0.5037, Grad norm G: 5.5260, Grad norm D: 3.8890\n",
      "Epoch [13/30], Step [75], D Loss: 0.0825, G Loss: 0.1015, Grad norm G: 5.3780, Grad norm D: 3.6483\n",
      "Epoch [13/30], Step [80], D Loss: 0.2151, G Loss: -0.1037, Grad norm G: 5.3607, Grad norm D: 4.0306\n",
      "Epoch [13/30], Step [85], D Loss: 0.5311, G Loss: -0.2295, Grad norm G: 5.2257, Grad norm D: 3.8040\n",
      "Epoch [13/30], Step [90], D Loss: 0.4512, G Loss: -0.2577, Grad norm G: 5.2639, Grad norm D: 4.1029\n",
      "Epoch [13/30], Step [95], D Loss: 0.0085, G Loss: 0.1141, Grad norm G: 5.2931, Grad norm D: 4.3829\n",
      "Epoch [13/30], Step [100], D Loss: -0.2730, G Loss: 0.4602, Grad norm G: 5.4167, Grad norm D: 4.2030\n",
      "Epoch [13/30], Step [105], D Loss: -0.4100, G Loss: 0.6407, Grad norm G: 5.5830, Grad norm D: 4.0390\n",
      "Epoch [13/30], Step [110], D Loss: -0.4462, G Loss: 0.7387, Grad norm G: 5.7138, Grad norm D: 4.0436\n",
      "Epoch [13/30], Step [115], D Loss: -0.2904, G Loss: 0.6377, Grad norm G: 5.7700, Grad norm D: 3.9005\n",
      "Epoch [13/30], Step [120], D Loss: 0.0767, G Loss: 0.4243, Grad norm G: 5.6384, Grad norm D: 3.9861\n",
      "Epoch [13/30], Step [125], D Loss: 0.2056, G Loss: 0.2056, Grad norm G: 5.4476, Grad norm D: 4.0282\n",
      "Epoch [13/30], Step [130], D Loss: 0.5916, G Loss: 0.0009, Grad norm G: 5.2074, Grad norm D: 3.8202\n",
      "Epoch [13/30], Step [135], D Loss: 0.5325, G Loss: -0.0311, Grad norm G: 5.1657, Grad norm D: 4.1836\n",
      "Epoch [14/30], Step [1], D Loss: 0.3149, G Loss: 0.0121, Grad norm G: 5.1509, Grad norm D: 3.9487\n",
      "Epoch [14/30], Step [5], D Loss: 0.0218, G Loss: 0.1001, Grad norm G: 5.2162, Grad norm D: 4.0485\n",
      "Epoch [14/30], Step [10], D Loss: -0.4248, G Loss: 0.4169, Grad norm G: 5.3689, Grad norm D: 4.2408\n",
      "Epoch [14/30], Step [15], D Loss: -0.6385, G Loss: 0.8921, Grad norm G: 5.5030, Grad norm D: 4.2067\n",
      "Epoch [14/30], Step [20], D Loss: -0.6851, G Loss: 0.8532, Grad norm G: 5.7620, Grad norm D: 3.9832\n",
      "Epoch [14/30], Step [25], D Loss: -0.3120, G Loss: 0.6240, Grad norm G: 5.8739, Grad norm D: 4.0819\n",
      "Epoch [14/30], Step [30], D Loss: 0.2313, G Loss: 0.2959, Grad norm G: 5.6728, Grad norm D: 3.8469\n",
      "Epoch [14/30], Step [35], D Loss: 0.5207, G Loss: -0.0335, Grad norm G: 5.3594, Grad norm D: 3.7109\n",
      "Epoch [14/30], Step [40], D Loss: 0.5334, G Loss: -0.0859, Grad norm G: 5.1948, Grad norm D: 3.9648\n",
      "Epoch [14/30], Step [45], D Loss: 0.3102, G Loss: -0.0192, Grad norm G: 5.0938, Grad norm D: 4.0577\n",
      "Epoch [14/30], Step [50], D Loss: -0.3382, G Loss: 0.0918, Grad norm G: 5.0845, Grad norm D: 4.3309\n",
      "Epoch [14/30], Step [55], D Loss: -0.0258, G Loss: 0.4543, Grad norm G: 5.1762, Grad norm D: 4.1503\n",
      "Epoch [14/30], Step [60], D Loss: -0.6211, G Loss: 0.8560, Grad norm G: 5.3835, Grad norm D: 4.1127\n",
      "Epoch [14/30], Step [65], D Loss: -0.6038, G Loss: 0.9718, Grad norm G: 5.5506, Grad norm D: 3.9321\n",
      "Epoch [14/30], Step [70], D Loss: -0.4253, G Loss: 0.8086, Grad norm G: 5.5315, Grad norm D: 3.8493\n",
      "Epoch [14/30], Step [75], D Loss: -0.1199, G Loss: 0.4050, Grad norm G: 5.4201, Grad norm D: 4.2117\n",
      "Epoch [14/30], Step [80], D Loss: 0.4392, G Loss: -0.0139, Grad norm G: 5.3091, Grad norm D: 3.6532\n",
      "Epoch [14/30], Step [85], D Loss: 0.6278, G Loss: -0.3245, Grad norm G: 5.2685, Grad norm D: 3.9336\n",
      "Epoch [14/30], Step [90], D Loss: 0.3641, G Loss: -0.3615, Grad norm G: 5.0953, Grad norm D: 4.4727\n",
      "Epoch [14/30], Step [95], D Loss: 0.6130, G Loss: -0.3807, Grad norm G: 5.1386, Grad norm D: 3.8697\n",
      "Epoch [14/30], Step [100], D Loss: 0.1996, G Loss: -0.0065, Grad norm G: 5.1624, Grad norm D: 4.2745\n",
      "Epoch [14/30], Step [105], D Loss: -0.5408, G Loss: 0.6181, Grad norm G: 5.3015, Grad norm D: 4.4486\n",
      "Epoch [14/30], Step [110], D Loss: -0.6090, G Loss: 0.9248, Grad norm G: 5.5777, Grad norm D: 3.8720\n",
      "Epoch [14/30], Step [115], D Loss: -0.6989, G Loss: 0.9905, Grad norm G: 5.7993, Grad norm D: 3.8781\n",
      "Epoch [14/30], Step [120], D Loss: -0.4712, G Loss: 0.8241, Grad norm G: 5.8499, Grad norm D: 4.0988\n",
      "Epoch [14/30], Step [125], D Loss: 0.0679, G Loss: 0.5274, Grad norm G: 5.5804, Grad norm D: 3.6654\n",
      "Epoch [14/30], Step [130], D Loss: 0.3101, G Loss: 0.1743, Grad norm G: 5.2971, Grad norm D: 3.6395\n",
      "Epoch [14/30], Step [135], D Loss: 0.4846, G Loss: -0.0376, Grad norm G: 5.1151, Grad norm D: 4.0846\n",
      "Epoch [15/30], Step [1], D Loss: 0.7135, G Loss: -0.2270, Grad norm G: 5.0736, Grad norm D: 3.8035\n",
      "Epoch [15/30], Step [5], D Loss: 0.7416, G Loss: -0.3245, Grad norm G: 5.1826, Grad norm D: 3.9179\n",
      "Epoch [15/30], Step [10], D Loss: 0.2580, G Loss: -0.0178, Grad norm G: 5.2802, Grad norm D: 4.3446\n",
      "Epoch [15/30], Step [15], D Loss: -0.0905, G Loss: 0.3765, Grad norm G: 5.4805, Grad norm D: 4.2511\n",
      "Epoch [15/30], Step [20], D Loss: -0.5761, G Loss: 0.7929, Grad norm G: 5.7217, Grad norm D: 4.1193\n",
      "Epoch [15/30], Step [25], D Loss: -0.8385, G Loss: 1.0562, Grad norm G: 5.7598, Grad norm D: 4.1034\n",
      "Epoch [15/30], Step [30], D Loss: -0.7614, G Loss: 1.0713, Grad norm G: 5.6362, Grad norm D: 4.0135\n",
      "Epoch [15/30], Step [35], D Loss: -0.2879, G Loss: 0.6992, Grad norm G: 5.5288, Grad norm D: 3.6437\n",
      "Epoch [15/30], Step [40], D Loss: 0.2425, G Loss: 0.3320, Grad norm G: 5.2609, Grad norm D: 3.7336\n",
      "Epoch [15/30], Step [45], D Loss: 0.5612, G Loss: -0.0207, Grad norm G: 5.0432, Grad norm D: 3.7369\n",
      "Epoch [15/30], Step [50], D Loss: 0.8469, G Loss: -0.3257, Grad norm G: 5.0255, Grad norm D: 3.6599\n",
      "Epoch [15/30], Step [55], D Loss: 0.5322, G Loss: -0.2408, Grad norm G: 5.0377, Grad norm D: 4.3786\n",
      "Epoch [15/30], Step [60], D Loss: 0.2657, G Loss: 0.1160, Grad norm G: 5.1671, Grad norm D: 4.1840\n",
      "Epoch [15/30], Step [65], D Loss: -0.1802, G Loss: 0.4558, Grad norm G: 5.3190, Grad norm D: 3.9853\n",
      "Epoch [15/30], Step [70], D Loss: -0.6331, G Loss: 0.8505, Grad norm G: 5.5331, Grad norm D: 4.0098\n",
      "Epoch [15/30], Step [75], D Loss: -0.6696, G Loss: 1.0569, Grad norm G: 5.7789, Grad norm D: 4.0240\n",
      "Epoch [15/30], Step [80], D Loss: -0.3902, G Loss: 0.8378, Grad norm G: 5.8266, Grad norm D: 3.6342\n",
      "Epoch [15/30], Step [85], D Loss: 0.0499, G Loss: 0.6487, Grad norm G: 5.5336, Grad norm D: 3.7265\n",
      "Epoch [15/30], Step [90], D Loss: 0.3290, G Loss: 0.3904, Grad norm G: 5.3499, Grad norm D: 3.9546\n",
      "Epoch [15/30], Step [95], D Loss: 0.6457, G Loss: -0.0065, Grad norm G: 5.1474, Grad norm D: 3.7811\n",
      "Epoch [15/30], Step [100], D Loss: 0.6929, G Loss: -0.1781, Grad norm G: 5.1504, Grad norm D: 3.9499\n",
      "Epoch [15/30], Step [105], D Loss: 0.5385, G Loss: -0.0317, Grad norm G: 5.1544, Grad norm D: 4.2169\n",
      "Epoch [15/30], Step [110], D Loss: 0.1783, G Loss: 0.1851, Grad norm G: 5.2109, Grad norm D: 4.0143\n",
      "Epoch [15/30], Step [115], D Loss: -0.2809, G Loss: 0.5419, Grad norm G: 5.4589, Grad norm D: 4.1603\n",
      "Epoch [15/30], Step [120], D Loss: -0.5808, G Loss: 0.9053, Grad norm G: 5.5499, Grad norm D: 4.0238\n",
      "Epoch [15/30], Step [125], D Loss: -0.5564, G Loss: 0.9930, Grad norm G: 5.6559, Grad norm D: 3.8144\n",
      "Epoch [15/30], Step [130], D Loss: -0.4990, G Loss: 0.8544, Grad norm G: 5.6471, Grad norm D: 3.9112\n",
      "Epoch [15/30], Step [135], D Loss: 0.0547, G Loss: 0.5271, Grad norm G: 5.4259, Grad norm D: 3.8888\n",
      "Epoch [16/30], Step [1], D Loss: 0.7676, G Loss: 0.0419, Grad norm G: 5.0457, Grad norm D: 3.5274\n",
      "Epoch [16/30], Step [5], D Loss: 0.7841, G Loss: -0.2823, Grad norm G: 5.0239, Grad norm D: 3.6340\n",
      "Epoch [16/30], Step [10], D Loss: 0.5769, G Loss: -0.3583, Grad norm G: 5.0800, Grad norm D: 4.2658\n",
      "Epoch [16/30], Step [15], D Loss: 0.7279, G Loss: -0.3013, Grad norm G: 5.0733, Grad norm D: 4.0544\n",
      "Epoch [16/30], Step [20], D Loss: 0.1926, G Loss: -0.1830, Grad norm G: 5.1444, Grad norm D: 4.1186\n",
      "Epoch [16/30], Step [25], D Loss: -0.2265, G Loss: 0.4751, Grad norm G: 5.3322, Grad norm D: 4.5656\n",
      "Epoch [16/30], Step [30], D Loss: -0.6747, G Loss: 0.9343, Grad norm G: 5.6185, Grad norm D: 4.0697\n",
      "Epoch [16/30], Step [35], D Loss: -0.9547, G Loss: 1.1586, Grad norm G: 5.8207, Grad norm D: 3.9782\n",
      "Epoch [16/30], Step [40], D Loss: -0.7009, G Loss: 1.0484, Grad norm G: 5.9011, Grad norm D: 3.8646\n",
      "Epoch [16/30], Step [45], D Loss: -0.1208, G Loss: 0.6915, Grad norm G: 5.5777, Grad norm D: 3.7496\n",
      "Epoch [16/30], Step [50], D Loss: 0.3955, G Loss: 0.2642, Grad norm G: 5.3105, Grad norm D: 3.4381\n",
      "Epoch [16/30], Step [55], D Loss: 0.8560, G Loss: -0.0898, Grad norm G: 5.1208, Grad norm D: 3.9313\n",
      "Epoch [16/30], Step [60], D Loss: 0.9904, G Loss: -0.2745, Grad norm G: 4.9674, Grad norm D: 3.8308\n",
      "Epoch [16/30], Step [65], D Loss: 0.7953, G Loss: -0.2816, Grad norm G: 4.9237, Grad norm D: 3.7779\n",
      "Epoch [16/30], Step [70], D Loss: 0.2644, G Loss: 0.0582, Grad norm G: 5.0051, Grad norm D: 4.3218\n",
      "Epoch [16/30], Step [75], D Loss: 0.0076, G Loss: 0.5280, Grad norm G: 5.2319, Grad norm D: 4.0407\n",
      "Epoch [16/30], Step [80], D Loss: -0.4866, G Loss: 0.8110, Grad norm G: 5.5149, Grad norm D: 3.9045\n",
      "Epoch [16/30], Step [85], D Loss: -0.8706, G Loss: 1.2339, Grad norm G: 5.6571, Grad norm D: 4.0157\n",
      "Epoch [16/30], Step [90], D Loss: -0.7342, G Loss: 1.2477, Grad norm G: 5.7459, Grad norm D: 4.0809\n",
      "Epoch [16/30], Step [95], D Loss: -0.3822, G Loss: 0.7913, Grad norm G: 5.5482, Grad norm D: 3.5477\n",
      "Epoch [16/30], Step [100], D Loss: 0.3324, G Loss: 0.3024, Grad norm G: 5.2330, Grad norm D: 3.4750\n",
      "Epoch [16/30], Step [105], D Loss: 0.7498, G Loss: 0.0189, Grad norm G: 4.9570, Grad norm D: 3.6549\n",
      "Epoch [16/30], Step [110], D Loss: 0.8671, G Loss: -0.3328, Grad norm G: 4.9529, Grad norm D: 3.6964\n",
      "Epoch [16/30], Step [115], D Loss: 0.8353, G Loss: -0.4000, Grad norm G: 4.9782, Grad norm D: 4.0353\n",
      "Epoch [16/30], Step [120], D Loss: 0.5667, G Loss: -0.1411, Grad norm G: 4.9841, Grad norm D: 4.1915\n",
      "Epoch [16/30], Step [125], D Loss: 0.2201, G Loss: 0.1041, Grad norm G: 5.3174, Grad norm D: 3.9179\n",
      "Epoch [16/30], Step [130], D Loss: -0.4876, G Loss: 0.6697, Grad norm G: 5.4952, Grad norm D: 4.1409\n",
      "Epoch [16/30], Step [135], D Loss: -0.9615, G Loss: 1.2728, Grad norm G: 5.7806, Grad norm D: 4.6662\n",
      "Epoch [17/30], Step [1], D Loss: -0.7993, G Loss: 1.3310, Grad norm G: 5.7566, Grad norm D: 3.9280\n",
      "Epoch [17/30], Step [5], D Loss: -0.5035, G Loss: 1.0611, Grad norm G: 5.7053, Grad norm D: 3.5843\n",
      "Epoch [17/30], Step [10], D Loss: -0.0345, G Loss: 0.5779, Grad norm G: 5.6391, Grad norm D: 3.8502\n",
      "Epoch [17/30], Step [15], D Loss: 0.5661, G Loss: 0.1083, Grad norm G: 5.1474, Grad norm D: 3.6470\n",
      "Epoch [17/30], Step [20], D Loss: 0.8314, G Loss: -0.1778, Grad norm G: 4.8524, Grad norm D: 3.6645\n",
      "Epoch [17/30], Step [25], D Loss: 0.9141, G Loss: -0.1779, Grad norm G: 4.7653, Grad norm D: 3.9371\n",
      "Epoch [17/30], Step [30], D Loss: 0.5627, G Loss: -0.0510, Grad norm G: 4.8711, Grad norm D: 3.9288\n",
      "Epoch [17/30], Step [35], D Loss: 0.1529, G Loss: 0.1892, Grad norm G: 5.1113, Grad norm D: 4.0425\n",
      "Epoch [17/30], Step [40], D Loss: -0.8940, G Loss: 0.8272, Grad norm G: 5.1720, Grad norm D: 4.5825\n",
      "Epoch [17/30], Step [45], D Loss: -0.8488, G Loss: 1.1316, Grad norm G: 5.4292, Grad norm D: 3.9746\n",
      "Epoch [17/30], Step [50], D Loss: -0.9484, G Loss: 1.1081, Grad norm G: 5.6819, Grad norm D: 3.7377\n",
      "Epoch [17/30], Step [55], D Loss: -0.4155, G Loss: 0.9398, Grad norm G: 5.5826, Grad norm D: 3.8229\n",
      "Epoch [17/30], Step [60], D Loss: 0.2349, G Loss: 0.5011, Grad norm G: 5.3382, Grad norm D: 3.3967\n",
      "Epoch [17/30], Step [65], D Loss: 0.6876, G Loss: 0.0318, Grad norm G: 5.0472, Grad norm D: 3.5729\n",
      "Epoch [17/30], Step [70], D Loss: 0.9593, G Loss: -0.2516, Grad norm G: 5.0086, Grad norm D: 3.8322\n",
      "Epoch [17/30], Step [75], D Loss: 0.8415, G Loss: -0.2677, Grad norm G: 5.0438, Grad norm D: 3.9278\n",
      "Epoch [17/30], Step [80], D Loss: 0.4585, G Loss: -0.1178, Grad norm G: 5.2518, Grad norm D: 4.1978\n",
      "Epoch [17/30], Step [85], D Loss: 0.0220, G Loss: 0.5043, Grad norm G: 5.2473, Grad norm D: 4.2411\n",
      "Epoch [17/30], Step [90], D Loss: -0.4904, G Loss: 0.9890, Grad norm G: 5.5569, Grad norm D: 4.0580\n",
      "Epoch [17/30], Step [95], D Loss: -0.9300, G Loss: 1.3601, Grad norm G: 5.8119, Grad norm D: 4.0035\n",
      "Epoch [17/30], Step [100], D Loss: -0.8723, G Loss: 1.3079, Grad norm G: 5.8381, Grad norm D: 3.8752\n",
      "Epoch [17/30], Step [105], D Loss: -0.2092, G Loss: 1.0206, Grad norm G: 5.7414, Grad norm D: 3.7316\n",
      "Epoch [17/30], Step [110], D Loss: 0.2205, G Loss: 0.6023, Grad norm G: 5.3580, Grad norm D: 3.6506\n",
      "Epoch [17/30], Step [115], D Loss: 0.6369, G Loss: 0.1513, Grad norm G: 5.0403, Grad norm D: 3.6373\n",
      "Epoch [17/30], Step [120], D Loss: 1.0225, G Loss: -0.0697, Grad norm G: 4.7549, Grad norm D: 3.5816\n",
      "Epoch [17/30], Step [125], D Loss: 0.9448, G Loss: -0.2603, Grad norm G: 4.8220, Grad norm D: 3.8506\n",
      "Epoch [17/30], Step [130], D Loss: 0.8010, G Loss: -0.2141, Grad norm G: 4.9141, Grad norm D: 3.9297\n",
      "Epoch [17/30], Step [135], D Loss: 0.3818, G Loss: 0.0865, Grad norm G: 5.1184, Grad norm D: 4.1650\n",
      "Epoch [18/30], Step [1], D Loss: -0.1822, G Loss: 0.6322, Grad norm G: 5.3547, Grad norm D: 4.1606\n",
      "Epoch [18/30], Step [5], D Loss: -0.7753, G Loss: 1.0579, Grad norm G: 5.5009, Grad norm D: 4.1256\n",
      "Epoch [18/30], Step [10], D Loss: -0.8318, G Loss: 1.3140, Grad norm G: 5.6094, Grad norm D: 4.1267\n",
      "Epoch [18/30], Step [15], D Loss: -0.5981, G Loss: 1.2128, Grad norm G: 5.7628, Grad norm D: 3.7605\n",
      "Epoch [18/30], Step [20], D Loss: -0.1790, G Loss: 0.8657, Grad norm G: 5.6261, Grad norm D: 3.6244\n",
      "Epoch [18/30], Step [25], D Loss: 0.3467, G Loss: 0.5248, Grad norm G: 5.2701, Grad norm D: 3.5665\n",
      "Epoch [18/30], Step [30], D Loss: 0.7170, G Loss: 0.1516, Grad norm G: 5.0207, Grad norm D: 3.5394\n",
      "Epoch [18/30], Step [35], D Loss: 0.9313, G Loss: -0.1120, Grad norm G: 4.8949, Grad norm D: 3.6685\n",
      "Epoch [18/30], Step [40], D Loss: 0.8287, G Loss: -0.1282, Grad norm G: 4.8338, Grad norm D: 3.7987\n",
      "Epoch [18/30], Step [45], D Loss: 0.6144, G Loss: -0.0913, Grad norm G: 5.0354, Grad norm D: 3.8345\n",
      "Epoch [18/30], Step [50], D Loss: 0.1582, G Loss: 0.3539, Grad norm G: 5.1385, Grad norm D: 4.2043\n",
      "Epoch [18/30], Step [55], D Loss: -0.6077, G Loss: 1.0895, Grad norm G: 5.3208, Grad norm D: 4.4997\n",
      "Epoch [18/30], Step [60], D Loss: -0.8279, G Loss: 1.4177, Grad norm G: 5.5261, Grad norm D: 3.9283\n",
      "Epoch [18/30], Step [65], D Loss: -0.7231, G Loss: 1.2513, Grad norm G: 5.6347, Grad norm D: 3.7037\n",
      "Epoch [18/30], Step [70], D Loss: -0.0415, G Loss: 0.8635, Grad norm G: 5.4191, Grad norm D: 3.7509\n",
      "Epoch [18/30], Step [75], D Loss: 0.3622, G Loss: 0.3646, Grad norm G: 5.1580, Grad norm D: 3.6132\n",
      "Epoch [18/30], Step [80], D Loss: 0.7495, G Loss: 0.0843, Grad norm G: 4.9474, Grad norm D: 3.7059\n",
      "Epoch [18/30], Step [85], D Loss: 0.7888, G Loss: 0.0054, Grad norm G: 4.8689, Grad norm D: 3.8451\n",
      "Epoch [18/30], Step [90], D Loss: 0.6083, G Loss: -0.1383, Grad norm G: 4.9563, Grad norm D: 3.8467\n",
      "Epoch [18/30], Step [95], D Loss: 0.4293, G Loss: -0.0080, Grad norm G: 5.0766, Grad norm D: 4.0753\n",
      "Epoch [18/30], Step [100], D Loss: 0.1136, G Loss: 0.4377, Grad norm G: 5.2611, Grad norm D: 4.1013\n",
      "Epoch [18/30], Step [105], D Loss: -0.3040, G Loss: 0.8930, Grad norm G: 5.4145, Grad norm D: 3.9833\n",
      "Epoch [18/30], Step [110], D Loss: -0.7383, G Loss: 1.3114, Grad norm G: 5.5886, Grad norm D: 3.9552\n",
      "Epoch [18/30], Step [115], D Loss: -0.5019, G Loss: 1.2888, Grad norm G: 5.7058, Grad norm D: 3.7574\n",
      "Epoch [18/30], Step [120], D Loss: -0.3262, G Loss: 1.0010, Grad norm G: 5.5877, Grad norm D: 3.6045\n",
      "Epoch [18/30], Step [125], D Loss: 0.2414, G Loss: 0.7086, Grad norm G: 5.2747, Grad norm D: 3.5384\n",
      "Epoch [18/30], Step [130], D Loss: 0.6466, G Loss: 0.4078, Grad norm G: 5.0507, Grad norm D: 3.7763\n",
      "Epoch [18/30], Step [135], D Loss: 0.9053, G Loss: 0.0112, Grad norm G: 4.8993, Grad norm D: 3.5568\n",
      "Epoch [19/30], Step [1], D Loss: 0.8849, G Loss: -0.2029, Grad norm G: 4.9223, Grad norm D: 3.7517\n",
      "Epoch [19/30], Step [5], D Loss: 0.7968, G Loss: -0.1723, Grad norm G: 5.0122, Grad norm D: 4.2016\n",
      "Epoch [19/30], Step [10], D Loss: 0.5727, G Loss: 0.0320, Grad norm G: 5.0625, Grad norm D: 3.9316\n",
      "Epoch [19/30], Step [15], D Loss: -0.0710, G Loss: 0.4002, Grad norm G: 5.2496, Grad norm D: 4.0451\n",
      "Epoch [19/30], Step [20], D Loss: -0.5573, G Loss: 0.9852, Grad norm G: 5.4097, Grad norm D: 4.3303\n",
      "Epoch [19/30], Step [25], D Loss: -0.5852, G Loss: 1.2128, Grad norm G: 5.5620, Grad norm D: 3.8632\n",
      "Epoch [19/30], Step [30], D Loss: -0.4800, G Loss: 1.0925, Grad norm G: 5.7227, Grad norm D: 3.5916\n",
      "Epoch [19/30], Step [35], D Loss: -0.2108, G Loss: 0.8567, Grad norm G: 5.6166, Grad norm D: 3.6681\n",
      "Epoch [19/30], Step [40], D Loss: -0.0349, G Loss: 0.6991, Grad norm G: 5.3392, Grad norm D: 4.0353\n",
      "Epoch [19/30], Step [45], D Loss: 0.4533, G Loss: 0.4028, Grad norm G: 4.9643, Grad norm D: 3.4413\n",
      "Epoch [19/30], Step [50], D Loss: 0.8598, G Loss: 0.0525, Grad norm G: 4.8345, Grad norm D: 3.5615\n",
      "Epoch [19/30], Step [55], D Loss: 0.6914, G Loss: -0.0371, Grad norm G: 4.8100, Grad norm D: 4.0440\n",
      "Epoch [19/30], Step [60], D Loss: 0.7034, G Loss: -0.1400, Grad norm G: 4.9634, Grad norm D: 3.6482\n",
      "Epoch [19/30], Step [65], D Loss: 0.4094, G Loss: 0.0993, Grad norm G: 5.1059, Grad norm D: 4.0660\n",
      "Epoch [19/30], Step [70], D Loss: 0.0802, G Loss: 0.5417, Grad norm G: 5.3308, Grad norm D: 4.0121\n",
      "Epoch [19/30], Step [75], D Loss: -0.5045, G Loss: 0.9328, Grad norm G: 5.5776, Grad norm D: 3.8645\n",
      "Epoch [19/30], Step [80], D Loss: -0.7095, G Loss: 1.2783, Grad norm G: 5.7440, Grad norm D: 3.7742\n",
      "Epoch [19/30], Step [85], D Loss: -0.6029, G Loss: 1.2657, Grad norm G: 5.7520, Grad norm D: 3.6964\n",
      "Epoch [19/30], Step [90], D Loss: -0.2017, G Loss: 0.8925, Grad norm G: 5.6074, Grad norm D: 3.7125\n",
      "Epoch [19/30], Step [95], D Loss: 0.3373, G Loss: 0.5151, Grad norm G: 5.2055, Grad norm D: 3.6307\n",
      "Epoch [19/30], Step [100], D Loss: 0.6630, G Loss: 0.1986, Grad norm G: 4.9128, Grad norm D: 3.6936\n",
      "Epoch [19/30], Step [105], D Loss: 0.9047, G Loss: -0.0298, Grad norm G: 4.9235, Grad norm D: 3.5977\n",
      "Epoch [19/30], Step [110], D Loss: 0.7633, G Loss: -0.2013, Grad norm G: 5.0744, Grad norm D: 3.8051\n",
      "Epoch [19/30], Step [115], D Loss: 0.6979, G Loss: -0.1705, Grad norm G: 5.1981, Grad norm D: 4.2701\n",
      "Epoch [19/30], Step [120], D Loss: 0.5006, G Loss: 0.1164, Grad norm G: 5.3444, Grad norm D: 4.0353\n",
      "Epoch [19/30], Step [125], D Loss: -0.2888, G Loss: 0.7339, Grad norm G: 5.5291, Grad norm D: 4.1706\n",
      "Epoch [19/30], Step [130], D Loss: -0.8883, G Loss: 1.3247, Grad norm G: 5.7219, Grad norm D: 4.0796\n",
      "Epoch [19/30], Step [135], D Loss: -0.8717, G Loss: 1.4329, Grad norm G: 5.7366, Grad norm D: 3.7771\n",
      "Epoch [20/30], Step [1], D Loss: -0.3684, G Loss: 1.1411, Grad norm G: 5.5475, Grad norm D: 3.7269\n",
      "Epoch [20/30], Step [5], D Loss: 0.0877, G Loss: 0.8542, Grad norm G: 5.3010, Grad norm D: 3.5936\n",
      "Epoch [20/30], Step [10], D Loss: 0.6641, G Loss: 0.3018, Grad norm G: 4.9028, Grad norm D: 3.4040\n",
      "Epoch [20/30], Step [15], D Loss: 1.2396, G Loss: -0.0447, Grad norm G: 4.6628, Grad norm D: 3.5298\n",
      "Epoch [20/30], Step [20], D Loss: 1.1776, G Loss: -0.3530, Grad norm G: 4.7615, Grad norm D: 3.5976\n",
      "Epoch [20/30], Step [25], D Loss: 1.1232, G Loss: -0.4931, Grad norm G: 4.9195, Grad norm D: 3.7422\n",
      "Epoch [20/30], Step [30], D Loss: 0.8147, G Loss: -0.3603, Grad norm G: 5.1330, Grad norm D: 4.0454\n",
      "Epoch [20/30], Step [35], D Loss: 0.1374, G Loss: 0.1876, Grad norm G: 5.3108, Grad norm D: 4.2087\n",
      "Epoch [20/30], Step [40], D Loss: -0.4538, G Loss: 0.8462, Grad norm G: 5.4268, Grad norm D: 4.2897\n",
      "Epoch [20/30], Step [45], D Loss: -0.8398, G Loss: 1.4018, Grad norm G: 5.6380, Grad norm D: 4.1009\n",
      "Epoch [20/30], Step [50], D Loss: -1.0234, G Loss: 1.4287, Grad norm G: 5.7392, Grad norm D: 3.9144\n",
      "Epoch [20/30], Step [55], D Loss: -0.6312, G Loss: 1.0654, Grad norm G: 5.6242, Grad norm D: 3.6245\n",
      "Epoch [20/30], Step [60], D Loss: 0.0022, G Loss: 0.6042, Grad norm G: 5.3137, Grad norm D: 3.9662\n",
      "Epoch [20/30], Step [65], D Loss: 0.7744, G Loss: 0.1778, Grad norm G: 4.9073, Grad norm D: 3.4315\n",
      "Epoch [20/30], Step [70], D Loss: 1.1924, G Loss: -0.1866, Grad norm G: 4.7693, Grad norm D: 3.3517\n",
      "Epoch [20/30], Step [75], D Loss: 1.4008, G Loss: -0.4251, Grad norm G: 4.9695, Grad norm D: 3.7306\n",
      "Epoch [20/30], Step [80], D Loss: 1.1647, G Loss: -0.4411, Grad norm G: 5.0535, Grad norm D: 3.6760\n",
      "Epoch [20/30], Step [85], D Loss: 0.6270, G Loss: -0.0517, Grad norm G: 5.1158, Grad norm D: 4.1668\n",
      "Epoch [20/30], Step [90], D Loss: -0.0385, G Loss: 0.6467, Grad norm G: 5.2833, Grad norm D: 4.3176\n",
      "Epoch [20/30], Step [95], D Loss: -0.5750, G Loss: 1.2968, Grad norm G: 5.5796, Grad norm D: 4.1236\n",
      "Epoch [20/30], Step [100], D Loss: -0.8861, G Loss: 1.5392, Grad norm G: 5.8001, Grad norm D: 3.9000\n",
      "Epoch [20/30], Step [105], D Loss: -0.6637, G Loss: 1.4312, Grad norm G: 5.7824, Grad norm D: 3.7078\n",
      "Epoch [20/30], Step [110], D Loss: -0.1794, G Loss: 1.0498, Grad norm G: 5.5176, Grad norm D: 3.4872\n",
      "Epoch [20/30], Step [115], D Loss: 0.5238, G Loss: 0.7001, Grad norm G: 4.9973, Grad norm D: 3.4386\n",
      "Epoch [20/30], Step [120], D Loss: 0.7879, G Loss: 0.3589, Grad norm G: 4.7512, Grad norm D: 3.5207\n",
      "Epoch [20/30], Step [125], D Loss: 0.9550, G Loss: 0.0012, Grad norm G: 4.7949, Grad norm D: 3.5949\n",
      "Epoch [20/30], Step [130], D Loss: 1.1465, G Loss: -0.2490, Grad norm G: 4.8634, Grad norm D: 3.6333\n",
      "Epoch [20/30], Step [135], D Loss: 0.9508, G Loss: -0.2945, Grad norm G: 5.0327, Grad norm D: 4.0956\n",
      "Epoch [21/30], Step [1], D Loss: 0.5503, G Loss: -0.0738, Grad norm G: 5.2467, Grad norm D: 4.1652\n",
      "Epoch [21/30], Step [5], D Loss: 0.2556, G Loss: 0.3328, Grad norm G: 5.2876, Grad norm D: 4.0346\n",
      "Epoch [21/30], Step [10], D Loss: -0.4905, G Loss: 0.9880, Grad norm G: 5.5058, Grad norm D: 4.2712\n",
      "Epoch [21/30], Step [15], D Loss: -1.1000, G Loss: 1.5106, Grad norm G: 5.6327, Grad norm D: 4.3137\n",
      "Epoch [21/30], Step [20], D Loss: -0.8564, G Loss: 1.5329, Grad norm G: 5.6321, Grad norm D: 3.7142\n",
      "Epoch [21/30], Step [25], D Loss: -0.3197, G Loss: 1.0668, Grad norm G: 5.5662, Grad norm D: 3.4102\n",
      "Epoch [21/30], Step [30], D Loss: 0.0643, G Loss: 0.4788, Grad norm G: 5.3076, Grad norm D: 4.1161\n",
      "Epoch [21/30], Step [35], D Loss: 1.1988, G Loss: 0.0520, Grad norm G: 4.8027, Grad norm D: 3.5323\n",
      "Epoch [21/30], Step [40], D Loss: 1.0451, G Loss: -0.2733, Grad norm G: 4.7549, Grad norm D: 3.6475\n",
      "Epoch [21/30], Step [45], D Loss: 1.0327, G Loss: -0.2712, Grad norm G: 4.8439, Grad norm D: 3.9403\n",
      "Epoch [21/30], Step [50], D Loss: 1.0144, G Loss: -0.2245, Grad norm G: 4.8963, Grad norm D: 3.6860\n",
      "Epoch [21/30], Step [55], D Loss: 0.5693, G Loss: -0.0747, Grad norm G: 5.1015, Grad norm D: 3.8262\n",
      "Epoch [21/30], Step [60], D Loss: -0.3231, G Loss: 0.5767, Grad norm G: 5.2796, Grad norm D: 4.3668\n",
      "Epoch [21/30], Step [65], D Loss: -0.5833, G Loss: 1.2399, Grad norm G: 5.5104, Grad norm D: 4.0510\n",
      "Epoch [21/30], Step [70], D Loss: -0.7328, G Loss: 1.5047, Grad norm G: 5.7940, Grad norm D: 3.8246\n",
      "Epoch [21/30], Step [75], D Loss: -0.6134, G Loss: 1.3591, Grad norm G: 5.7683, Grad norm D: 3.6609\n",
      "Epoch [21/30], Step [80], D Loss: 0.0099, G Loss: 0.9223, Grad norm G: 5.4927, Grad norm D: 3.6900\n",
      "Epoch [21/30], Step [85], D Loss: 0.7195, G Loss: 0.4195, Grad norm G: 5.0678, Grad norm D: 3.4751\n",
      "Epoch [21/30], Step [90], D Loss: 0.9514, G Loss: 0.0819, Grad norm G: 4.8124, Grad norm D: 3.6431\n",
      "Epoch [21/30], Step [95], D Loss: 1.1928, G Loss: -0.1034, Grad norm G: 4.7253, Grad norm D: 3.6171\n",
      "Epoch [21/30], Step [100], D Loss: 0.9893, G Loss: -0.2704, Grad norm G: 4.8122, Grad norm D: 3.6670\n",
      "Epoch [21/30], Step [105], D Loss: 0.4927, G Loss: -0.1914, Grad norm G: 5.1341, Grad norm D: 4.3343\n",
      "Epoch [21/30], Step [110], D Loss: 0.4333, G Loss: 0.3313, Grad norm G: 5.1355, Grad norm D: 4.0427\n",
      "Epoch [21/30], Step [115], D Loss: -0.2312, G Loss: 0.8910, Grad norm G: 5.3776, Grad norm D: 3.9131\n",
      "Epoch [21/30], Step [120], D Loss: -0.7166, G Loss: 1.3344, Grad norm G: 5.5355, Grad norm D: 3.8898\n",
      "Epoch [21/30], Step [125], D Loss: -0.4895, G Loss: 1.3753, Grad norm G: 5.7306, Grad norm D: 3.7714\n",
      "Epoch [21/30], Step [130], D Loss: -0.1995, G Loss: 1.1183, Grad norm G: 5.5811, Grad norm D: 3.5428\n",
      "Epoch [21/30], Step [135], D Loss: 0.2410, G Loss: 0.7594, Grad norm G: 5.1237, Grad norm D: 3.6958\n",
      "Epoch [22/30], Step [1], D Loss: 0.5563, G Loss: 0.3993, Grad norm G: 4.8175, Grad norm D: 3.5986\n",
      "Epoch [22/30], Step [5], D Loss: 0.8486, G Loss: 0.0787, Grad norm G: 4.7482, Grad norm D: 3.6213\n",
      "Epoch [22/30], Step [10], D Loss: 1.0666, G Loss: -0.1426, Grad norm G: 4.7023, Grad norm D: 3.6635\n",
      "Epoch [22/30], Step [15], D Loss: 0.9517, G Loss: -0.3403, Grad norm G: 4.9047, Grad norm D: 3.6402\n",
      "Epoch [22/30], Step [20], D Loss: 0.8465, G Loss: -0.2563, Grad norm G: 4.9598, Grad norm D: 3.8531\n",
      "Epoch [22/30], Step [25], D Loss: 0.3797, G Loss: 0.2947, Grad norm G: 5.0047, Grad norm D: 4.0339\n",
      "Epoch [22/30], Step [30], D Loss: -0.4180, G Loss: 0.9574, Grad norm G: 5.2676, Grad norm D: 4.0868\n",
      "Epoch [22/30], Step [35], D Loss: -0.7840, G Loss: 1.3161, Grad norm G: 5.5841, Grad norm D: 3.9654\n",
      "Epoch [22/30], Step [40], D Loss: -0.6789, G Loss: 1.2533, Grad norm G: 5.7362, Grad norm D: 3.7588\n",
      "Epoch [22/30], Step [45], D Loss: -0.1411, G Loss: 0.9926, Grad norm G: 5.4555, Grad norm D: 3.5746\n",
      "Epoch [22/30], Step [50], D Loss: 0.4520, G Loss: 0.5975, Grad norm G: 4.9478, Grad norm D: 3.5883\n",
      "Epoch [22/30], Step [55], D Loss: 0.8948, G Loss: 0.2972, Grad norm G: 4.6406, Grad norm D: 3.4608\n",
      "Epoch [22/30], Step [60], D Loss: 0.9285, G Loss: 0.0738, Grad norm G: 4.7074, Grad norm D: 3.3727\n",
      "Epoch [22/30], Step [65], D Loss: 1.0025, G Loss: -0.1463, Grad norm G: 4.7965, Grad norm D: 3.6679\n",
      "Epoch [22/30], Step [70], D Loss: 1.0532, G Loss: -0.1608, Grad norm G: 4.9701, Grad norm D: 3.7493\n",
      "Epoch [22/30], Step [75], D Loss: 0.6868, G Loss: -0.0119, Grad norm G: 5.3136, Grad norm D: 3.8062\n",
      "Epoch [22/30], Step [80], D Loss: -0.0381, G Loss: 0.5976, Grad norm G: 5.5155, Grad norm D: 4.1522\n",
      "Epoch [22/30], Step [85], D Loss: -0.4011, G Loss: 1.2523, Grad norm G: 5.5948, Grad norm D: 4.1609\n",
      "Epoch [22/30], Step [90], D Loss: -0.6239, G Loss: 1.5373, Grad norm G: 5.6999, Grad norm D: 3.9116\n",
      "Epoch [22/30], Step [95], D Loss: -0.6713, G Loss: 1.4638, Grad norm G: 5.6469, Grad norm D: 3.6003\n",
      "Epoch [22/30], Step [100], D Loss: 0.0337, G Loss: 1.0534, Grad norm G: 5.2140, Grad norm D: 3.5915\n",
      "Epoch [22/30], Step [105], D Loss: 0.4848, G Loss: 0.6076, Grad norm G: 4.8546, Grad norm D: 3.4668\n",
      "Epoch [22/30], Step [110], D Loss: 0.8854, G Loss: 0.1994, Grad norm G: 4.6957, Grad norm D: 3.4546\n",
      "Epoch [22/30], Step [115], D Loss: 1.1748, G Loss: -0.0842, Grad norm G: 4.7343, Grad norm D: 3.5971\n",
      "Epoch [22/30], Step [120], D Loss: 1.2407, G Loss: -0.3621, Grad norm G: 4.8477, Grad norm D: 3.6226\n",
      "Epoch [22/30], Step [125], D Loss: 1.0262, G Loss: -0.2575, Grad norm G: 4.8185, Grad norm D: 3.7446\n",
      "Epoch [22/30], Step [130], D Loss: 0.5465, G Loss: 0.1571, Grad norm G: 5.1177, Grad norm D: 4.0239\n",
      "Epoch [22/30], Step [135], D Loss: -0.2566, G Loss: 0.9393, Grad norm G: 5.3925, Grad norm D: 4.0964\n",
      "Early stopping at epoch 22\n",
      "\n",
      "Generating synthetic samples for Class 2...\n",
      "\n",
      "Generating synthetic samples for Class 3...\n",
      "\n",
      "Generating synthetic samples for Class 4...\n",
      "Generated shape: (3000, 1258)\n",
      "PCA expected input: 1258\n",
      "\n",
      "Evaluating synthetic data for Class 2...\n",
      "Mean Silhouette Score for Class 2: 0.1548\n",
      "\n",
      "Evaluating synthetic data for Class 3...\n",
      "Mean Silhouette Score for Class 3: 0.1936\n",
      "\n",
      "Evaluating synthetic data for Class 4...\n",
      "Mean Silhouette Score for Class 4: 0.1619\n",
      "Classifier Accuracy for Class 2: 1.0000\n",
      "Classifier Accuracy for Class 3: 1.0000\n",
      "Classifier Accuracy for Class 4: 1.0000\n",
      "Overall Mean Silhouette Score: 0.1701\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from joblib import dump\n",
    "\n",
    "# \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "z_dim = 128  #   128\n",
    "cond_dim = 5\n",
    "output_dim = 1258  #   1258\n",
    "batch_size = 64  #  128\n",
    "num_epochs_gan = 30\n",
    "lr_g = 1e-4\n",
    "lr_d = 4e-4\n",
    "lambda_gp = 10\n",
    "n_critic = 1  #  2\n",
    "sim_weight = 0.0  #    \n",
    "max_norm = 5\n",
    "patience = 20\n",
    "output_dir = \"F:\\\\payan-nameh\\\\faz2 . 1404.04.02\\\\Date\\\\RNALocate\\\\\"\n",
    "\n",
    "#  )       PCA\n",
    "scaler_bert = StandardScaler()\n",
    "X_bert_train = scaler_bert.fit_transform(np.load(f\"{output_dir}X_train_bert.npy\"))\n",
    "\n",
    "scaler_handcrafted = StandardScaler()\n",
    "X_handcrafted_train = scaler_handcrafted.fit_transform(np.load(f\"{output_dir}X_train_handcrafted.npy\"))\n",
    "\n",
    "y_train = np.load(f\"{output_dir}y_train.npy\")\n",
    "\n",
    "print(f\"Shape of X_bert_train: {X_bert_train.shape}\")\n",
    "print(f\"Shape of X_handcrafted_train: {X_handcrafted_train.shape}\")\n",
    "if X_bert_train.shape[0] != X_handcrafted_train.shape[0]:\n",
    "    raise ValueError(f\"   X_bert_train ({X_bert_train.shape[0]})  X_handcrafted_train ({X_handcrafted_train.shape[0]})  !\")\n",
    "X_train_dual = np.concatenate([X_bert_train, X_handcrafted_train], axis=1)\n",
    "dual_dim = X_train_dual.shape[1]\n",
    "print(f\"Shape of X_train_dual: {X_train_dual.shape}, dual_dim: {dual_dim}\")\n",
    "\n",
    "#  PCA\n",
    "pca = PCA(n_components=512, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_dual)\n",
    "np.save(f\"{output_dir}X_train_pca.npy\", X_train_pca)\n",
    "print(f\"Shape of X_train_pca: {X_train_pca.shape}\")\n",
    "\n",
    "#   X_train_dual   X_train_pca  train_loader\n",
    "X_train_tensor = torch.FloatTensor(X_train_dual)  #   X_train_dual (1258 )\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "class_counts = np.bincount(y_train, minlength=cond_dim)\n",
    "majority_class_size = np.max(class_counts)\n",
    "threshold = 0.3 * majority_class_size\n",
    "rare_classes = np.where(class_counts < threshold)[0].tolist()\n",
    "print(f\"Rare classes identified: {rare_classes} with counts: {class_counts[rare_classes]}\")\n",
    "\n",
    "dump({'scaler_bert': scaler_bert, 'scaler_handcrafted': scaler_handcrafted, 'pca': pca}, f\"{output_dir}preprocessing.pkl\")\n",
    "\n",
    "#  )    Conditional GAN (WGAN-GP)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=128, class_dim=5, output_dim=1258):  #   1258\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(class_dim, class_dim)  # Embedding   \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim + class_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, output_dim),  #   1258\n",
    "            nn.Tanh()  #    [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        c = self.label_emb(labels)  #    Embedding\n",
    "        x = torch.cat([z, c], dim=1)  #    \n",
    "        return self.net(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=output_dim + cond_dim):  #   output_dim \n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(input_dim, 256)),  # 1258 + 5 = 1263\n",
    "            nn.LeakyReLU(0.2),\n",
    "            spectral_norm(nn.Linear(256, 128)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            spectral_norm(nn.Linear(128, 1))\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples, labels):\n",
    "    alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    class_onehot = F.one_hot(labels, num_classes=cond_dim).float().to(device)\n",
    "    disc_input_interpolates = torch.cat((interpolates, class_onehot), dim=1)\n",
    "    d_interpolates = D(disc_input_interpolates)\n",
    "    fake = torch.ones(real_samples.size(0), 1).to(device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates, inputs=interpolates, grad_outputs=fake,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# )  \n",
    "generator = Generator(z_dim=z_dim, class_dim=cond_dim, output_dim=output_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "g_optimizer = optim.AdamW(generator.parameters(), lr=lr_g, betas=(0.5, 0.9))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.9))\n",
    "scheduler_g = ReduceLROnPlateau(g_optimizer, 'min', patience=patience, factor=0.5)\n",
    "\n",
    "d_losses, g_losses = [], []\n",
    "best_combined_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs_gan):\n",
    "    epoch_g_losses = []\n",
    "    for i, (real_dual, labels) in enumerate(train_loader):\n",
    "        batch_size = real_dual.size(0)\n",
    "        real_dual, labels = real_dual.to(device), labels.to(device)\n",
    "\n",
    "        #  Discriminator\n",
    "        for _ in range(n_critic):\n",
    "            d_optimizer.zero_grad()\n",
    "            labels_batch = labels[:real_dual.size(0)]\n",
    "            class_onehot = F.one_hot(labels_batch, num_classes=cond_dim).float().to(device)\n",
    "            disc_input_real = torch.cat((real_dual, class_onehot), dim=1)  # 1258 + 5 = 1263\n",
    "            real_validity = discriminator(disc_input_real)\n",
    "            z = torch.randn(real_dual.size(0), z_dim).to(device)\n",
    "            fake_dual = generator(z, labels_batch)  #  \n",
    "            disc_input_fake = torch.cat((fake_dual.detach(), class_onehot), dim=1)\n",
    "            fake_validity = discriminator(disc_input_fake)\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_dual, fake_dual, labels_batch)  #  detach   \n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "            d_loss.backward()\n",
    "            total_norm_d = 0\n",
    "            for p in discriminator.parameters():\n",
    "                if p.grad is not None:\n",
    "                    total_norm_d += p.grad.data.norm(2).item() ** 2\n",
    "            total_norm_d = total_norm_d ** 0.5\n",
    "            torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=max_norm)\n",
    "            d_optimizer.step()\n",
    "\n",
    "        #  Generator\n",
    "        g_optimizer.zero_grad()\n",
    "        labels_batch = labels[:real_dual.size(0)]\n",
    "        class_onehot = F.one_hot(labels_batch, num_classes=cond_dim).float().to(device)  #  class_onehot\n",
    "        z = torch.randn(real_dual.size(0), z_dim).to(device)\n",
    "        fake_dual = generator(z, labels_batch)\n",
    "        disc_input_fake = torch.cat((fake_dual, class_onehot), dim=1)\n",
    "        fake_validity = discriminator(disc_input_fake)\n",
    "        g_loss = -torch.mean(fake_validity)\n",
    "        g_loss.backward()\n",
    "        total_norm_g = 0\n",
    "        for p in generator.parameters():\n",
    "            if p.grad is not None:\n",
    "                total_norm_g += p.grad.data.norm(2).item() ** 2\n",
    "        total_norm_g = total_norm_g ** 0.5\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=max_norm)\n",
    "        g_optimizer.step()\n",
    "\n",
    "        d_losses.append(d_loss.item())\n",
    "        g_losses.append(g_loss.item())\n",
    "        epoch_g_losses.append(g_loss.item())\n",
    "\n",
    "        if (i + 1) % 5 == 0 or i == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs_gan}], Step [{i+1}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, \"\n",
    "                  f\"Grad norm G: {total_norm_g:.4f}, Grad norm D: {total_norm_d:.4f}\")\n",
    "\n",
    "    combined_loss = np.mean(epoch_g_losses)\n",
    "    scheduler_g.step(combined_loss)\n",
    "\n",
    "    if best_combined_loss - combined_loss > 1e-3:\n",
    "        best_combined_loss = combined_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(generator.state_dict(), f\"{output_dir}gan_generator_dual.pt\")\n",
    "        torch.save(generator, f\"{output_dir}gan_generator_dual_full.pt\")\n",
    "        torch.save(discriminator.state_dict(), f\"{output_dir}gan_discriminator_dual.pt\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            torch.save(generator.state_dict(), f\"{output_dir}gan_generator_dual_last.pt\")\n",
    "            break\n",
    "\n",
    "#   \n",
    "generator.load_state_dict(torch.load(f\"{output_dir}gan_generator_dual.pt\"))\n",
    "generator.eval()\n",
    "\n",
    "#   synthetic  \n",
    "generated_data = []\n",
    "generated_labels = []\n",
    "num_samples_per_class = 1000\n",
    "for cls in rare_classes:\n",
    "    print(f\"\\nGenerating synthetic samples for Class {cls}...\")\n",
    "    z = torch.randn(num_samples_per_class, z_dim).to(device)\n",
    "    labels_c = torch.full((num_samples_per_class,), cls, dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        fake_dual = generator(z, labels_c)  #   1258 \n",
    "    generated_data.append(fake_dual.cpu().numpy())\n",
    "    generated_labels.append(np.full(num_samples_per_class, cls))\n",
    "\n",
    "X_synthetic = np.vstack(generated_data)  # : (num_samples, 1258)\n",
    "y_synthetic = np.concatenate(generated_labels)\n",
    "\n",
    "#  \n",
    "print(\"Generated shape:\", X_synthetic.shape)  #   (n_samples, 1258)\n",
    "print(\"PCA expected input:\", pca.n_features_in_)  #  1258 \n",
    "\n",
    "#  PCA   X_synthetic\n",
    "X_synthetic_pca = pca.transform(X_synthetic)  #   \n",
    "\n",
    "np.save(f\"{output_dir}X_synthetic_pca.npy\", X_synthetic_pca)\n",
    "np.save(f\"{output_dir}y_synthetic.npy\", y_synthetic)\n",
    "\n",
    "#   synthetic    train\n",
    "X_train_final = np.vstack([X_train_pca, X_synthetic_pca])\n",
    "y_train_final = np.concatenate([y_train, y_synthetic])\n",
    "\n",
    "np.save(f\"{output_dir}X_train_augmented.npy\", X_train_final)\n",
    "np.save(f\"{output_dir}y_train_augmented.npy\", y_train_final)\n",
    "\n",
    "#  t-SNE  Silhouette\n",
    "sil_scores = []\n",
    "for cls in rare_classes:\n",
    "    print(f\"\\nEvaluating synthetic data for Class {cls}...\")\n",
    "    real_idx = np.where(y_train == cls)[0][:500]\n",
    "    real_samples = X_train_pca[real_idx]\n",
    "    synthetic_samples = X_synthetic_pca[y_synthetic == cls][:500]\n",
    "\n",
    "    min_len = min(len(real_samples), len(synthetic_samples))\n",
    "    real_samples, synthetic_samples = real_samples[:min_len], synthetic_samples[:min_len]\n",
    "\n",
    "    X_vis = np.concatenate([real_samples, synthetic_samples])\n",
    "    labels_vis = np.concatenate([np.zeros(min_len), np.ones(min_len)])\n",
    "    X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X_vis)\n",
    "    sil_score = silhouette_score(X_vis, labels_vis)\n",
    "    sil_scores.append(sil_score)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels_vis, palette=[\"blue\", \"red\"], alpha=0.5)\n",
    "    plt.title(f\"t-SNE: Real vs Synthetic for Class {cls}\")\n",
    "    plt.savefig(f\"{output_dir}tsne_class_{cls}.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    mean_sil_score = np.mean(sil_scores[-1:])\n",
    "    print(f\"Mean Silhouette Score for Class {cls}: {mean_sil_score:.4f}\")\n",
    "\n",
    "# Classifier Accuracy\n",
    "for cls in rare_classes:\n",
    "    real_idx = np.where(y_train == cls)[0][:500]\n",
    "    real_samples = X_train_pca[real_idx]\n",
    "    synthetic_samples = X_synthetic_pca[y_synthetic == cls][:500]\n",
    "    min_len = min(len(real_samples), len(synthetic_samples))\n",
    "    real_samples, synthetic_samples = real_samples[:min_len], synthetic_samples[:min_len]\n",
    "    X_train_eval = np.concatenate([real_samples, synthetic_samples])\n",
    "    y_train_eval = np.concatenate([np.zeros(min_len), np.ones(min_len)])\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(128,), max_iter=100, random_state=42)\n",
    "    clf.fit(X_train_eval, y_train_eval)\n",
    "    accuracy = clf.score(X_train_eval, y_train_eval)\n",
    "    print(f\"Classifier Accuracy for Class {cls}: {accuracy:.4f}\")\n",
    "\n",
    "#  Overall Mean Silhouette Score\n",
    "overall_mean_sil = np.mean(sil_scores)\n",
    "print(f\"Overall Mean Silhouette Score: {overall_mean_sil:.4f}\")\n",
    "\n",
    "#  t-SNE   synthetic\n",
    "X_synthetic_tsne = TSNE(n_components=2, random_state=42).fit_transform(X_synthetic_pca)\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=X_synthetic_tsne[:, 0], y=X_synthetic_tsne[:, 1], hue=y_synthetic, palette=\"deep\", alpha=0.5)\n",
    "plt.title(\"t-SNE of All Synthetic Data\")\n",
    "plt.savefig(f\"{output_dir}tsne_all_synthetic.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "#   Loss   \n",
    "np.save(f\"{output_dir}d_losses.npy\", np.array(d_losses))\n",
    "np.save(f\"{output_dir}g_losses.npy\", np.array(g_losses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio-ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
